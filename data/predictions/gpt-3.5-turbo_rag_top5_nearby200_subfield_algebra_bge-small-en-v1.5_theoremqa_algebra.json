[
    {
        "id": "elainewan/math_algebra_1.json",
        "question": "Find the x value of the solutions to the linear system: 7x - y = 15x, -6x + 8y = 15y.",
        "prediction": "\\(x = 0, y = 0\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFind the x value of the solutions to the linear system: 7x - y = 15x, -6x + 8y = 15y.\n\nDocument 1:\n----\nOver-determined and under-determined systems of linear equations.The set of linear equations is called _over-determined_ if \\(m>n\\), _under-determined_ if \\(m<n\\), and _square_ if \\(m=n\\); these correspond to the coefficient matrix being tall, wide, and square, respectively. When the system of linear equations is over-determined, there are more equations than variables or unknowns. When the system of linear equations is under-determined, there are more unknowns than equations. When the system of linear equations is square, the numbers of unknowns and equations is the same. A set of equations with zero right-hand side, \\(Ax=0\\), is called a _homogeneous_ set of equations. Any homogeneous set of equations has \\(x=0\\) as a solution.\nIn chapter 11 we will address the question of how to determine if a system of linear equations has a solution, and how to find one when it does. For now, we give a few interesting examples.\n#### Examples\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\n\nDocument 2:\n----\nIn summary, a right inverse can be used to find \\(a\\) solution of a square or under-determined set of linear equations, for any vector \\(b\\).\nExamples.Consider the matrix appearing in the example above on page 199,\n\\[A=\\left[\\begin{array}{rr}-3&-4\\\\ 4&6\\\\ 1&1\\end{array}\\right]\\]\nand the two left inverses\n\\[B=\\frac{1}{9}\\left[\\begin{array}{rr}-11&-10&16\\\\ 7&8&-11\\end{array}\\right],\\qquad C=\\frac{1}{2}\\left[\\begin{array}{rr}0&-1&6 \\\\ 0&1&-4\\end{array}\\right].\\]\n* The over-determined linear equations \\(Ax=(1,-2,0)\\) have the unique solution \\(x=(1,-1)\\), which can be obtained from _either_ left inverse: \\[x=B(1,-2,0)=C(1,-2,0).\\]\n* The over-determined linear equations \\(Ax=(1,-1,0)\\) do not have a solution, since \\(x=C(1,-1,0)=(1/2,-1/2)\\) does not satisfy \\(Ax=(1,-1,0)\\).\n* The under-determined linear equations \\(A^{T}y=(1,2)\\) has (different) solutions \\[B^{T}(1,2)=(1/3,2/3,38/9),\\qquad C^{T}(1,2)=(0,1/2,-1).\\] (Recall that \\(B^{T}\\) and \\(C^{T}\\) are both right inverses of \\(A^{T}\\).) We can find a solution of \\(A^{T}y=b\\) for any vector \\(b\\).\nLeft and right inverse of matrix product.Suppose \\(A\\) and \\(D\\) are compatible for the matrix product \\(AD\\) (_i.e._, the number of columns in \\(A\\) is equal to the number of rows in \\(D\\).) If \\(A\\) has a right inverse \\(B\\) and \\(D\\) has a right inverse \\(E\\), then \\(EB\\) is a right inverse of \\(AD\\). This follows from \\[(AD)(EB)=A(DE)B=A(IB)=AB=I.\\] If \\(A\\) has a left inverse \\(C\\) and \\(D\\) has a left inverse \\(F\\), then \\(FC\\) is a left inverse of \\(AD\\). This follows from \\[(FC)(AD)=F(CA)D=FD=I.\\]\n\nDocument 3:\n----\n**8.13**: _Portfolio sector exposures._ (See exercise 1.14.) The \\(n\\)-vector \\(h\\) denotes a portfolio of investments in \\(n\\) assets, with \\(h_{i}\\) the dollar value invested in asset \\(i\\). We consider a set of \\(m\\) industry sectors, such as pharmaceuticals or consumer electronics. Each asset is assigned to one of these sectors. (More complex models allow for an asset to be assigned to more than one sector.) The _exposure_ of the portfolio to sector \\(i\\) is defined as the sum of investments in the assets in that sector. We denote the sector exposures using the \\(m\\)-vector \\(s\\), where \\(s_{i}\\) is the portfolio exposure to sector \\(i\\). (When \\(s_{i}=0\\), the portfolio is said to be _neutral_ to sector \\(i\\).) An investment advisor specifies a set of desired sector exposures, given as the \\(m\\)-vector \\(s^{\\rm des}\\). Express the requirement \\(s=s^{\\rm des}\\) as a set of linear equations of the form \\(Ah=b\\). (You must describe the matrix \\(A\\) and the vector \\(b\\).) _Remark._ A typical practical case involves \\(n=1000\\) assets and \\(m=50\\) sectors. An advisor might specify \\(s_{i}^{\\rm des}=0\\) if she does not have an opinion as how companies in that sector will do in the future; she might specify a positive value for \\(s_{i}^{\\rm des}\\) if she thinks the companies in that sector will do well (_i.e._, generate positive returns) in the future, and a negative value if she thinks they will do poorly.\n**8.14**: _Affine combinations of solutions of linear equations._ Consider the set of \\(m\\) linear equations in \\(n\\) variables \\(Ax=b\\), where \\(A\\) is an \\(m\\times n\\) matrix, \\(b\\) is an \\(m\\)-vector, and \\(x\\) is the \\(n\\)-vector of variables. Suppose that the \\(n\\)-vectors \\(z_{1},\\ldots,z_{k}\\) are solutions of this set of equations, _i.e._, satisfy \\(Az_{i}=b\\). Show that if the coefficients \\(\\alpha_{1},\\ldots,\\alpha_{k}\\) satisfy \\(\\alpha_{1}+\\cdots+\\alpha_{k}=1\\), then the affine combination\n\nDocument 4:\n----\nIn this context, the \\(m\\times n\\) matrix \\(A\\) is called the _coefficient matrix_, and the \\(m\\)-vector \\(b\\) is called the _right-hand side_. An \\(n\\)-vector \\(x\\) is called a _solution_ of the linear equations if \\(Ax=b\\) holds. A set of linear equations can have no solutions, one solution, or multiple solutions.\nExamples.\n* The set of linear equations \\[x_{1}+x_{2}=1,\\quad x_{1}=-1,\\quad x_{1}-x_{2}=0\\] is written as \\(Ax=b\\) with \\[A=\\left[\\begin{array}{cc}1&1\\\\ 1&0\\\\ 1&-1\\end{array}\\right],\\qquad b=\\left[\\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right].\\] It has no solutions.\n* The set of linear equations \\[x_{1}+x_{2}=1,\\quad x_{2}+x_{3}=2\\] is written as \\(Ax=b\\) with \\[A=\\left[\\begin{array}{cc}1&1&0\\\\ 0&1&1\\end{array}\\right],\\qquad b=\\left[\\begin{array}{c}1\\\\ 2\\end{array}\\right].\\] It has multiple solutions, including \\(x=(1,0,2)\\) and \\(x=(0,1,1)\\).\nOver-determined and under-determined systems of linear equations.The set of linear equations is called _over-determined_ if \\(m>n\\), _under-determined_ if \\(m<n\\), and _square_ if \\(m=n\\); these correspond to the coefficient matrix being tall, wide, and square, respectively. When the system of linear equations is over-determined, there are more equations than variables or unknowns. When the system of linear equations is under-determined, there are more unknowns than equations. When the system of linear equations is square, the numbers of unknowns and equations is the same. A set of equations with zero right-hand side, \\(Ax=0\\), is called a _homogeneous_ set of equations. Any homogeneous set of equations has \\(x=0\\) as a solution.\nIn chapter 11 we will address the question of how to determine if a system of linear equations has a solution, and how to find one when it does. For now, we give a few interesting examples.\n\nDocument 5:\n----\nWhen a matrix \\(A\\) has both a left inverse \\(Y\\) and a right inverse \\(X\\), we call the matrix \\(X=Y\\) simply the _inverse_ of \\(A\\), and denote it as \\(A^{-1}\\). We say that \\(A\\) is _invertible_ or _nonsingular_. A square matrix that is not invertible is called _singular_.\nDimensions of invertible matrices.Invertible matrices must be square, since tall matrices are not right-invertible, while wide matrices are not left-invertible. A matrix \\(A\\) and its inverse (if it exists) satisfy\n\\[AA^{-1}=A^{-1}A=I.\\]\nIf \\(A\\) has inverse \\(A^{-1}\\), then the inverse of \\(A^{-1}\\) is \\(A\\); in other words, we have \\((A^{-1})^{-1}=A\\). For this reason we say that \\(A\\) and \\(A^{-1}\\) are inverses (of each other).\nSolving linear equations with the inverse.Consider the square system of \\(n\\) linear equations with \\(n\\) variables, \\(Ax=b\\). If \\(A\\) is invertible, then for any \\(n\\)-vector \\(b\\),\n\\[x=A^{-1}b\\] (11.1)\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\n\n\n",
        "answer": 0,
        "rationale": {
            "Question": "Find the x value of the solutions to the linear system: 7x - y = 15x, -6x + 8y = 15y.",
            "Answer": 0,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_1.json",
            "explanation": "solutions/math_algebra_1.txt",
            "theorem": "linear systems",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "elainewan/math_algebra_3.json",
        "question": "Is W = {[x, y] in R^2: x >= 0 and y >= 0} a subspace of R^2?",
        "prediction": "False",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs W = {[x, y] in R^2: x >= 0 and y >= 0} a subspace of R^2?\n\nDocument 1:\n----\n## 17 Constrained least squares applications\n### 17.1 A variation on the portfolio optimization formulation\nConsider the following variation on the linearly constrained least squares problem (17.2):\n\\[\\begin{array}{ll}\\mbox{minimize}&\\|Rw\\|^{2}\\\\ \\mbox{subject to}&\\left[\\begin{array}{c}\\mathbf{1}^{T}\\\\ \\mu^{T}\\end{array}\\right]w=\\left[\\begin{array}{c}1\\\\ \\rho\\end{array}\\right],\\end{array}\\] (17.12)\nwith variable \\(w\\). (The difference is that here we drop the term \\(\\rho\\mathbf{1}\\) that appears inside the norm square objective in (17.2).) Show that this problem is equivalent to (17.2). This means \\(w\\) is a solution of (17.12) if and only if it is a solution of (17.2).\n_Hint._ You can argue directly by expanding the objective in (17.2) or via the KKT systems of the two problems.\n### 17.2 A more conventional formulation of the portfolio optimization problem\nIn this problem we derive an equivalent formulation of the portfolio optimization problem (17.2) that appears more frequently in the literature than our version. (Equivalent means that the two problems always have the same solution.) This formulation is based on the _return covariance matrix_, which we define below. (See also exercise 10.16.)\nThe means of the columns of the asset return matrix \\(R\\) are the entries of the vector \\(\\mu\\). The _de-meaned returns matrix_ is given by \\(\\tilde{R}=R-\\mathbf{1}\\mu^{T}\\). (The columns of the matrix \\(\\tilde{R}=R-\\mathbf{1}\\mu^{T}\\) are the de-meaned return time series for the assets.) The return covariance matrix, traditionally denoted \\(\\Sigma\\), is its Gram matrix \\(\\Sigma=(1/T)\\tilde{R}^{T}\\tilde{R}\\).\n\nDocument 2:\n----\n\\[\\tilde{R}^{T}\\tilde{R}w=2\\tilde{R}^{T}\\tilde{Q}^{T}Q_{1}^{T}b-2d,\\]\nwhich we can write as\n\\[\\tilde{R}w=2\\tilde{Q}^{T}Q_{1}^{T}b-2\\tilde{R}^{-T}d.\\]\nWe can use this to compute \\(w\\), first by computing \\(\\tilde{R}^{-T}d\\) (by forward substitution), then forming the right-hand side, and then solving for \\(w\\) using back substitution. Once we know \\(w\\), we can find \\(\\hat{x}\\) from (16.9). The method is summarized in the following algorithm.\n**Algorithm 16.2** Constrained least squares via QR factorization\nIn the unconstrained case (when \\(p=0\\)), step 1 reduces to computing the QR factorization of \\(A\\), steps 2 and 3 are not needed, and step 4 reduces to solving \\(R\\hat{x}=Q_{1}^{T}b\\). This is the same as algorithm 12.1 for solving (unconstrained) least squares problems.\nWe now give a complexity analysis. Step 1 involves the QR factorizations of an \\((m+p)\\times n\\) and an \\(n\\times p\\) matrix, which costs \\(2(m+p)n^{2}+2np^{2}\\) flops. Step 2 requires \\(p^{2}\\) flops. In step 3, we first evaluate \\(Q_{1}^{T}b\\) (\\(2mn\\) flops), multiply the result by \\(\\tilde{Q}^{T}\\) (\\(2pn\\) flops), and then solve for \\(w\\) using forward substitution (\\(p^{2}\\) flops). Step 4 requires \\(2mn+2pn\\) flops to form the right-hand side, and \\(n^{2}\\) flops to compute \\(\\hat{x}\\) via back substitution. The costs of steps 2, 3, and 4 are quadratic in the dimensions, and so are negligible compared to the cost of step 1, so our final complexity is\n\nDocument 3:\n----\n2. For each degree, plot the polynomial \\(\\tilde{f}(x)\\) and the classifier \\(\\hat{f}(x)=\\mathbf{sign}(\\tilde{f}(x))\\).\n3. It is possible to classify this data set perfectly using a classifier \\(\\hat{f}(x)=\\mathbf{sign}(\\tilde{f}(x))\\) and a cubic polynomial \\[\\tilde{f}(x)=c(x+0.5)(x-0.1)(x-0.5),\\] for any positive \\(c\\). Compare this classifier with the least squares classifier of degree 3 that you found and explain why there is a difference.\n**14.7**: _Polynomial classifier with two features._ Generate 200 random 2-vectors \\(x^{(1)},\\ldots,x^{(200)}\\) in a plane, from a standard normal distribution. Define\n\\[y^{(i)}=\\left\\{\\begin{array}{ll}+1&x_{1}^{(i)}x_{2}^{(i)}\\geq 0\\\\ -1&\\text{otherwise}\\end{array}\\right.\\]\nfor \\(i=1,\\ldots,200\\). In other words, \\(y^{(i)}\\) is \\(+1\\) when \\(x^{(i)}\\) is in the first or third quadrant, and \\(-1\\) otherwise. Fit a polynomial least squares classifier of degree 2 to the data set, _i.e._, use a polynomial\n\\[\\tilde{f}(x)=\\theta_{1}+\\theta_{2}x_{1}+\\theta_{3}x_{2}+\\theta_{4}x_{1}^{2}+ \\theta_{5}x_{1}x_{2}+\\theta_{6}x_{2}^{2}.\\]\nGive the error rate of the classifier. Show the regions in the plane where \\(\\hat{f}(x)=1\\) and \\(\\hat{f}(x)=-1\\). Also compare the computed coefficients with the polynomial \\(\\tilde{f}(x)=x_{1}x_{2}\\), which classifies the data points with zero error.\n**14.8**: _Author attribution._ Suppose that the \\(N\\) feature \\(n\\)-vectors \\(x^{(1)},\\ldots,x^{(N)}\\) are word count histograms, and the labels \\(y^{(1)},\\ldots,y^{(N)}\\) give the document authors (as one of \\(1,\\ldots,K\\)). A classifier guesses which of the \\(K\\) authors wrote an unseen document, which is called _author attribution_. A least squares classifier using regression is fit to the data, resulting in the classifier\n\nDocument 4:\n----\nfrom chapter 3. From the first of the two normal equations, \\(N\\theta_{1}+(\\mathbf{1}^{T}x^{\\rm d})\\theta_{2}=\\mathbf{1}^{T}y^{\\rm d}\\), we also obtain a simple expression for \\(\\hat{\\theta}_{1}\\):\n\\[\\hat{\\theta}_{1}=\\mathbf{avg}(y^{\\rm d})-\\hat{\\theta}_{2}\\,\\mathbf{avg}(x^{ \\rm d}).\\]\nPutting these results together, we can write the least squares fit as\n\\[\\hat{f}(x)=\\mathbf{avg}(y^{\\rm d})+\\rho\\frac{\\mathbf{std}(y^{\\rm d})}{\\mathbf{ std}(x^{\\rm d})}(x-\\mathbf{avg}(x^{\\rm d})).\\] (13.3)\n(Note that \\(x\\) and \\(y\\) are generic scalar values, while \\(x^{\\rm d}\\) and \\(y^{\\rm d}\\) are vectors of the observed data values.) When \\(\\mathbf{std}(y^{\\rm d})\\neq 0\\), this can be expressed in the more symmetric form\n\\[\\frac{\\hat{y}-\\mathbf{avg}(y^{\\rm d})}{\\mathbf{std}(y^{\\rm d})}=\\rho\\frac{x- \\mathbf{avg}(x^{\\rm d})}{\\mathbf{std}(x^{\\rm d})},\\]\nwhich has a nice interpretation. The left-hand side is the difference between the predicted response value and the mean response value, divided by its standard deviation. The right-hand side is the correlation coefficient \\(\\rho\\) times the same quantity, computed for the dependent variable.\nThe least squares straight-line fit is used in many application areas.\nAsset \\(\\alpha\\) and \\(\\beta\\) in finance.In finance, the straight-line fit is used to predict the return of an individual asset from the return of the whole market. (The return of the whole market is typically taken to be a sum of the individual asset returns, weighted by their capitalizations.) The straight-line model \\(\\hat{f}(x)=\\theta_{1}+\\theta_{2}x\\) predicts the asset return from the market return \\(x\\). The least squares straight-line fit is computed from observed market returns \\(r_{1}^{\\rm mkt},\\ldots,r_{T}^{\\rm mkt}\\) and individual asset returns \\(r_{1}^{\\rm ind},\\ldots,r_{T}^{\\rm ind}\\) over some period of length \\(T\\). We therefore take\n\nDocument 5:\n----\nholds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). It is a good exercise to parse this simple looking equation, since it involves overloading of notation. On the left-hand side, the scalar-vector multiplications \\(\\alpha x\\) and \\(\\beta y\\) involve \\(n\\)-vectors, and the sum \\(\\alpha x+\\beta y\\) is the sum of two \\(n\\)-vectors. The function \\(f\\) maps \\(n\\)-vectors to \\(m\\)-vectors, so \\(f(\\alpha x+\\beta y)\\) is an \\(m\\)-vector. On the right-hand side, the scalar-vector multiplications and the sum are those for \\(m\\)-vectors. Finally, the equality sign is equality between two \\(m\\)-vectors.\nWe can verify that superposition holds for \\(f\\) using properties of matrix-vector and scalar-vector multiplication:\n\\[f(\\alpha x+\\beta y) = A(\\alpha x+\\beta y)\\] \\[= A(\\alpha x)+A(\\beta y)\\] \\[= \\alpha(Ax)+\\beta(Ay)\\] \\[= \\alpha f(x)+\\beta f(y)\\]\nThus we can associate with every matrix \\(A\\) a linear function \\(f(x)=Ax\\).\nThe converse is also true. Suppose \\(f\\) is a function that maps \\(n\\)-vectors to \\(m\\)-vectors, and is linear, _i.e._, (8.1) holds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). Then there exists an \\(m\\times n\\) matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\). This can be shown in the same way as for scalar-valued functions in SS2.1, by showing that if \\(f\\) is linear, then\n\\[f(x)=x_{1}f(e_{1})+x_{2}f(e_{2})+\\cdots+x_{n}f(e_{n}),\\] (8.2)\nwhere \\(e_{k}\\) is the \\(k\\)th unit vector of size \\(n\\). The right-hand side can also be written as a matrix-vector product \\(Ax\\), with\n\\[A=\\left[\\begin{array}{cccc}f(e_{1})&f(e_{2})&\\cdots&f(e_{n})\\end{array} \\right].\\]\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is W = {[x, y] in R^2: x >= 0 and y >= 0} a subspace of R^2?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3.json",
            "explanation": "solutions/math_algebra_3.png",
            "theorem": "linear subspaces",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "elainewan/math_algebra_3_6.json",
        "question": "Consider the basis B of R^2 consisting of vectors v_1 = [3,1] and v_2 = [-1, 3]. If x = [10, 10], find the B-coordinate vector of x",
        "prediction": "\\[ [x]_B = \\begin{bmatrix} 4 \\\\ 2 \\end{bmatrix} \\]",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider the basis B of R^2 consisting of vectors v_1 = [3,1] and v_2 = [-1, 3]. If x = [10, 10], find the B-coordinate vector of x\n\nDocument 1:\n----\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\\[l_{i}=\\left[\\begin{array}{c}0_{i-1}\\\\ 1\\\\ -(1+r)\\\\ 0_{n-i-1}\\end{array}\\right],\\quad i=1,\\ldots,n-1,\\]\nwhere \\(r\\geq 0\\) is the per-period interest rate. The cash flow \\(l_{i}\\) represents a loan of $1 in period \\(i\\), which is paid back in period \\(i+1\\) with interest \\(r\\). (The subscripts on the zero vectors above give their dimensions.) Scaling \\(l_{i}\\) changes the loan amount; scaling \\(l_{i}\\) by a negative coefficient converts it into a loan _to_ another entity (which is paid back in period \\(i+1\\) with interest).\n\nDocument 2:\n----\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\nNow suppose that \\(x\\) is any \\(n\\)-vector. It can be expressed as a linear combination of the basis vectors \\(a_{1},\\ldots,a_{n}\\):\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{n}a_{n}.\\]\nThe dual basis gives us a simple way to find the coefficients \\(\\beta_{1},\\ldots,\\beta_{n}\\).\nWe start with \\(AB=I\\), and multiply by \\(x\\) to get\n\\[x=ABx=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right]\\left[ \\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right]x=(b_{1}^{T}x)a_{1}+\\cdots+(b_{n}^{T}x)a_{n}.\\]\nThis means (since the vectors \\(a_{1},\\ldots,a_{n}\\) are linearly independent) that \\(\\beta_{i}=b_{i}^{T}x\\). In words: The coefficients in the expansion of a vector in a basis are given by the inner products with the dual basis vectors. Using matrix notation, we can say that \\(\\beta=B^{T}x=(A^{-1})^{T}x\\) is the vector of coefficients of \\(x\\) in the basis given by the columns of \\(A\\).\n\nDocument 3:\n----\nInner product.When \\(a\\) and \\(b\\) are \\(n\\)-vectors, \\(a^{T}b\\) is exactly the inner product of \\(a\\) and \\(b\\), obtained from the rules for transposing matrices and forming a matrix-vector product. We start with the (column) \\(n\\)-vector \\(a\\), consider it as an \\(n\\times 1\\) matrix, and transpose it to obtain the \\(n\\)-row-vector \\(a^{T}\\). Now we multiply this \\(1\\times n\\) matrix by the \\(n\\)-vector \\(b\\), to obtain the 1-vector \\(a^{T}b\\), which we also consider a scalar. So the notation \\(a^{T}b\\) for the inner product is just a special case of matrix-vector multiplication.\nLinear dependence of columns.We can express the concepts of linear dependence and independence in a compact form using matrix-vector multiplication. The columns of a matrix \\(A\\) are linearly dependent if \\(Ax=0\\) for some \\(x\\neq 0\\). The columns of a matrix \\(A\\) are linearly independent if \\(Ax=0\\) implies \\(x=0\\).\nExpansion in a basis.If the columns of \\(A\\) are a basis, which means \\(A\\) is square with linearly independent columns \\(a_{1},\\ldots,a_{n}\\), then for any \\(n\\)-vector \\(b\\) there is a unique \\(n\\)-vector \\(x\\) that satisfies \\(Ax=b\\). In this case the vector \\(x\\) gives the coefficients in the expansion of \\(b\\) in the basis \\(a_{1},\\ldots,a_{n}\\).\nProperties of matrix-vector multiplication.Matrix-vector multiplication satisfies several properties that are readily verified. First, it distributes across the vector argument: For any \\(m\\times n\\) matrix \\(A\\) and any \\(n\\)-vectors \\(u\\) and \\(v\\), we have\n\nDocument 4:\n----\n**2.7**: _General formula for affine functions._ Verify that formula (2.4) holds for any affine function \\(f:{\\bf R}^{n}\\to{\\bf R}\\). You can use the fact that \\(f(x)=a^{T}x+b\\) for some \\(n\\)-vector \\(a\\) and scalar \\(b\\).\n**2.8**: _Integral and derivative of polynomial._ Suppose the \\(n\\)-vector \\(c\\) gives the coefficients of a polynomial \\(p(x)=c_{1}+c_{2}x+\\cdots+c_{n}x^{n-1}\\).\n1. Let \\(\\alpha\\) and \\(\\beta\\) be numbers with \\(\\alpha<\\beta\\). Find an \\(n\\)-vector \\(a\\) for which \\[a^{T}c=\\int_{\\alpha}^{\\beta}p(x)\\;dx\\] always holds. This means that the integral of a polynomial over an interval is a linear function of its coefficients.\n2. Let \\(\\alpha\\) be a number. Find an \\(n\\)-vector \\(b\\) for which \\[b^{T}c=p^{\\prime}(\\alpha).\\] This means that the derivative of the polynomial at a given point is a linear function of its coefficients.\n**2.9**: _Taylor approximation._ Consider the function \\(f:{\\bf R}^{2}\\to{\\bf R}\\) given by \\(f(x_{1},x_{2})=x_{1}x_{2}\\). Find the Taylor approximation \\(\\hat{f}\\) at the point \\(z=(1,1)\\). Compare \\(f(x)\\) and \\(\\hat{f}(x)\\) for the following values of \\(x\\): \\[x=(1,1),\\quad x=(1.05,0.95),\\quad x=(0.85,1.25),\\quad x=(-1,2).\\] Make a brief comment about the accuracy of the Taylor approximation in each case.\n**2.10**: _Regression model._ Consider the regression model \\(\\hat{y}=x^{T}\\beta+v\\), where \\(\\hat{y}\\) is the predicted response, \\(x\\) is an 8-vector of features, \\(\\beta\\) is an 8-vector of coefficients, and \\(v\\) is the offset term. Determine whether each of the following statements is true or false.\n\nDocument 5:\n----\nas the _expansion of \\(b\\) in the \\(a_{1},\\ldots,a_{n}\\) basis_. The numbers \\(\\alpha_{1},\\ldots,\\alpha_{n}\\) are called the _coefficients_ of the expansion of \\(b\\) in the basis \\(a_{1},\\ldots,a_{n}\\). (We will see later how to find the coefficients in the expansion of a vector in a basis.)\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\n\n",
        "answer": [
            4,
            2
        ],
        "rationale": {
            "Question": "Consider the basis B of R^2 consisting of vectors v_1 = [3,1] and v_2 = [-1, 3]. If x = [10, 10], find the B-coordinate vector of x",
            "Answer": [
                4,
                2
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_6.json",
            "explanation": "solutions/math_algebra_3_6.txt",
            "theorem": "basis",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "elainewan/math_algebra_4_2.json",
        "question": "Let V be the space of all infinite sequences of real numbers. Consider the transformation T(x_0, x_1, x_2, ...) = (x_1, x_2, x_3, ...) from V to V. Is the sequence (1,2,3,...) in the image of T?",
        "prediction": "False",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet V be the space of all infinite sequences of real numbers. Consider the transformation T(x_0, x_1, x_2, ...) = (x_1, x_2, x_3, ...) from V to V. Is the sequence (1,2,3,...) in the image of T?\n\nDocument 1:\n----\nIn this problem we consider several linear functions of a monochrome image with \\(N\\times N\\) pixels. To keep the matrices small enough to work out by hand, we will consider the case with \\(N=3\\) (which would hardly qualify as an image). We represent a \\(3\\times 3\\) image as a 9-vector using the ordering of pixels shown below.\n\\begin{tabular}{|c|c|c|} \\hline\n1 & 4 & 7 \\\\ \\hline\n2 & 5 & 8 \\\\ \\hline\n3 & 6 & 9 \\\\ \\hline \\end{tabular} (This ordering is called _column-major_.) Each of the operations or transformations below defines a function \\(y=f(x)\\), where the 9-vector \\(x\\) represents the original image, and the 9-vector \\(y\\) represents the resulting or transformed image. For each of these operations, give the \\(9\\times 9\\) matrix \\(A\\) for which \\(y=Ax\\).\n* Turn the original image \\(x\\) upside-down.\n* Rotate the original image \\(x\\) clockwise \\(90^{\\circ}\\).\n* Translate the image up by 1 pixel and to the right by 1 pixel. In the translated image, assign the value \\(y_{i}=0\\) to the pixels in the first column and the last row.\n* Set each pixel value \\(y_{i}\\) to be the average of the neighbors of pixel \\(i\\) in the original image. By neighbors, we mean the pixels immediately above and below, and immediately to the left and right. The center pixel has 4 neighbors; corner pixels have 2 neighbors, and the remaining pixels have 3 neighbors.\n\nDocument 2:\n----\nTriangular matrix.A triangular matrix with nonzero diagonal elements is invertible. We first discuss this for a lower triangular matrix. Let \\(L\\) be \\(n\\times n\\) and lower triangular with nonzero diagonal elements. We show that the columns are linearly independent, _i.e._, \\(Lx=0\\) is only possible if \\(x=0\\). Expanding the matrix-vector product, we can write \\(Lx=0\\) as\n\\[L_{11}x_{1} = 0\\] \\[L_{21}x_{1}+L_{22}x_{2} = 0\\] \\[L_{31}x_{1}+L_{32}x_{2}+L_{33}x_{3} = 0\\] \\[\\vdots\\] \\[L_{n1}x_{1}+L_{n2}x_{2}+\\cdots+L_{n,n-1}x_{n-1}+L_{nn}x_{n} = 0.\\]\nSince \\(L_{11}\\neq 0\\), the first equation implies \\(x_{1}=0\\). Using \\(x_{1}=0\\), the second equation reduces to \\(L_{22}x_{2}=0\\). Since \\(L_{22}\\neq 0\\), we conclude that \\(x_{2}=0\\). Using \\(x_{1}=x_{2}=0\\), the third equation now reduces to \\(L_{33}x_{3}=0\\), and since \\(L_{33}\\) is assumed to be nonzero, we have \\(x_{3}=0\\). Continuing this argument, we find that all entries of \\(x\\) are zero, and this shows that the columns of \\(L\\) are linearly independent. It follows that \\(L\\) is invertible.\nA similar argument can be followed to show that an upper triangular matrix with nonzero diagonal elements is invertible. One can also simply note that if \\(R\\) is upper triangular, then \\(L=R^{T}\\) is lower triangular with the same diagonal, and use the formula \\((L^{T})^{-1}=(L^{-1})^{T}\\) for the inverse of the transpose.\nInverse via QR factorization.The QR factorization gives a simple expression for the inverse of an invertible matrix. If \\(A\\) is square and invertible, its columns are linearly independent, so it has a QR factorization \\(A=QR\\). The matrix \\(Q\\) is orthogonal and \\(R\\) is upper triangular with positive diagonal entries. Hence \\(Q\\) and \\(R\\) are invertible, and the formula for the inverse product gives\n\nDocument 3:\n----\n### 11.5 Pseudo-inverse\nLinearly independent columns and Gram invertibility.We first show that an \\(m\\times n\\) matrix \\(A\\) has linearly independent columns if and only if its \\(n\\times n\\) Gram matrix \\(A^{T}A\\) is invertible.\nFirst suppose that the columns of \\(A\\) are linearly independent. Let \\(x\\) be an \\(n\\)-vector which satisfies \\((A^{T}A)x=0\\). Multiplying on the left by \\(x^{T}\\) we get\n\\[0=x^{T}0=x^{T}(A^{T}Ax)=x^{T}A^{T}Ax=\\|Ax\\|^{2},\\]\nwhich implies that \\(Ax=0\\). Since the columns of \\(A\\) are linearly independent, we conclude that \\(x=0\\). Since the only solution of \\((A^{T}A)x=0\\) is \\(x=0\\), we conclude that \\(A^{T}A\\) is invertible.\nNow let's show the converse. Suppose the columns of \\(A\\) are linearly dependent, which means there is a nonzero \\(n\\)-vector \\(x\\) which satisfies \\(Ax=0\\). Multiply on the left by \\(A^{T}\\) to get \\((A^{T}A)x=0\\). This shows that the Gram matrix \\(A^{T}A\\) is singular.\nPseudo-inverse of square or tall matrix.We show here that if \\(A\\) has linearly independent columns (and therefore, is square or tall) then it has a left inverse. (We already have observed the converse, that a matrix with a left inverse has linearly independent columns.) Assuming \\(A\\) has linearly independent columns, we know that \\(A^{T}A\\) is invertible. We now observe that the matrix \\((A^{T}A)^{-1}A^{T}\\) is a left inverse of \\(A\\):\n\\[\\left((A^{T}A)^{-1}A^{T}\\right)A=(A^{T}A)^{-1}(A^{T}A)=I.\\]\n\nDocument 4:\n----\n\\[a*b=T(b)a=T(a)b,\\]\nwhere \\(T(b)\\) is the \\((n+m-1)\\times n\\) matrix with entries \\[T(b)_{ij}=\\left\\{\\begin{array}{ll}b_{i-j+1}&1\\leq i-j+1\\leq m\\\\ 0&\\text{otherwise}\\end{array}\\right.\\] (7.3) and similarly for \\(T(a)\\). For example, with \\(n=4\\) and \\(m=3\\), we have \\[T(b)=\\left[\\begin{array}{cccc}b_{1}&0&0&0\\\\ b_{2}&b_{1}&0&0\\\\ b_{3}&b_{2}&b_{1}&0\\\\ 0&b_{3}&b_{2}&b_{1}\\\\ 0&0&b_{3}&b_{2}\\\\ 0&0&0&b_{3}\\end{array}\\right],\\qquad T(a)=\\left[\\begin{array}{cccc}a_{1}&0&0 \\\\ a_{2}&a_{1}&0\\\\ a_{3}&a_{2}&a_{1}\\\\ a_{4}&a_{3}&a_{2}\\\\ 0&a_{4}&a_{3}\\\\ 0&0&a_{4}\\end{array}\\right].\\] The matrices \\(T(b)\\) and \\(T(a)\\) are called _Toeplitz_ matrices (named after the mathematician Otto Toeplitz), which means the entries on any diagonal (_i.e._, indices with \\(i-j\\) constant) are the same. The columns of the Toeplitz matrix \\(T(a)\\) are simply shifted versions of the vector \\(a\\), padded with zero entries.\nVariations.Several slightly different definitions of convolution are used in different applications. In one variation, \\(a\\) and \\(b\\) are infinite two-sided sequences (and not vectors) with indices ranging from \\(-\\infty\\) to \\(\\infty\\). In another variation, the rows of \\(T(a)\\) at the top and bottom that do not contain all the coefficients of \\(a\\) are dropped. (In this version, the rows of \\(T(a)\\) are shifted versions of the vector \\(a\\), reversed.) For consistency, we will use the one definition (7.2).\nExamples.\n* _Time series smoothing._ Suppose the \\(n\\)-vector \\(x\\) is a time series, and \\(a=(1/3,1/3,1/3)\\). Then the \\((n+2)\\)-vector \\(y=a*x\\) can be interpreted as a _smoothed_ version of the original time series: for \\(i=3,\\ldots,n\\), \\(y_{i}\\) is the average of \\(x_{i}\\), \\(x_{i-1}\\), \\(x_{i-2}\\). The time series \\(y\\) is called the (3-period) _moving average_ of the time series \\(x\\). Figure 7.6 shows an example.\n\nDocument 5:\n----\n### 9.1 Linear dynamical systems\nSuppose \\(x_{1},x_{2},\\ldots\\) is a sequence of \\(n\\)-vectors. The index (subscript) denotes time or period, and is written as \\(t\\); \\(x_{t}\\), the value of the sequence at time (or period) \\(t\\), is called the _state_ at time \\(t\\). We can think of \\(x_{t}\\) as a vector that changes over time, _i.e._, one that changes dynamically. In this context, the sequence \\(x_{1},x_{2},\\ldots\\) is sometimes called a _trajectory_ or _state trajectory_. We sometimes refer to \\(x_{t}\\) as the _current state_ of the system (implicitly assuming the current time is \\(t\\)), and \\(x_{t+1}\\) as the _next state_, \\(x_{t-1}\\) as the _previous state_, and so on.\nThe state \\(x_{t}\\) can represent a portfolio that changes daily, or the positions and velocities of the parts of a mechanical system, or the quarterly activity of an economy. If \\(x_{t}\\) represents a portfolio that changes daily, \\((x_{5})_{3}\\) is the amount of asset 3 held in the portfolio on (trading) day 5.\nA _linear dynamical system_ is a simple model for the sequence, in which each \\(x_{t+1}\\) is a linear function of \\(x_{t}\\):\n\\[x_{t+1}=A_{t}x_{t},\\quad t=1,2,\\ldots.\\] (9.1)\nHere the \\(n\\times n\\) matrices \\(A_{t}\\) are called the _dynamics matrices_. The equation above is called the _dynamics_ or _update_ equation, since it gives us the next value of \\(x\\), _i.e._, \\(x_{t+1}\\), as a function of the current value \\(x_{t}\\). Often the dynamics matrix does not depend on \\(t\\), in which case the linear dynamical system is called _time-invariant_.\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Let V be the space of all infinite sequences of real numbers. Consider the transformation T(x_0, x_1, x_2, ...) = (x_1, x_2, x_3, ...) from V to V. Is the sequence (1,2,3,...) in the image of T?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_4_2.json",
            "explanation": "solutions/math_algebra_4_2.png",
            "theorem": "image of linear transformations",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "mingyin/linear-dependence3.json",
        "question": "suppose the 10-by-10 matrix A has the form: if i \\neq j, A_{i,j}=a_i*b_j; if i=j,  A_{i,j}=1+a_i*b_j for all 1<=i,j<=10. Here a_i = 1/i, b_i=1/(i+1). Find the determinant of A. return the numeric.",
        "prediction": "1",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nsuppose the 10-by-10 matrix A has the form: if i \\neq j, A_{i,j}=a_i*b_j; if i=j,  A_{i,j}=1+a_i*b_j for all 1<=i,j<=10. Here a_i = 1/i, b_i=1/(i+1). Find the determinant of A. return the numeric.\n\nDocument 1:\n----\n### 15.5 Complexity\nFigure 15.12: RMS training and test errors as a function of the regularization parameter \\(\\lambda\\). _Bottom_. The regularization path. The dashed horizontal lines show the values of the coefficients used to generate the data.\n#### Gram caching\nWe start from the formula (15.3) for the minimizer of the weighted sum objective,\n\\[\\hat{x}=(\\lambda_{1}A_{1}^{T}A_{1}+\\cdots+\\lambda_{k}A_{k}^{T}A_{k})^{-1}( \\lambda_{1}A_{1}^{T}b_{1}+\\cdots+\\lambda_{k}A_{k}^{T}b_{k}).\\]\nThe matrix appearing in the inverse is a weighted sum of the Gram matrices \\(G_{i}=A_{i}^{T}A_{i}\\) associated with the matrices \\(A_{i}\\). We can compute \\(\\hat{x}\\) by forming these Gram matrices \\(G_{i}\\), along with the vectors \\(h_{i}=A_{i}^{T}b_{i}\\), then forming the weighted sums\n\\[G=\\lambda_{1}G_{1}+\\cdots+\\lambda_{k}G_{k},\\qquad h=\\lambda_{1}h_{1}+\\cdots+ \\lambda_{k}h_{k},\\]\nand finally, solving the \\(n\\times n\\) set of equations \\(G\\hat{x}=h\\). Forming \\(G_{i}\\) and \\(h_{i}\\) costs \\(m_{i}n^{2}\\) and \\(2m_{i}n\\) flops, respectively. (We save a factor of two in forming the Gram matrix; see page 182.) Ignoring the second term and adding over \\(i=1,\\ldots,k\\) we get a total of \\(mn^{2}\\) flops. Forming the weighted sums \\(G\\) and \\(h\\) costs \\(2kn^{2}\\) flops. Solving \\(G\\hat{x}=h\\) costs order \\(2n^{3}\\) flops.\n_Gram caching_ is the simple trick of computing \\(G_{i}\\) (and \\(h_{i}\\)) just once, and re-using these matrices and vectors for the \\(L\\) different choices of weights. This leads to a complexity of\n\\[mn^{2}+L(k+2n)n^{2}\\]\nflops. When \\(m\\) is much larger than \\(k+n\\), which is a common occurrence, this cost is smaller than \\(Lmn^{2}\\), the cost for the simple method.\n\nDocument 2:\n----\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n**10.11**: _Trace of matrix-matrix product_. The sum of the diagonal entries of a square matrix is called the _trace_ of the matrix, denoted \\(\\mathbf{tr}(A)\\).\n1. Suppose \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices. Show that \\[\\mathbf{tr}(A^{T}B)=\\sum_{i=1}^{m}\\sum_{j=1}^{n}A_{ij}B_{ij}.\\] What is the complexity of calculating \\(\\mathbf{tr}(A^{T}B)\\)?\n2. The number \\(\\mathbf{tr}(A^{T}B)\\) is sometimes referred to as the inner product of the matrices \\(A\\) and \\(B\\). (This allows us to extend concepts like angle to matrices.) Show that \\(\\mathbf{tr}(A^{T}B)=\\mathbf{tr}(B^{T}A)\\).\n3. Show that \\(\\mathbf{tr}(A^{T}A)=\\left\\|A\\right\\|^{2}\\). In other words, the square of the norm of a matrix is the trace of its Gram matrix.\n\nDocument 3:\n----\nThis gives an alternative method for solving the square set of linear equations \\(Ax=b\\): We first compute the inverse matrix \\(A^{-1}\\), and then the matrix-vector product \\(x=(A^{-1})b\\). This method has a higher flop count than directly solving\nthe equations using algorithm 11.2 (\\(3n^{3}\\) versus \\(2n^{3}\\)), so algorithm 11.2 is the usual method of choice. While the matrix inverse appears in many formulas (such as the solution of a set of linear equations), it is _computed_ far less often.\nSparse linear equations.Systems of linear equations with sparse coefficient matrix arise in many applications. By exploiting the sparsity of the coefficient matrix, these linear equations can be solved far more efficiently than by using the generic algorithm 11.2. One method is to use the same basic algorithm 11.2, replacing the QR factorization with a variant that handles sparse matrices (see page 190). The memory usage and complexity of these methods depends in a complicated way on the sparsity pattern of the coefficient matrix. In order, the memory usage is typically a modest multiple of \\(\\mathbf{nnz}(A)+n\\), the number of scalars required to specify the problem data \\(A\\) and \\(b\\), which is typically much smaller than \\(n^{2}+n\\), the number of scalars required to store \\(A\\) and \\(b\\) if they are not sparse. The flop count for solving sparse linear equations is also typically closer in order to \\(\\mathbf{nnz}(A)\\) than \\(n^{3}\\), the order when the matrix \\(A\\) is not sparse.\n\nDocument 4:\n----\n_Remark_.: Suppose the vertices represent all people on earth, and the graph edges represent acquaintance, _i.e._, \\(A_{ij}=1\\) if person \\(j\\) and person \\(i\\) are acquainted. (This graph is symmetric.) Even though \\(n\\) is measured in billions, the diameter of this acquaintance graph is thought to be quite small, perhaps 6 or 7. In other words, any two people on earth can be connected though a set of 6 or 7 (or fewer) acquaintances. This idea, originally conjectured in the 1920s, is sometimes called _six degrees of separation_.\n**10.32**: _Matrix exponential._ You may know that for any real number \\(a\\), the sequence \\((1+a/k)^{k}\\) converges as \\(k\\to\\infty\\) to the exponential of \\(a\\), denoted \\(\\exp a\\) or \\(e^{a}\\). The _matrix exponential_ of a square matrix \\(A\\) is defined as the limit of the matrix sequence \\((I+A/k)^{k}\\) as \\(k\\to\\infty\\). (It can shown that this sequence always converges.) The matrix exponential arises in many applications, and is covered in more advanced courses on linear algebra.\n1. Find \\(\\exp 0\\) (the zero matrix) and \\(\\exp I\\).\n2. Find \\(\\exp A\\), for \\(A=\\left[\\begin{array}{cc}0&1\\\\ 0&0\\end{array}\\right]\\).\n**10.33**: _Matrix equations._ Consider two \\(m\\times n\\) matrices \\(A\\) and \\(B\\). Suppose that for \\(j=1,\\ldots,n\\), the \\(j\\)th column of \\(A\\) is a linear combination of the first \\(j\\) columns of \\(B\\). How do we express this as a matrix equation? Choose one of the matrix equations below and justify your choice.\n1. \\(A=GB\\) for some upper triangular matrix \\(G\\).\n2. \\(A=BH\\) for some upper triangular matrix \\(H\\).\n3. \\(A=FB\\) for some lower triangular matrix \\(F\\).\n\nDocument 5:\n----\n**16.10**: _Smallest force sequence to move a mass to a given position._ We consider the same setup as the example given on page 343, where the 10-vector \\(f\\) represents a sequence of forces applied to a unit mass over 10 1-second intervals. In that example the goal is to find the smallest force sequence (measured by \\(\\|f\\|^{2}\\)) that achieves zero final velocity and final position one. Here we ask, what is the smallest force sequence that achieves final position one? (We impose no condition on the final velocity.) Explain how to find this force sequence. Compare it to the force sequence found in the example, and give a brief intuitive explanation of the difference. _Remark._ Problems in which the final position of an object is specified, but the final velocity doesn't matter, generally arise in applications that are not socially positive, for example control of missiles.\n**16.11**: _Least distance problem._ A variation on the least norm problem (16.2) is the least distance problem,\n\\[\\begin{array}{ll}\\mbox{minimize}&\\left\\|x-a\\right\\|^{2}\\\\ \\mbox{subject to}&Cx=d,\\end{array}\\]\nwhere the \\(n\\)-vector \\(x\\) is to be determined, the \\(n\\)-vector \\(a\\) is given, the \\(p\\times n\\) matrix \\(C\\) is given, and the \\(p\\)-vector \\(d\\) is given. Show that the solution of this problem is\n\\[\\hat{x}=a-C^{\\dagger}(Ca-d),\\]\nassuming the rows of \\(C\\) are linearly independent. _Hint._ You can argue directly from the KKT equations for the least distance problem, or solve for the variable \\(y=x-a\\) instead of \\(x\\).\n\n\n",
        "answer": 1.9,
        "rationale": {
            "Question": "suppose the 10-by-10 matrix A has the form: if i \\neq j, A_{i,j}=a_i*b_j; if i=j,  A_{i,j}=1+a_i*b_j for all 1<=i,j<=10. Here a_i = 1/i, b_i=1/(i+1). Find the determinant of A. return the numeric.",
            "Answer": 1.9,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 4.5 example 4",
            "id": "mingyin/linear-dependence3.json",
            "explanation": "NONE",
            "theorem": "linear dependence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float"
    },
    {
        "id": "elainewan/math_algebra_7_3.json",
        "question": "Does the following transformation have an eigenvector: Counterclockwise rotation through an angle of 45 degrees followed by a scaling by 2 in R^2.",
        "prediction": "Therefore, the transformation of counterclockwise rotation through an angle of 45 degrees followed by a scaling by 2 in \\( \\mathbb{R}^2 \\) does not have a real eigenvector",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nDoes the following transformation have an eigenvector: Counterclockwise rotation through an angle of 45 degrees followed by a scaling by 2 in R^2.\n\nDocument 1:\n----\nSuppose the 2-vector (or 3-vector) \\(x\\) represents a position in 2-D (or 3-D) space. Several important geometric transformations or mappings from points to points can be expressed as matrix-vector products \\(y=Ax\\), with \\(A\\) a \\(2\\times 2\\) (or \\(3\\times 3\\)) matrix. In the examples below, we consider the mapping from \\(x\\) to \\(y\\), and focus on the 2-D case (for which some of the matrices are simpler to describe).\nScaling.Scaling is the mapping \\(y=ax\\), where \\(a\\) is a scalar. This can be expressed as \\(y=Ax\\) with \\(A=aI\\). This mapping stretches a vector by the factor \\(|a|\\) (or shrinks it when \\(|a|<1\\)), and it flips the vector (reverses its direction) if \\(a<0\\).\nDilation.Dilation is the mapping \\(y=Dx\\), where \\(D\\) is a diagonal matrix, \\(D=\\mathbf{diag}(d_{1},d_{2})\\). This mapping stretches the vector \\(x\\) by different factors along the two different axes. (Or shrinks, if \\(|d_{i}|<1\\), and flips, if \\(d_{i}<0\\).)\nRotation.Suppose that \\(y\\) is the vector obtained by rotating \\(x\\) by \\(\\theta\\) radians counterclockwise. Then we have\n\\[y=\\left[\\begin{array}{cc}\\cos\\theta&-\\sin\\theta\\\\ \\sin\\theta&\\cos\\theta\\end{array}\\right]x.\\] (7.1)\nThis matrix is called (for obvious reasons) a _rotation matrix_.\nReflection.Suppose that \\(y\\) is the vector obtained by reflecting \\(x\\) through the line that passes through the origin, inclined \\(\\theta\\) radians with respect to horizontal. Then we have\n\nDocument 2:\n----\n\\[y=\\left[\\begin{array}{cc}(1/2)(1+\\cos(2\\theta))&(1/2)\\sin(2\\theta)\\\\ (1/2)\\sin(2\\theta)&(1/2)(1-\\cos(2\\theta))\\end{array}\\right]x.\\]\nSome of these geometric transformations are illustrated in figure 7.1.\nFinding the matrix.When a geometric transformation is represented by matrix-vector multiplication (as in the examples above), a simple method to find the matrix is to find its columns. The \\(i\\)th column is the vector obtained by applying the transformation to \\(e_{i}\\). As a simple example consider clockwise rotation by \\(90^{\\circ}\\) in 2-D. Rotating the vector \\(e_{1}=(1,0)\\) by \\(90^{\\circ}\\) gives \\((0,-1)\\); rotating \\(e_{2}=(0,1)\\) by \\(90^{\\circ}\\) gives \\((1,0)\\). So rotation by \\(90^{\\circ}\\) is given by\n\\[y=\\left[\\begin{array}{cc}0&1\\\\ -1&0\\end{array}\\right]x.\\]\nChange of coordinates.In many applications multiple coordinate systems are used to describe locations or positions in 2-D or 3-D. For example in aerospace engineering we can describe a position using _earth-fixed_ coordinates or _body-fixed_ coordinates, where the body refers to an aircraft. Earth-fixed coordinates are with respect to a specific origin, with the three axes pointing East, North, and straight up, respectively. The origin of the body-fixed coordinates is a specific location on the aircraft (typically the center of gravity), and the three axes point forward (along the aircraft body), left (with respect to the aircraft body), and up (with respect to the aircraft body). Suppose the 3-vector \\(x^{\\text{body}}\\) describes a location using the body coordinates, and \\(x^{\\text{earth}}\\) describes the same location in earth-fixed coordinates. These are related by\n\nDocument 3:\n----\n### 7.1 Projection on a line\nLet \\(P(x)\\) denote the projection of the 2-D point (2-vector) \\(x\\) onto the line that passes through \\((0,0)\\) and \\((1,3)\\). (This means that \\(P(x)\\) is the point on the line that is closest to \\(x\\); see exercise 3.12.) Show that \\(P\\) is a linear function, and give the matrix \\(A\\) for which \\(P(x)=Ax\\) for any \\(x\\).\n_3-D rotation._ Let \\(x\\) and \\(y\\) be 3-vectors representing positions in 3-D. Suppose that the vector \\(y\\) is obtained by rotating the vector \\(x\\) about the vertical axis (_i.e._, \\(e_{3}\\)) by \\(45^{\\circ}\\) (counterclockwise, _i.e._, from \\(e_{1}\\) toward \\(e_{2}\\)). Find the \\(3\\times 3\\) matrix \\(A\\) for which \\(y=Ax\\). _Hint._ Determine the three columns of \\(A\\) by finding the result of the transformation on the unit vectors \\(e_{1},e_{2},e_{3}\\).\n_3-Trimming a vector._ Find a matrix \\(A\\) for which \\(Ax=(x_{2},\\ldots,x_{n-1})\\), where \\(x\\) is an \\(n\\)-vector. (Be sure to specify the size of \\(A\\), and describe all its entries.)\n_3-Down-sampling and up-conversion._ We consider \\(n\\)-vectors \\(x\\) that represent signals, with \\(x_{k}\\) the value of the signal at time \\(k\\) for \\(k=1,\\ldots,n\\). Below we describe two functions of \\(x\\) that produce new signals \\(f(x)\\). For each function, give a matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\).\n\nDocument 4:\n----\n* _When is the outer product symmetric?_ Let \\(a\\) and \\(b\\) be \\(n\\)-vectors. The inner product is symmetric, _i.e._, we have \\(a^{T}b=b^{T}a\\). The outer product of the two vectors is generally _not_ symmetric; that is, we generally have \\(ab^{T}\\neq ba^{T}\\). What are the conditions on \\(a\\) and \\(b\\) under which \\(ab=ba^{T}\\)? You can assume that all the entries of \\(a\\) and \\(b\\) are nonzero. (The conclusion you come to will hold even when some entries of \\(a\\) or \\(b\\) are zero.) _Hint_. Show that \\(ab^{T}=ba^{T}\\) implies that \\(a_{i}/b_{i}\\) is a constant (_i.e._, independent of \\(i\\)).\n* _Product of rotation matrices_. Let \\(A\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\theta\\) radians, defined in (7.1), and let \\(B\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\omega\\) radians. Show that \\(AB\\) is also a rotation matrix, and give the angle by which it rotates vectors. Verify that \\(AB=BA\\) in this case, and give a simple English explanation.\n* _Two rotations_. Two 3-vectors \\(x\\) and \\(y\\) are related as follows. First, the vector \\(x\\) is rotated \\(40^{\\circ}\\) around the \\(e_{3}\\) axis, counterclockwise (from \\(e_{1}\\) toward \\(e_{2}\\)), to obtain the 3-vector \\(z\\). Then, \\(z\\) is rotated \\(20^{\\circ}\\) around the \\(e_{1}\\) axis, counterclockwise (from \\(e_{2}\\) toward \\(e_{3}\\)), to form \\(y\\). Find the \\(3\\times 3\\) matrix \\(A\\) for which \\(y=Ax\\). Verify that \\(A\\) is an orthogonal matrix. _Hint_. Express \\(A\\) as a product of two matrices, which carry out the two rotations described above.\n\nDocument 5:\n----\nThe angle between \\(a\\) and \\(b\\) is written as \\(\\angle(a,b)\\), and is sometimes expressed in degrees. (The default angle unit is _radians_; \\(360^{\\circ}\\) is \\(2\\pi\\) radians.) For example, \\(\\angle(a,b)=60^{\\circ}\\) means \\(\\angle(a,b)=\\pi/3\\), _i.e._, \\(a^{T}b=(1/2)\\|a\\|\\|b\\|\\).\nThe angle coincides with the usual notion of angle between vectors, when they have dimension two or three, and they are thought of as displacements from a\ncommon point. For example, the angle between the vectors \\(a=(1,2,-1)\\) and \\(b=(2,0,-3)\\) is\n\\[\\arccos\\left(\\frac{5}{\\sqrt{6}\\,\\sqrt{13}}\\right)=\\arccos(0.5661)=0.9690=55.52^{\\circ}\\]\n(to 4 digits). But the definition of angle is more general; we can refer to the angle between two vectors with dimension 100.\nThe angle is a symmetric function of \\(a\\) and \\(b\\): We have \\(\\angle(a,b)=\\angle(b,a)\\). The angle is not affected by scaling each of the vectors by a positive scalar: We have, for any vectors \\(a\\) and \\(b\\), and any positive numbers \\(\\alpha\\) and \\(\\beta\\),\n\\[\\angle(\\alpha a,\\beta b)=\\angle(a,b).\\]\nAcute and obtuse angles.Angles are classified according to the sign of \\(a^{T}b\\). Suppose \\(a\\) and \\(b\\) are nonzero vectors of the same size.\n* If the angle is \\(\\pi/2=90^{\\circ}\\), _i.e._, \\(a^{T}b=0\\), the vectors are said to be _orthogonal_. We write \\(a\\perp b\\) if \\(a\\) and \\(b\\) are orthogonal. (By convention, we also say that a zero vector is orthogonal to any vector.)\n* If the angle is zero, which means \\(a^{T}b=\\|a\\|\\|b\\|\\), the vectors are _aligned_. Each vector is a positive multiple of the other.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Does the following transformation have an eigenvector: Counterclockwise rotation through an angle of 45 degrees followed by a scaling by 2 in R^2.",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_3.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "wenhuchen/factor's_theory.json",
        "question": "Is x-1 a factor of 2*x^4+3*x^2-5x+7?",
        "prediction": "False",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs x-1 a factor of 2*x^4+3*x^2-5x+7?\n\nDocument 1:\n----\nOver-determined and under-determined systems of linear equations.The set of linear equations is called _over-determined_ if \\(m>n\\), _under-determined_ if \\(m<n\\), and _square_ if \\(m=n\\); these correspond to the coefficient matrix being tall, wide, and square, respectively. When the system of linear equations is over-determined, there are more equations than variables or unknowns. When the system of linear equations is under-determined, there are more unknowns than equations. When the system of linear equations is square, the numbers of unknowns and equations is the same. A set of equations with zero right-hand side, \\(Ax=0\\), is called a _homogeneous_ set of equations. Any homogeneous set of equations has \\(x=0\\) as a solution.\nIn chapter 11 we will address the question of how to determine if a system of linear equations has a solution, and how to find one when it does. For now, we give a few interesting examples.\n#### Examples\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\n\nDocument 2:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\nBalancing chemical reactions.(See page 154 for background.) We consider the problem of balancing the chemical reaction\n\\[a_{1}\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+a_{2}\\mathrm{Fe}^{2+}+a_{3}\\mathrm{H} ^{+}\\longrightarrow b_{1}\\mathrm{Cr}^{3+}+b_{2}\\mathrm{Fe}^{3+}+b_{3} \\mathrm{H}_{2}\\mathrm{O},\\]\n\nDocument 3:\n----\n_i.e._, any left inverse of \\(A\\) is equal to any right inverse of \\(A\\). This implies that the left inverse is unique: If we have \\(A\\tilde{X}=I\\), then the argument above tells us that \\(\\tilde{X}=Y\\), so we have \\(\\tilde{X}=X\\), _i.e._, there is only one right inverse of \\(A\\). A similar argument shows that \\(Y\\) (which is the same as \\(X\\)) is the only left inverse of \\(A\\).\nWhen a matrix \\(A\\) has both a left inverse \\(Y\\) and a right inverse \\(X\\), we call the matrix \\(X=Y\\) simply the _inverse_ of \\(A\\), and denote it as \\(A^{-1}\\). We say that \\(A\\) is _invertible_ or _nonsingular_. A square matrix that is not invertible is called _singular_.\nDimensions of invertible matrices.Invertible matrices must be square, since tall matrices are not right-invertible, while wide matrices are not left-invertible. A matrix \\(A\\) and its inverse (if it exists) satisfy\n\\[AA^{-1}=A^{-1}A=I.\\]\nIf \\(A\\) has inverse \\(A^{-1}\\), then the inverse of \\(A^{-1}\\) is \\(A\\); in other words, we have \\((A^{-1})^{-1}=A\\). For this reason we say that \\(A\\) and \\(A^{-1}\\) are inverses (of each other).\nSolving linear equations with the inverse.Consider the square system of \\(n\\) linear equations with \\(n\\) variables, \\(Ax=b\\). If \\(A\\) is invertible, then for any \\(n\\)-vector \\(b\\),\n\\[x=A^{-1}b\\] (11.1)\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n\nDocument 4:\n----\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\nDocument 5:\n----\nthen (evidently) \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); conversely, if \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\), the identity (5.4) holds for \\(y\\).\nOrthonormal basis.If the \\(n\\)-vectors \\(a_{1},\\ldots,a_{n}\\) are orthonormal, they are linearly independent, and therefore also a basis. In this case they are called an _orthonormal basis_. The three examples above (on page 5.2) are orthonormal bases.\nIf \\(a_{1},\\ldots,a_{n}\\) is an orthonormal basis, then we have, for any \\(n\\)-vector \\(x\\), the identity\n\\[x=(a_{1}^{T}x)a_{1}+\\cdots+(a_{n}^{T}x)a_{n}.\\] (5.5)\nTo see this, we note that since \\(a_{1},\\ldots,a_{n}\\) are a basis, \\(x\\) can be expressed as a linear combination of them; hence the identity (5.4) above holds. The equation above is sometimes called the _orthonormal expansion formula_; the right-hand side is called the _expansion of \\(x\\) in the basis \\(a_{1},\\ldots,a_{n}\\)_. It shows that any \\(n\\)-vector can be expressed as a linear combination of the basis elements, with the coefficients given by taking the inner product of \\(x\\) with the elements of the basis.\nAs an example, we express the 3-vector \\(x=(1,2,3)\\) as a linear combination of the orthonormal basis given in (5.3). The inner products of \\(x\\) with these vectors\nare\n\\[\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right]^{T}x=-3,\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ 1\\\\ 0\\end{array}\\right]^{T}x=\\frac{3}{\\sqrt{2}},\\qquad\\frac{1}{\\sqrt{2}}\\left[ \\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right]^{T}x=\\frac{-1}{\\sqrt{2}}.\\]\nIt can be verified that the expansion of \\(x\\) in this basis is\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is x-1 a factor of 2*x^4+3*x^2-5x+7?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "self",
            "id": "wenhuchen/factor's_theory.json",
            "explanation": "NONE",
            "theorem": "factor's theorem",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "wenhuchen/cramer's_rule1.json",
        "question": "For the two linear equations $2 * x + 3 * y = 10$ and $4 * x + 4 * y = 12$ iwth variables x and y. Use cramer's rule to solve these two variables.",
        "prediction": "\\((-1, 4)\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor the two linear equations $2 * x + 3 * y = 10$ and $4 * x + 4 * y = 12$ iwth variables x and y. Use cramer's rule to solve these two variables.\n\nDocument 1:\n----\nIn summary, a right inverse can be used to find \\(a\\) solution of a square or under-determined set of linear equations, for any vector \\(b\\).\nExamples.Consider the matrix appearing in the example above on page 199,\n\\[A=\\left[\\begin{array}{rr}-3&-4\\\\ 4&6\\\\ 1&1\\end{array}\\right]\\]\nand the two left inverses\n\\[B=\\frac{1}{9}\\left[\\begin{array}{rr}-11&-10&16\\\\ 7&8&-11\\end{array}\\right],\\qquad C=\\frac{1}{2}\\left[\\begin{array}{rr}0&-1&6 \\\\ 0&1&-4\\end{array}\\right].\\]\n* The over-determined linear equations \\(Ax=(1,-2,0)\\) have the unique solution \\(x=(1,-1)\\), which can be obtained from _either_ left inverse: \\[x=B(1,-2,0)=C(1,-2,0).\\]\n* The over-determined linear equations \\(Ax=(1,-1,0)\\) do not have a solution, since \\(x=C(1,-1,0)=(1/2,-1/2)\\) does not satisfy \\(Ax=(1,-1,0)\\).\n* The under-determined linear equations \\(A^{T}y=(1,2)\\) has (different) solutions \\[B^{T}(1,2)=(1/3,2/3,38/9),\\qquad C^{T}(1,2)=(0,1/2,-1).\\] (Recall that \\(B^{T}\\) and \\(C^{T}\\) are both right inverses of \\(A^{T}\\).) We can find a solution of \\(A^{T}y=b\\) for any vector \\(b\\).\nLeft and right inverse of matrix product.Suppose \\(A\\) and \\(D\\) are compatible for the matrix product \\(AD\\) (_i.e._, the number of columns in \\(A\\) is equal to the number of rows in \\(D\\).) If \\(A\\) has a right inverse \\(B\\) and \\(D\\) has a right inverse \\(E\\), then \\(EB\\) is a right inverse of \\(AD\\). This follows from \\[(AD)(EB)=A(DE)B=A(IB)=AB=I.\\] If \\(A\\) has a left inverse \\(C\\) and \\(D\\) has a left inverse \\(F\\), then \\(FC\\) is a left inverse of \\(AD\\). This follows from \\[(FC)(AD)=F(CA)D=FD=I.\\]\n\nDocument 2:\n----\n* A list consisting of a single vector is linearly dependent only if the vector is zero. It is linearly independent only if the vector is nonzero.\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\n\nDocument 3:\n----\n* _Recursive least squares_ is the following algorithm. For \\(k=n,\\ldots,m\\), compute \\(G^{(k+1)}\\) and \\(h^{(k+1)}\\) using (b); then compute \\(\\hat{x}^{(k)}\\) using (a). Work out the total flop count for this method, keeping only dominant terms. (You can include the cost of computing \\(G^{(n)}\\) and \\(h^{(n)}\\), which should be negligible in the total.) Compare to the flop count for the naive method.\n_Remark_.: A further trick called the matrix inversion lemma (which is beyond the scope of this book) can be used to reduce the complexity of recursive least squares to order \\(mn^{2}\\).\n**12.15**: _Minimizing a squared norm plus an affine function._ A generalization of the least squares problem (12.1) adds an affine function to the least squares objective,\n\\[\\text{minimize}\\quad\\|Ax-b\\|^{2}+c^{T}x+d,\\]\nwhere the \\(n\\)-vector \\(x\\) is the variable to be chosen, and the (given) data are the \\(m\\times n\\) matrix \\(A\\), the \\(m\\)-vector \\(b\\), the \\(n\\)-vector \\(c\\), and the number \\(d\\). We will use the same assumption we use in least squares: The columns of \\(A\\) are linearly independent. This generalized problem can be solved by reducing it to a standard least squares problem, using a trick called _completing the square_.\nShow that the objective of the problem above can be expressed in the form\n\\[\\|Ax-b\\|^{2}+c^{T}x+d=\\|Ax-b+f\\|^{2}+g,\\]\nfor some \\(m\\)-vector \\(f\\) and some constant \\(g\\). It follows that we can solve the generalized least squares problem by minimizing \\(\\|Ax-(b-f)\\|\\), an ordinary least squares problem with solution \\(\\hat{x}=A^{\\dagger}(b-f)\\).\n\nDocument 4:\n----\n* [19] M. Cacciari, G. P. Salam, and G. Soyez, \"The anti-jet clustering algorithm\", _JHEP_ **04** (2008) 063, doi:10.1088/1126-6708/2008/04/063, arXiv:0802.1189.\n[MISSING_PAGE_POST]\n### 19.4 Nonlinear control\nFigure 19.9: Feasibility and optimality condition residuals in the augmented Lagrangian algorithm for computing the trajectories in figure 19.7.\n## 19 Constrained nonlinear least squares\n### 19.1 Projection on a curve\nWe consider a constrained nonlinear least squares problem with three variables \\(x=(x_{1},x_{2},x_{3})\\) and two equations:\n\\[\\begin{array}{ll}\\mbox{minimize}&(x_{1}-1)^{2}+(x_{2}-1)^{2}+(x_{3}-1)^{2}\\\\ \\mbox{subject to}&x_{1}^{2}+0.5x_{2}^{2}+x_{3}^{2}-1=0\\\\ &0.8x_{1}^{2}+2.5x_{2}^{2}+x_{3}^{2}+2x_{1}x_{3}-x_{1}-x_{2}-x_{3}-1=0.\\end{array}\\]\nThe solution is the point closest to \\((1,1,1)\\) on the nonlinear curve defined by the two equations.\n1. Solve the problem using the augmented Lagrangian method. You can start the algorithm at \\(x^{(1)}=0\\), \\(z^{(1)}=0\\), \\(\\mu^{(1)}=1\\), and start each run of the Levenberg-Marquardt method with \\(\\lambda^{(1)}=1\\). Stop the augmented Lagrangian method when the feasibility residual \\(\\|g(x^{(k)})\\|\\) and the optimality condition residual \\[\\|2Df(x^{(k)})^{T}f(x^{(k)})+Dg(x^{(k)})^{T}z^{(k)}\\|\\] are less than \\(10^{-5}\\). Make a plot of the two residuals and of the penalty parameter \\(\\mu\\) versus the cumulative number of Levenberg-Marquardt iterations.\n2. Solve the problem using the penalty method, started at \\(x^{(1)}=0\\) and \\(\\mu^{(1)}=1\\), and with the same stopping condition. Compare the convergence and the value of the penalty parameter with the results for the augmented Lagrangian method in part (a).\n### 19.2 Portfolio optimization with downside risk\n\nDocument 5:\n----\nwhere \\(c_{1},c_{2},c_{3},d_{1},d_{2}\\) are coefficients. ('Rational' refers to the fact that \\(f\\) is a ratio of polynomials. Another name for \\(f\\) is _bi-quadratic_.) Consider the interpolation conditions\n\\[f(t_{i})=y_{i},\\quad i=1,\\ldots,K,\\]\nwhere \\(t_{i}\\) and \\(y_{i}\\) are given numbers. Express the interpolation conditions as a set of linear equations in the vector of coefficients \\(\\theta=(c_{1},c_{2},c_{3},d_{1},d_{2})\\), as \\(A\\theta=b\\). Give \\(A\\) and \\(b\\), and their dimensions.\n**8.9**: _Required nutrients._ We consider a set of \\(n\\) basic foods (such as rice, beans, apples) and a set of \\(m\\) nutrients or components (such as protein, fat, sugar, vitamin C). Food \\(j\\) has a cost given by \\(c_{j}\\) (say, in dollars per gram), and contains an amount \\(N_{ij}\\) of nutrient \\(i\\) (per gram). (The nutrients are given in some appropriate units, which can depend on the particular nutrient.) A daily diet is represented by an \\(n\\)-vector \\(d\\), with \\(d_{i}\\) the daily intake (in grams) of food \\(i\\). Express the condition that a diet \\(d\\) contains the total nutrient amounts given by the \\(m\\)-vector \\(n^{\\rm des}\\), and has a total cost \\(B\\) (the budget) as a set of linear equations in the variables \\(d_{1},\\ldots,d_{n}\\). (The entries of \\(d\\) must be nonnegative, but we ignore this issue here.)\n**8.10**: _Blending crude oil._ A set of \\(K\\) different types of crude oil are blended (mixed) together in proportions \\(\\theta_{1},\\ldots,\\theta_{K}\\). These numbers sum to one; they must also be nonnegative, but we will ignore that requirement here. Associated with crude oil type \\(k\\) is an \\(n\\)-vector \\(c_{k}\\) that gives its concentration of \\(n\\) different constituents, such as specific hydrocarbons. Find a set of linear equations on the blending coefficients, \\(A\\theta=b\\), that expresses the requirement that the blended crude oil achieves a target set of constituent concentrations, given by the \\(n\\)-vector \\(c^{\\rm tsr}\\). (Include the condition that \\(\\theta_{1}+\\cdots+\\theta_{K}=1\\) in your equations.)\n\n\n",
        "answer": [
            -1,
            4
        ],
        "rationale": {
            "Question": "For the two linear equations $2 * x + 3 * y = 10$ and $4 * x + 4 * y = 12$ iwth variables x and y. Use cramer's rule to solve these two variables.",
            "Answer": [
                -1,
                4
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/cramer's_rule1.json",
            "explanation": "NONE",
            "theorem": "cramer's rule",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "elainewan/math_algebra_6_3.json",
        "question": "What is the determinant of matrix [[0, 1, 2], [7, 8, 3], [6, 5, 4]]?",
        "prediction": "\\(\\boxed{64}\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the determinant of matrix [[0, 1, 2], [7, 8, 3], [6, 5, 4]]?\n\nDocument 1:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\nDocument 2:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\nDocument 3:\n----\n_3-Down-sampling and up-conversion._ We consider \\(n\\)-vectors \\(x\\) that represent signals, with \\(x_{k}\\) the value of the signal at time \\(k\\) for \\(k=1,\\ldots,n\\). Below we describe two functions of \\(x\\) that produce new signals \\(f(x)\\). For each function, give a matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\).\n* \\(2\\times\\)_downsampling._ We assume \\(n\\) is even and define \\(f(x)\\) as the \\(n/2\\)-vector \\(y\\) with elements \\(y_{k}=x_{2k}\\). To simplify your notation you can assume that \\(n=8\\), _i.e._, \\[f(x)=(x_{2},\\,x_{4},\\,x_{6},\\,x_{8}).\\] (On page 131 we describe a different type of down-sampling, that uses the average of pairs of original values.)\n* \\(2\\times\\)_up-conversion with linear interpolation_. We define \\(f(x)\\) as the \\((2n-1)\\)-vector \\(y\\) with elements \\(y_{k}=x_{(k+1)/2}\\) if \\(k\\) is odd and \\(y_{k}=(x_{k/2}+x_{k/2+1})/2\\) if \\(k\\) is even. To simplify your notation you can assume that \\(n=5\\), _i.e._, \\[f(x)=\\left(x_{1},\\,\\frac{x_{1}+x_{2}}{2},\\,x_{2},\\,\\frac{x_{2}+x_{3}}{2},\\,x_ {3},\\,\\frac{x_{3}+x_{4}}{2},\\,x_{4},\\,\\frac{x_{4}+x_{5}}{2},\\,x_{5}\\right).\\]\n_4-T transpose of selector matrix._ Suppose the \\(m\\times n\\) matrix \\(A\\) is a selector matrix. Describe the relation between the \\(m\\)-vector \\(u\\) and the \\(n\\)-vector \\(v=A^{T}u\\).\n_4-Rows of incidence matrix._ Show that the rows of the incidence matrix of a graph are always linearly dependent. _Hint._ Consider the sum of the rows.\n_4-T Incidence matrix of reversed graph._ (See exercise 6.5.) Suppose \\(A\\) is the incidence matrix of a graph. The reversed graph is obtained by reversing the directions of all the edges of the original graph. What is the incidence matrix of the reversed graph? (Express your answer in terms of \\(A\\).)\n\nDocument 4:\n----\n* \\(A\\), \\(B\\), and \\(C\\) have the same number of rows.\n* \\(B\\) is a tall matrix.\n* _Block matrix notation_. Consider the block matrix \\[A=\\left[\\begin{array}{ccc}I&B&0\\\\ B^{T}&0&0\\\\ 0&0&BB^{T}\\end{array}\\right],\\] where \\(B\\) is \\(10\\times 5\\). What are the dimensions of the four zero matrices and the identity matrix in the definition of \\(A\\)? What are the dimensions of \\(A\\)?\n* _When is the outer product symmetric?_ Let \\(a\\) and \\(b\\) be \\(n\\)-vectors. The inner product is symmetric, _i.e._, we have \\(a^{T}b=b^{T}a\\). The outer product of the two vectors is generally _not_ symmetric; that is, we generally have \\(ab^{T}\\neq ba^{T}\\). What are the conditions on \\(a\\) and \\(b\\) under which \\(ab=ba^{T}\\)? You can assume that all the entries of \\(a\\) and \\(b\\) are nonzero. (The conclusion you come to will hold even when some entries of \\(a\\) or \\(b\\) are zero.) _Hint_. Show that \\(ab^{T}=ba^{T}\\) implies that \\(a_{i}/b_{i}\\) is a constant (_i.e._, independent of \\(i\\)).\n* _Product of rotation matrices_. Let \\(A\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\theta\\) radians, defined in (7.1), and let \\(B\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\omega\\) radians. Show that \\(AB\\) is also a rotation matrix, and give the angle by which it rotates vectors. Verify that \\(AB=BA\\) in this case, and give a simple English explanation.\n\nDocument 5:\n----\n\\[y=x_{1}a_{1}+x_{2}a_{2}+\\cdots+x_{n}a_{n}.\\]\nThis shows that \\(y=Ax\\) is a linear combination of the columns of \\(A\\); the coefficients in the linear combination are the elements of \\(x\\).\nGeneral examples.In the examples below, \\(A\\) is an \\(m\\times n\\) matrix and \\(x\\) is an \\(n\\)-vector.\n* _Zero matrix._ When \\(A=0\\), we have \\(Ax=0\\). In other words, \\(0x=0\\). (The left-hand \\(0\\) is an \\(m\\times n\\) matrix, and the right-hand zero is an \\(m\\)-vector.)\n* _Identity._ We have \\(Ix=x\\) for any vector \\(x\\). (The identity matrix here has dimension \\(n\\times n\\).) In other words, multiplying a vector by the identity matrix gives the same vector.\n* _Picking out columns and rows._ An important identity is \\(Ae_{j}=a_{j}\\), the \\(j\\)th column of \\(A\\). Multiplying a unit vector by a matrix 'picks out' one of the columns of the matrix. \\(A^{T}e_{i}\\), which is an \\(n\\)-vector, is the \\(i\\)th row of \\(A\\), transposed. (In other words, \\((A^{T}e_{i})^{T}\\) is the \\(i\\)th row of \\(A\\).)\n* _Summing or averaging columns or rows._ The \\(m\\)-vector \\(A\\mathbf{1}\\) is the sum of the columns of \\(A\\); its \\(i\\)th entry is the sum of the entries in the \\(i\\)th row of \\(A\\). The \\(m\\)-vector \\(A(\\mathbf{1}/n)\\) is the average of the columns of \\(A\\); its \\(i\\)th entry is the average of the entries in the \\(i\\)th row of \\(A\\). In a similar way, \\(A^{T}\\mathbf{1}\\) is an \\(n\\)-vector, whose \\(j\\)th entry is the sum of the entries in the \\(j\\)th column of \\(A\\).\n\n\n",
        "answer": -36,
        "rationale": {
            "Question": "What is the determinant of matrix [[0, 1, 2], [7, 8, 3], [6, 5, 4]]?",
            "Answer": -36,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_3.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "wenhuchen/eigen_value1.json",
        "question": "For matrix A = [[5, 4], [1, 2]], what are its eigen values?",
        "prediction": "the eigenvalues of matrix \\(A = \\begin{bmatrix} 5 & 4 \\\\ 1 & 2 \\end{bmatrix}\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[5, 4], [1, 2]], what are its eigen values?\n\nDocument 1:\n----\n* \\(A\\), \\(B\\), and \\(C\\) have the same number of rows.\n* \\(B\\) is a tall matrix.\n* _Block matrix notation_. Consider the block matrix \\[A=\\left[\\begin{array}{ccc}I&B&0\\\\ B^{T}&0&0\\\\ 0&0&BB^{T}\\end{array}\\right],\\] where \\(B\\) is \\(10\\times 5\\). What are the dimensions of the four zero matrices and the identity matrix in the definition of \\(A\\)? What are the dimensions of \\(A\\)?\n* _When is the outer product symmetric?_ Let \\(a\\) and \\(b\\) be \\(n\\)-vectors. The inner product is symmetric, _i.e._, we have \\(a^{T}b=b^{T}a\\). The outer product of the two vectors is generally _not_ symmetric; that is, we generally have \\(ab^{T}\\neq ba^{T}\\). What are the conditions on \\(a\\) and \\(b\\) under which \\(ab=ba^{T}\\)? You can assume that all the entries of \\(a\\) and \\(b\\) are nonzero. (The conclusion you come to will hold even when some entries of \\(a\\) or \\(b\\) are zero.) _Hint_. Show that \\(ab^{T}=ba^{T}\\) implies that \\(a_{i}/b_{i}\\) is a constant (_i.e._, independent of \\(i\\)).\n* _Product of rotation matrices_. Let \\(A\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\theta\\) radians, defined in (7.1), and let \\(B\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\omega\\) radians. Show that \\(AB\\) is also a rotation matrix, and give the angle by which it rotates vectors. Verify that \\(AB=BA\\) in this case, and give a simple English explanation.\n\nDocument 2:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\nDocument 3:\n----\n* Find a nondiagonal \\(2\\times 2\\) matrix \\(A\\) that satisfies \\(A^{2}=I\\). This means that in general there are even more squareroots of \\(I_{n}\\) than you found in part (a).\n* _Circular shift matrices._ Let \\(A\\) be the \\(5\\times 5\\) matrix \\[A=\\left[\\begin{array}{cccc}0&0&0&0&1\\\\ 1&0&0&0&0\\\\ 0&1&0&0&0\\\\ 0&0&1&0&0\\\\ 0&0&0&1&0\\end{array}\\right].\\] 1. How is \\(Ax\\) related to \\(x\\)? Your answer should be in English. _Hint._ See exercise title. 2. What is \\(A^{5}\\)? _Hint._ The answer should make sense, given your answer to part (a).\n* _Dynamics of an economy._ Let \\(x_{1},x_{2},\\ldots\\) be \\(n\\)-vectors that give the level of economic activity of a country in years \\(1,2,\\ldots\\), in \\(n\\) different sectors (like energy, defense, manufacturing). Specifically, \\((x_{t})_{i}\\) is the level of economic activity in economic sector \\(i\\) (say, in billions of dollars) in year \\(t\\). A common model that connects these economic activity vectors is \\(x_{t+1}=Bx_{t}\\), where \\(B\\) is an \\(n\\times n\\) matrix. (See exercise 9.2.) Five a matrix expression for the total economic activity across all sectors in year \\(t=6\\), in terms of the matrix \\(B\\) and the vector of initial activity levels \\(x_{1}\\). Suppose you can increase economic activity in year \\(t=1\\) by some fixed amount (say, one billion dollars) in _one_ sector, by government spending. How should you choose which sector to stimulate so as to maximize the total economic output in year \\(t=6\\)?\n\nDocument 4:\n----\nAn important attribute of a matrix is its _size_ or _dimensions_, _i.e._, the numbers of rows and columns. The matrix above has 3 rows and 4 columns, so its size is \\(3\\times 4\\). A matrix of size \\(m\\times n\\) is called an \\(m\\times n\\) matrix.\nThe _elements_ (or _entries_ or _coefficients_) of a matrix are the values in the array. The \\(i,j\\) element is the value in the \\(i\\)th row and \\(j\\)th column, denoted by double subscripts: the \\(i,j\\) element of a matrix \\(A\\) is denoted \\(A_{ij}\\) (or \\(A_{i,j}\\), when \\(i\\) or \\(j\\) is more than one digit or character). The positive integers \\(i\\) and \\(j\\) are called the (row and column) _indices_. If \\(A\\) is an \\(m\\times n\\) matrix, then the row index \\(i\\) runs from 1 to \\(m\\) and the column index \\(j\\) runs from 1 to \\(n\\). Row indices go from top to bottom, so row 1 is the top row and row \\(m\\) is the bottom row. Column indices go from left to right, so column 1 is the left column and column \\(n\\) is the right column.\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\n\nDocument 5:\n----\n* _Entries of matrix triple product_. (See page 10.) Suppose \\(A\\) has dimensions \\(m\\times n\\), \\(B\\) has dimensions \\(n\\times p\\), \\(C\\) has dimensions \\(p\\times q\\), and let \\(D=ABC\\). Show that \\[D_{ij}=\\sum_{k=1}^{n}\\sum_{l=1}^{p}A_{ik}B_{kl}C_{lj}.\\] This is the formula analogous to (10.1) for the product of two matrices.\n* _Multiplication by a diagonal matrix_. Suppose that \\(A\\) is an \\(m\\times n\\) matrix, \\(D\\) is a diagonal matrix, and \\(B=DA\\). Describe \\(B\\) in terms of \\(A\\) and the entries of \\(D\\). You can refer to the rows or columns or entries of \\(A\\).\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n\n\n",
        "answer": [
            1,
            6
        ],
        "rationale": {
            "Question": "For matrix A = [[5, 4], [1, 2]], what are its eigen values?",
            "Answer": [
                1,
                6
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/eigen_value1.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "elainewan/math_algebra_5.json",
        "question": "Find the orthogonal projection of 9e_1 onto the subspace of R^4 spanned by [2, 2, 1, 0] and [-2, 2, 0, 1].",
        "prediction": "the orthogonal projection of \\( 9e_1 \\) onto the subspace spanned by \\( \\textbf{v}_1 \\) and \\( \\textbf{v}_2 \\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFind the orthogonal projection of 9e_1 onto the subspace of R^4 spanned by [2, 2, 1, 0] and [-2, 2, 0, 1].\n\nDocument 1:\n----\n4. Express \\(2x_{1}^{2}-3x_{1}x_{2}-x_{2}^{2}\\) as a quadratic form, with symmetric coefficient matrix \\(A\\).\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n**10.38**: _Orthogonal matrix with nonnegative entries._ Suppose the \\(n\\times n\\) matrix \\(A\\) is orthogonal, and all of its entries are nonnegative, _i.e._, \\(A_{ij}\\geq 0\\) for \\(i,j=1,\\ldots,n\\). Show that \\(A\\) must be a permutation matrix, _i.e._, each entry is either \\(0\\) or \\(1\\), each row has exactly one entry with value one, and each column has exactly one entry with value one. (See page 132.)\n**10.39**: _Gram matrix and QR factorization._ Suppose the matrix \\(A\\) has linearly independent columns and QR factorization \\(A=QR\\). What is the relationship between the Gram matrix of \\(A\\) and the Gram matrix of \\(R\\)? What can you say about the angles between the columns of \\(A\\) and the angles between the columns of \\(R\\)?\n\nDocument 2:\n----\nExample.We define three vectors\n\\[a_{1}=(-1,1,-1,1),\\qquad a_{2}=(-1,3,-1,3),\\qquad a_{3}=(1,3,5,7).\\]\nApplying the Gram-Schmidt algorithm gives the following results.\n* \\(i=1\\). We have \\(\\|\\tilde{q}_{1}\\|=2\\), so \\[q_{1}=\\frac{1}{\\|\\tilde{q}_{1}\\|}\\tilde{q}_{1}=(-1/2,1/2,-1/2,1/2),\\] which is simply \\(a_{1}\\) normalized.\n* \\(i=2\\). We have \\(q_{1}^{T}a_{2}=4\\), so \\[\\tilde{q}_{2}=a_{2}-(q_{1}^{T}a_{2})q_{1}=\\left[\\begin{array}{c}-1\\\\ 3\\\\ -1\\\\ 3\\end{array}\\right]-4\\left[\\begin{array}{c}-1/2\\\\ 1/2\\\\ -1/2\\\\ 1/2\\end{array}\\right]=\\left[\\begin{array}{c}1\\\\ 1\\\\ 1\\\\ 1\\end{array}\\right],\\]\nwhich is indeed orthogonal to \\(q_{1}\\) (and \\(a_{1}\\)). It has norm \\(\\|\\tilde{q}_{2}\\|=2\\); normalizing it gives \\[q_{2}=\\frac{1}{\\|\\tilde{q}_{2}\\|}\\tilde{q}_{2}=(1/2,1/2,1/2,1/2).\\]\n* \\(i=3\\). We have \\(q_{1}^{T}a_{3}=2\\) and \\(q_{2}^{T}a_{3}=8\\), so \\[\\tilde{q}_{3} = a_{3}-(q_{1}^{T}a_{3})q_{1}-(q_{2}^{T}a_{3})q_{2}\\] \\[= \\left[\\begin{array}{c}1\\\\ 3\\\\ 5\\\\ 7\\end{array}\\right]-2\\left[\\begin{array}{c}-1/2\\\\ 1/2\\\\ -1/2\\\\ 1/2\\end{array}\\right]-8\\left[\\begin{array}{c}1/2\\\\ 1/2\\\\ 1/2\\\\ 1/2\\end{array}\\right]\\] \\[= \\left[\\begin{array}{c}-2\\\\ -2\\\\ 2\\\\ 2\\end{array}\\right],\\] which is orthogonal to \\(q_{1}\\) and \\(q_{2}\\) (and \\(a_{1}\\) and \\(a_{2}\\)). We have \\(\\|\\tilde{q}_{3}\\|=4\\), so the normalized vector is \\[q_{3}=\\frac{1}{\\|\\tilde{q}_{3}\\|}\\tilde{q}_{3}=(-1/2,-1/2,1/2,1/2).\\]\nCompletion of the Gram-Schmidt algorithm without early termination tells us that the vectors \\(a_{1}\\), \\(a_{2}\\), \\(a_{3}\\) are linearly independent.\nDetermining if a vector is a linear combination of linearly independent vectors.Suppose the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, and we wish to determine if another vector \\(b\\) is a linear combination of them. (We have already noted on page 91 that if it is a linear combination of them, the coefficients are unique.) The Gram-Schmidt algorithm provides an explicit way to do this. We apply the Gram-Schmidt algorithm to the list of \\(k+1\\) vectors\n\\[a_{1},\\ldots,a_{k},b.\\]\nThese vectors are linearly dependent if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); they are linearly independent if \\(b\\) is not a linear combination of \\(a_{1},\\ldots,a_{k}\\). The Gram-Schmidt algorithm will determine which of these two cases holds. It cannot terminate in the first \\(k\\) steps, since we assume that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. It will terminate in the \\((k+1)\\)st step with \\(\\tilde{q}_{k+1}=0\\) if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\). It will not terminate in the \\((k+1)\\)st step (_i.e._, \\(\\tilde{q}_{k+1}\\neq 0\\)), otherwise.\n\nDocument 3:\n----\nIt is immediate that \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i}\\):\n\\[a_{i} = \\tilde{q}_{i}+(q_{1}^{T}a_{i})q_{1}+\\cdots+(q_{i-1}^{T}a_{i})q_{i -1}\\] \\[= (q_{1}^{T}a_{i})q_{1}+\\cdots+(q_{i-1}^{T}a_{i})q_{i-1}+\\|\\tilde{ q}_{i}\\|q_{i}.\\]\nFrom step 1 of the algorithm, we see that \\(\\tilde{q}_{i}\\) is a linear combination of the vectors \\(a_{1},q_{1},\\ldots,q_{i-1}\\). By the induction hypothesis, each of \\(q_{1},\\ldots,q_{i-1}\\) is a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), so \\(\\tilde{q}_{i}\\) (and therefore also \\(q_{i}\\)) is a linear combination of \\(a_{1},\\ldots,a_{i}\\). Thus assertions 3 and 4 hold.\nGram-Schmidt completion implies linear independence.From the properties 1-4 above, we can argue that the original collection of vectors \\(a_{1},\\ldots,a_{k}\\) is linearly independent. To see this, suppose that\n\\[\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}=0\\] (5.6)\nholds for some \\(\\beta_{1},\\ldots,\\beta_{k}\\). We will show that \\(\\beta_{1}=\\cdots=\\beta_{k}=0\\).\nWe first note that any linear combination of \\(q_{1},\\ldots,q_{k-1}\\) is orthogonal to any multiple of \\(q_{k}\\), since \\(q_{1}^{T}q_{k}=\\cdots=q_{k-1}^{T}q_{k}=0\\) (by definition). But each of \\(a_{1},\\ldots,a_{k-1}\\) is a linear combination of \\(q_{1},\\ldots,q_{k-1}\\), so we have \\(q_{k}^{T}a_{1}=\\cdots=q_{k}^{T}a_{k-1}=0\\). Taking the inner product of \\(q_{k}\\) with the left- and right-hand sides of (5.6) we obtain\n\\[0 = q_{k}^{T}(\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k})\\] \\[= \\beta_{1}q_{k}^{T}a_{1}+\\cdots+\\beta_{k-1}q_{k}^{T}a_{k-1}+\\beta _{k}q_{k}^{T}a_{k}\\] \\[= \\beta_{k}\\|\\tilde{q}_{k}\\|,\\]\nwhere we use \\(q_{k}^{T}a_{k}=\\|\\tilde{q}_{k}\\|\\) in the last line. We conclude that \\(\\beta_{k}=0\\).\nFrom (5.6) and \\(\\beta_{k}=0\\) we have\n\\[\\beta_{1}a_{1}+\\cdots+\\beta_{k-1}a_{k-1}=0.\\]\nWe now repeat the argument above to conclude that \\(\\beta_{k-1}=0\\). Repeating it \\(k\\) times we conclude that all \\(\\beta_{i}\\) are zero.\n\nDocument 4:\n----\n\\[(Ax)^{T}(Ay) = (x^{T}A^{T})(Ay)\\] \\[= x^{T}(A^{T}A)y\\] \\[= x^{T}Iy\\] \\[= x^{T}y.\\]\nIn the first line, we use the transpose-of-product rule; in the second, we re-associate a product of 4 matrices (considering the row vector \\(x^{T}\\) and column vector \\(x\\) as matrices); in the third line, we use \\(A^{T}A=I\\); and in the fourth line, we use \\(Iy=y\\).\nFrom the second property we can derive the first one: By taking \\(y=x\\) we get \\((Ax)^{T}(Ax)=x^{T}x\\); taking the squareroot of each side gives \\(\\|Ax\\|=\\|x\\|\\). The third property, angle preservation, follows from the first two, since\n\\[\\angle(Ax,Ay)=\\arccos\\left(\\frac{(Ax)^{T}(Ay)}{\\|Ax\\|\\|Ay\\|}\\right)=\\arccos \\left(\\frac{x^{T}y}{\\|x\\|\\|y\\|}\\right)=\\angle(x,y).\\]\nQR factorization.We can express the result of the Gram-Schmidt algorithm described in SS5.4 in a compact form using matrices. Let \\(A\\) be an \\(n\\times k\\) matrix with linearly independent columns \\(a_{1},\\ldots,a_{k}\\). By the independence-dimension inequality, \\(A\\) is tall or square. Let \\(Q\\) be the \\(n\\times k\\) matrix with columns \\(q_{1},\\ldots,q_{k}\\), the orthonormal vectors produced by the Gram-Schmidt algorithm applied to the \\(n\\)-vectors \\(a_{1},\\ldots,a_{k}\\). Orthonormality of \\(q_{1},\\ldots,q_{k}\\) is expressed in matrix form as \\(Q^{T}Q=I\\). We express the equation relating \\(a_{i}\\) and \\(q_{i}\\),\n\\[a_{i}=(q_{1}^{T}a_{i})q_{1}+\\cdots+(q_{i-1}^{T}a_{i})q_{i-1}+\\|\\tilde{q}_{i}\\| q_{i},\\]\nwhere \\(\\tilde{q}_{i}\\) is the vector obtained in the first step of the Gram-Schmidt algorithm, as\n\\[a_{i}=R_{1i}q_{1}+\\cdots+R_{ii}q_{i},\\]\nwhere \\(R_{ij}=q_{i}^{T}a_{j}\\) for \\(i<j\\) and \\(R_{ii}=\\|\\tilde{q}_{i}\\|\\). Defining \\(R_{ij}=0\\) for \\(i>j\\), we can write the equations above in compact matrix form as\n\nDocument 5:\n----\nSuppose our assertion holds for some \\(i-1\\), with \\(i<k\\); we will show it holds for \\(i\\). If \\(\\tilde{q}_{i}=0\\), then \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i-1}\\) (from the first step in the algorithm); but each of these is (by the induction hypothesis) a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), so it follows that \\(a_{i}\\) is a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), which contradicts our assumption that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. So assertion 1 holds for \\(i\\).\nStep 3 of the algorithm ensures that \\(q_{1},\\ldots,q_{i}\\) are normalized; to show they are orthogonal we will show that \\(q_{i}\\perp q_{j}\\) for \\(j=1,\\ldots,i-1\\). (Our induction hypothesis tells us that \\(q_{r}\\perp q_{s}\\) for \\(r,s<i\\).) For any \\(j=1,\\ldots,i-1\\), we have (using step 1)\n\\[q_{j}^{T}\\tilde{q}_{i} = q_{j}^{T}a_{i}-(q_{1}^{T}a_{i})(q_{j}^{T}q_{1})-\\cdots-(q_{i-1}^ {T}a_{i})(q_{j}^{T}q_{i-1})\\] \\[= q_{j}^{T}a_{i}-q_{j}^{T}a_{i}=0,\\]\nusing \\(q_{j}^{T}q_{k}=0\\) for \\(j\\neq k\\) and \\(q_{j}^{T}q_{j}=1\\). (This explains why step 1 is called the orthogonalization step: We subtract from \\(a_{i}\\) a linear combination of \\(q_{1},\\ldots,q_{i-1}\\) that ensures \\(q_{i}\\perp\\tilde{q}_{j}\\) for \\(j<i\\).) Since \\(q_{i}=(1/\\|\\tilde{q}_{i}\\|)\\tilde{q}_{i}\\), we have \\(q_{i}^{T}q_{j}=0\\) for \\(j=1,\\ldots,i-1\\). So assertion 2 holds for \\(i\\).\nIt is immediate that \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i}\\):\n\\[a_{i} = \\tilde{q}_{i}+(q_{1}^{T}a_{i})q_{1}+\\cdots+(q_{i-1}^{T}a_{i})q_{i -1}\\] \\[= (q_{1}^{T}a_{i})q_{1}+\\cdots+(q_{i-1}^{T}a_{i})q_{i-1}+\\|\\tilde{ q}_{i}\\|q_{i}.\\]\nFrom step 1 of the algorithm, we see that \\(\\tilde{q}_{i}\\) is a linear combination of the vectors \\(a_{1},q_{1},\\ldots,q_{i-1}\\). By the induction hypothesis, each of \\(q_{1},\\ldots,q_{i-1}\\) is a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), so \\(\\tilde{q}_{i}\\) (and therefore also \\(q_{i}\\)) is a linear combination of \\(a_{1},\\ldots,a_{i}\\). Thus assertions 3 and 4 hold.\n\n\n",
        "answer": [
            8,
            0,
            2,
            -2
        ],
        "rationale": {
            "Question": "Find the orthogonal projection of 9e_1 onto the subspace of R^4 spanned by [2, 2, 1, 0] and [-2, 2, 0, 1].",
            "Answer": [
                8,
                0,
                2,
                -2
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_5.json",
            "explanation": "NONE",
            "theorem": "projection theory",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "elainewan/math_algebra_6.json",
        "question": "For a matrix A, is the function F(A) = det A from the linear space R^{3*3} to R a linear transformation?",
        "prediction": "False",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor a matrix A, is the function F(A) = det A from the linear space R^{3*3} to R a linear transformation?\n\nDocument 1:\n----\nholds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). It is a good exercise to parse this simple looking equation, since it involves overloading of notation. On the left-hand side, the scalar-vector multiplications \\(\\alpha x\\) and \\(\\beta y\\) involve \\(n\\)-vectors, and the sum \\(\\alpha x+\\beta y\\) is the sum of two \\(n\\)-vectors. The function \\(f\\) maps \\(n\\)-vectors to \\(m\\)-vectors, so \\(f(\\alpha x+\\beta y)\\) is an \\(m\\)-vector. On the right-hand side, the scalar-vector multiplications and the sum are those for \\(m\\)-vectors. Finally, the equality sign is equality between two \\(m\\)-vectors.\nWe can verify that superposition holds for \\(f\\) using properties of matrix-vector and scalar-vector multiplication:\n\\[f(\\alpha x+\\beta y) = A(\\alpha x+\\beta y)\\] \\[= A(\\alpha x)+A(\\beta y)\\] \\[= \\alpha(Ax)+\\beta(Ay)\\] \\[= \\alpha f(x)+\\beta f(y)\\]\nThus we can associate with every matrix \\(A\\) a linear function \\(f(x)=Ax\\).\nThe converse is also true. Suppose \\(f\\) is a function that maps \\(n\\)-vectors to \\(m\\)-vectors, and is linear, _i.e._, (8.1) holds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). Then there exists an \\(m\\times n\\) matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\). This can be shown in the same way as for scalar-valued functions in SS2.1, by showing that if \\(f\\) is linear, then\n\\[f(x)=x_{1}f(e_{1})+x_{2}f(e_{2})+\\cdots+x_{n}f(e_{n}),\\] (8.2)\nwhere \\(e_{k}\\) is the \\(k\\)th unit vector of size \\(n\\). The right-hand side can also be written as a matrix-vector product \\(Ax\\), with\n\\[A=\\left[\\begin{array}{cccc}f(e_{1})&f(e_{2})&\\cdots&f(e_{n})\\end{array} \\right].\\]\n\nDocument 2:\n----\nwhere \\(e_{k}\\) is the \\(k\\)th unit vector of size \\(n\\). The right-hand side can also be written as a matrix-vector product \\(Ax\\), with\n\\[A=\\left[\\begin{array}{cccc}f(e_{1})&f(e_{2})&\\cdots&f(e_{n})\\end{array} \\right].\\]\nThe expression (8.2) is the same as (2.3), but here \\(f(x)\\) and \\(f(e_{k})\\) are vectors. The implications are exactly the same: A linear vector-valued function \\(f\\) is completely characterized by evaluating \\(f\\) at the \\(n\\) unit vectors \\(e_{1},\\ldots,e_{n}\\).\nAs in SS2.1 it is easily shown that the matrix-vector representation of a linear function is unique. If \\(f:{\\bf R}^{n}\\to{\\bf R}^{m}\\) is a linear function, then there exists exactly one matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\).\nExamples of linear functions.In the examples below we define functions \\(f\\) that map \\(n\\)-vectors \\(x\\) to \\(n\\)-vectors \\(f(x)\\). Each function is described in words, in terms of its effect on an arbitrary \\(x\\). In each case we give the associated matrix multiplication representation.\n* _Negation._\\(f\\) changes the sign of \\(x\\): \\(f(x)=-x\\). Negation can be expressed as \\(f(x)=Ax\\) with \\(A=-I\\).\n* _Reversal._\\(f\\) reverses the order of the elements of \\(x\\): \\(f(x)=(x_{n},x_{n-1},\\ldots,x_{1})\\). The reversal function can be expressed as \\(f(x)=Ax\\) with \\[A=\\left[\\begin{array}{cccc}0&\\cdots&0&1\\\\ 0&\\cdots&1&0\\\\ \\vdots&\\cdot\\cdot\\cdot&\\vdots&\\vdots\\\\ 1&\\cdots&0&0\\end{array}\\right].\\] (This is the \\(n\\times n\\) identity matrix with the order of its columns reversed. It is the _reverser matrix_ introduced in SS7.2.)\n\nDocument 3:\n----\nbe the averages of these two lists of vectors. (Here \\(\\overline{x}\\) is an \\(n\\)-vector and \\(\\overline{y}\\) is an \\(m\\)-vector.) Show that we always have \\(\\overline{y}=G(\\overline{x})\\). In words: The average of an affine function applied to a list of vectors is the same as the affine function applied to the average of the list of vectors.\n### Cross-product\nThe cross product of two 3-vectors \\(a=(a_{1},a_{2},a_{3})\\) and \\(x=(x_{1},x_{2},x_{3})\\) is defined as the vector\n\\[a\\times x=\\left[\\begin{array}{c}a_{2}x_{3}-a_{3}x_{2}\\\\ a_{3}x_{1}-a_{1}x_{3}\\\\ a_{1}x_{2}-a_{2}x_{1}\\end{array}\\right].\\]\nThe cross product comes up in physics, for example in electricity and magnetism, and in dynamics of mechanical systems like robots or satellites. (You do not need to know this for this exercise.)\nAssume \\(a\\) is fixed. Show that the function \\(f(x)=a\\times x\\) is a linear function of \\(x\\), by giving a matrix \\(A\\) that satisfies \\(f(x)=Ax\\) for all \\(x\\).\n### Linear functions of images\nIn this problem we consider several linear functions of a monochrome image with \\(N\\times N\\) pixels. To keep the matrices small enough to work out by hand, we will consider the case with \\(N=3\\) (which would hardly qualify as an image). We represent a \\(3\\times 3\\) image as a 9-vector using the ordering of pixels shown below.\n\\begin{tabular}{|c|c|c|} \\hline\n1 & 4 & 7 \\\\ \\hline\n\nDocument 4:\n----\nFirst suppose that the columns of \\(A\\) are linearly independent. Let \\(x\\) be an \\(n\\)-vector which satisfies \\((A^{T}A)x=0\\). Multiplying on the left by \\(x^{T}\\) we get\n\\[0=x^{T}0=x^{T}(A^{T}Ax)=x^{T}A^{T}Ax=\\|Ax\\|^{2},\\]\nwhich implies that \\(Ax=0\\). Since the columns of \\(A\\) are linearly independent, we conclude that \\(x=0\\). Since the only solution of \\((A^{T}A)x=0\\) is \\(x=0\\), we conclude that \\(A^{T}A\\) is invertible.\nNow let's show the converse. Suppose the columns of \\(A\\) are linearly dependent, which means there is a nonzero \\(n\\)-vector \\(x\\) which satisfies \\(Ax=0\\). Multiply on the left by \\(A^{T}\\) to get \\((A^{T}A)x=0\\). This shows that the Gram matrix \\(A^{T}A\\) is singular.\nPseudo-inverse of square or tall matrix.We show here that if \\(A\\) has linearly independent columns (and therefore, is square or tall) then it has a left inverse. (We already have observed the converse, that a matrix with a left inverse has linearly independent columns.) Assuming \\(A\\) has linearly independent columns, we know that \\(A^{T}A\\) is invertible. We now observe that the matrix \\((A^{T}A)^{-1}A^{T}\\) is a left inverse of \\(A\\):\n\\[\\left((A^{T}A)^{-1}A^{T}\\right)A=(A^{T}A)^{-1}(A^{T}A)=I.\\]\nThis particular left-inverse of \\(A\\) will come up in the sequel, and has a name,\nFigure 11.3: Temperature distribution on a \\(100\\times 100\\) grid of nodes. Nodes in the top and bottom rows are held at zero temperature. The three sets of nodes with rectilinear shapes are held at temperature one.\n\nDocument 5:\n----\nIn summary, a left inverse can be used to determine whether or not a solution of an over-determined set of linear equations exists, and when it does, find the unique solution.\nRight inverse.Now we turn to the closely related concept of right inverse. A matrix \\(X\\) that satisfies\n\\[AX=I\\]\nis called a _right inverse_ of \\(A\\). The matrix \\(A\\) is _right-invertible_ if a right inverse exists. Any right inverse has the same dimensions as \\(A^{T}\\).\nLeft and right inverse of matrix transpose.If \\(A\\) has a right inverse \\(B\\), then \\(B^{T}\\) is a left inverse of \\(A^{T}\\), since \\(B^{T}A^{T}=(AB)^{T}=I\\). If \\(A\\) has a left inverse \\(C\\), then \\(C^{T}\\) is a right inverse of \\(A^{T}\\), since \\(A^{T}C^{T}=(CA)^{T}=I\\). This observation allows us to map all the results for left-invertibility given above to similar results for right-invertibility. Some examples are given below.\n* A matrix is right-invertible if and only if its rows are linearly independent.\n* A tall matrix cannot have a right inverse. Only square or wide matrices can be right-invertible.\nSolving linear equations with a right inverse.Consider the set of \\(m\\) linear equations in \\(n\\) variables \\(Ax=b\\). Suppose \\(A\\) is right-invertible, with right inverse \\(B\\). This implies that \\(A\\) is square or wide, so the linear equations \\(Ax=b\\) are square or under-determined.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "For a matrix A, is the function F(A) = det A from the linear space R^{3*3} to R a linear transformation?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "elainewan/math_algebra_3_4.json",
        "question": "Are the vectors [1, 2], [2, 3], and [3, 4] linearly independent?",
        "prediction": "False",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nAre the vectors [1, 2], [2, 3], and [3, 4] linearly independent?\n\nDocument 1:\n----\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\\[x=\\gamma_{1}a_{1}+\\cdots+\\gamma_{k}a_{k},\\]\nthen \\(\\beta_{i}=\\gamma_{i}\\) for \\(i=1,\\ldots,k\\). This tells us that, in principle at least, we can find the coefficients that form a vector \\(x\\) as a linear combination of linearly independent vectors.\nTo see this, we subtract the two equations above to get\n\\[0=(\\beta_{1}-\\gamma_{1})a_{1}+\\cdots+(\\beta_{k}-\\gamma_{k})a_{k}.\\]\nSince \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we conclude that \\(\\beta_{i}-\\gamma_{i}\\) are all zero.\nThe converse is also true: If each linear combination of a list of vectors can only be expressed as a linear combination with one set of coefficients, then the list of vectors is linearly independent. This gives a nice interpretation of linear independence: A list of vectors is linearly independent if and only if for any linear combination of them, we can infer or deduce the associated coefficients. (We will see later how to do this.)\nSupersets and subsets.If a collection of vectors is linearly dependent, then any superset of it is linearly dependent. In other words: If we add vectors to a linearly dependent collection of vectors, the new collection is also linearly dependent. Any nonempty subset of a linearly independent collection of vectors is linearly independent. In other words: Removing vectors from a collection of vectors preserves linear independence.\n\nDocument 2:\n----\n* \\(i=3\\). We have \\(q_{1}^{T}a_{3}=2\\) and \\(q_{2}^{T}a_{3}=8\\), so \\[\\tilde{q}_{3} = a_{3}-(q_{1}^{T}a_{3})q_{1}-(q_{2}^{T}a_{3})q_{2}\\] \\[= \\left[\\begin{array}{c}1\\\\ 3\\\\ 5\\\\ 7\\end{array}\\right]-2\\left[\\begin{array}{c}-1/2\\\\ 1/2\\\\ -1/2\\\\ 1/2\\end{array}\\right]-8\\left[\\begin{array}{c}1/2\\\\ 1/2\\\\ 1/2\\\\ 1/2\\end{array}\\right]\\] \\[= \\left[\\begin{array}{c}-2\\\\ -2\\\\ 2\\\\ 2\\end{array}\\right],\\] which is orthogonal to \\(q_{1}\\) and \\(q_{2}\\) (and \\(a_{1}\\) and \\(a_{2}\\)). We have \\(\\|\\tilde{q}_{3}\\|=4\\), so the normalized vector is \\[q_{3}=\\frac{1}{\\|\\tilde{q}_{3}\\|}\\tilde{q}_{3}=(-1/2,-1/2,1/2,1/2).\\]\nCompletion of the Gram-Schmidt algorithm without early termination tells us that the vectors \\(a_{1}\\), \\(a_{2}\\), \\(a_{3}\\) are linearly independent.\nDetermining if a vector is a linear combination of linearly independent vectors.Suppose the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, and we wish to determine if another vector \\(b\\) is a linear combination of them. (We have already noted on page 91 that if it is a linear combination of them, the coefficients are unique.) The Gram-Schmidt algorithm provides an explicit way to do this. We apply the Gram-Schmidt algorithm to the list of \\(k+1\\) vectors\n\\[a_{1},\\ldots,a_{k},b.\\]\nThese vectors are linearly dependent if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); they are linearly independent if \\(b\\) is not a linear combination of \\(a_{1},\\ldots,a_{k}\\). The Gram-Schmidt algorithm will determine which of these two cases holds. It cannot terminate in the first \\(k\\) steps, since we assume that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. It will terminate in the \\((k+1)\\)st step with \\(\\tilde{q}_{k+1}=0\\) if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\). It will not terminate in the \\((k+1)\\)st step (_i.e._, \\(\\tilde{q}_{k+1}\\neq 0\\)), otherwise.\n\nDocument 3:\n----\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\nDocument 4:\n----\nA collection of vectors \\(a_{1},\\ldots,a_{k}\\) is _orthogonal_ or _mutually orthogonal_ if \\(a_{i}\\perp a_{j}\\) for any \\(i\\), \\(j\\) with \\(i\\neq j\\), \\(i,j=1,\\ldots,k\\). A collection of vectors \\(a_{1},\\ldots,a_{k}\\) is _orthonormal_ if it is orthogonal and \\(\\|a_{i}\\|=1\\) for \\(i=1,\\ldots,k\\). (A vector of norm one is called _normalized_; dividing a vector by its norm is called _normalizing_ it.) Thus, each vector in an orthonormal collection of vectors is normalized, and two different vectors from the collection are orthogonal. These two conditions can be combined into one statement about the inner products of pairs of vectors in the collection: \\(a_{1},\\ldots,a_{k}\\) is orthonormal means that\n\\[a_{i}^{T}a_{j}=\\left\\{\\begin{array}{ll}1&i=j\\\\ 0&i\\neq j.\\end{array}\\right.\\]\nOrthonormality, like linear dependence and independence, is an attribute of a collection of vectors, and not an attribute of vectors individually. By convention, though, we say \"The vectors \\(a_{1},\\ldots,a_{k}\\) are orthonormal\" to mean \"The collection of vectors \\(a_{1},\\ldots,a_{k}\\) is orthonormal\".\nExamples.The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are orthonormal. As another example, the 3-vectors\n\\[\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right],\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ 1\\\\ 0\\end{array}\\right],\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right],\\] (5.3)\nare orthonormal. Figure 5.2 shows a set of two orthonormal 2-vectors.\nLinear independence of orthonormal vectors.Orthonormal vectors are linearly independent. To see this, suppose \\(a_{1},\\ldots,a_{k}\\) are orthonormal, and\n\\[\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}=0.\\]\nFigure 5.2: Orthonormal vectors in a plane.\nTaking the inner product of this equality with \\(a_{i}\\) yields\n\\[0 = a_{i}^{T}(\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k})\\] \\[= \\beta_{1}(a_{i}^{T}a_{1})+\\cdots+\\beta_{k}(a_{i}^{T}a_{k})\\] \\[= \\beta_{i},\\]\nsince \\(a_{i}^{T}a_{j}=0\\) for \\(j\\neq i\\) and \\(a_{i}^{T}a_{i}=1\\). Thus, the only linear combination of \\(a_{1},\\ldots,a_{k}\\) that is zero is the one with all coefficients zero.\n\nDocument 5:\n----\n_Any \\(n\\)-vector \\(b\\) can be written in a unique way as a linear combination of a basis \\(a_{1},\\ldots,a_{n}\\)._\nExpansion in a basis.When we express an \\(n\\)-vector \\(b\\) as a linear combination of a basis \\(a_{1},\\ldots,a_{n}\\), we refer to\n\\[b=\\alpha_{1}a_{1}+\\cdots+\\alpha_{n}a_{n},\\]\nas the _expansion of \\(b\\) in the \\(a_{1},\\ldots,a_{n}\\) basis_. The numbers \\(\\alpha_{1},\\ldots,\\alpha_{n}\\) are called the _coefficients_ of the expansion of \\(b\\) in the basis \\(a_{1},\\ldots,a_{n}\\). (We will see later how to find the coefficients in the expansion of a vector in a basis.)\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Are the vectors [1, 2], [2, 3], and [3, 4] linearly independent?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_4.json",
            "explanation": "solutions/math_algebra_3_4.txt",
            "theorem": "linear independence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "elainewan/math_algebra_3_2.json",
        "question": "Are the vectors v_1 = [1,2,3], v_2 = [4,5,6], v_3 = [7,8,9] linearly independent?",
        "prediction": "linearly independent",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nAre the vectors v_1 = [1,2,3], v_2 = [4,5,6], v_3 = [7,8,9] linearly independent?\n\nDocument 1:\n----\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\\[x=\\gamma_{1}a_{1}+\\cdots+\\gamma_{k}a_{k},\\]\nthen \\(\\beta_{i}=\\gamma_{i}\\) for \\(i=1,\\ldots,k\\). This tells us that, in principle at least, we can find the coefficients that form a vector \\(x\\) as a linear combination of linearly independent vectors.\nTo see this, we subtract the two equations above to get\n\\[0=(\\beta_{1}-\\gamma_{1})a_{1}+\\cdots+(\\beta_{k}-\\gamma_{k})a_{k}.\\]\nSince \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we conclude that \\(\\beta_{i}-\\gamma_{i}\\) are all zero.\nThe converse is also true: If each linear combination of a list of vectors can only be expressed as a linear combination with one set of coefficients, then the list of vectors is linearly independent. This gives a nice interpretation of linear independence: A list of vectors is linearly independent if and only if for any linear combination of them, we can infer or deduce the associated coefficients. (We will see later how to do this.)\nSupersets and subsets.If a collection of vectors is linearly dependent, then any superset of it is linearly dependent. In other words: If we add vectors to a linearly dependent collection of vectors, the new collection is also linearly dependent. Any nonempty subset of a linearly independent collection of vectors is linearly independent. In other words: Removing vectors from a collection of vectors preserves linear independence.\n\nDocument 2:\n----\n* \\(i=3\\). We have \\(q_{1}^{T}a_{3}=2\\) and \\(q_{2}^{T}a_{3}=8\\), so \\[\\tilde{q}_{3} = a_{3}-(q_{1}^{T}a_{3})q_{1}-(q_{2}^{T}a_{3})q_{2}\\] \\[= \\left[\\begin{array}{c}1\\\\ 3\\\\ 5\\\\ 7\\end{array}\\right]-2\\left[\\begin{array}{c}-1/2\\\\ 1/2\\\\ -1/2\\\\ 1/2\\end{array}\\right]-8\\left[\\begin{array}{c}1/2\\\\ 1/2\\\\ 1/2\\\\ 1/2\\end{array}\\right]\\] \\[= \\left[\\begin{array}{c}-2\\\\ -2\\\\ 2\\\\ 2\\end{array}\\right],\\] which is orthogonal to \\(q_{1}\\) and \\(q_{2}\\) (and \\(a_{1}\\) and \\(a_{2}\\)). We have \\(\\|\\tilde{q}_{3}\\|=4\\), so the normalized vector is \\[q_{3}=\\frac{1}{\\|\\tilde{q}_{3}\\|}\\tilde{q}_{3}=(-1/2,-1/2,1/2,1/2).\\]\nCompletion of the Gram-Schmidt algorithm without early termination tells us that the vectors \\(a_{1}\\), \\(a_{2}\\), \\(a_{3}\\) are linearly independent.\nDetermining if a vector is a linear combination of linearly independent vectors.Suppose the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, and we wish to determine if another vector \\(b\\) is a linear combination of them. (We have already noted on page 91 that if it is a linear combination of them, the coefficients are unique.) The Gram-Schmidt algorithm provides an explicit way to do this. We apply the Gram-Schmidt algorithm to the list of \\(k+1\\) vectors\n\\[a_{1},\\ldots,a_{k},b.\\]\nThese vectors are linearly dependent if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); they are linearly independent if \\(b\\) is not a linear combination of \\(a_{1},\\ldots,a_{k}\\). The Gram-Schmidt algorithm will determine which of these two cases holds. It cannot terminate in the first \\(k\\) steps, since we assume that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. It will terminate in the \\((k+1)\\)st step with \\(\\tilde{q}_{k+1}=0\\) if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\). It will not terminate in the \\((k+1)\\)st step (_i.e._, \\(\\tilde{q}_{k+1}\\neq 0\\)), otherwise.\n\nDocument 3:\n----\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\nDocument 4:\n----\nLinear independence is a generalization of the theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of\nSince the vectors \\(a_{i}=(b_{i},\\alpha_{i})\\) are linearly independent, the equality (5.2) only holds when all the coefficients \\(\\beta_{i}\\) and \\(\\gamma\\) are all zero. This in turns implies that the vectors \\(c_{1},\\ldots,c_{k-1}\\) are linearly independent. By the induction hypothesis \\(k-1\\leq n-1\\), so we have established that \\(k\\leq n\\).\n\nDocument 5:\n----\nA collection of vectors \\(a_{1},\\ldots,a_{k}\\) is _orthogonal_ or _mutually orthogonal_ if \\(a_{i}\\perp a_{j}\\) for any \\(i\\), \\(j\\) with \\(i\\neq j\\), \\(i,j=1,\\ldots,k\\). A collection of vectors \\(a_{1},\\ldots,a_{k}\\) is _orthonormal_ if it is orthogonal and \\(\\|a_{i}\\|=1\\) for \\(i=1,\\ldots,k\\). (A vector of norm one is called _normalized_; dividing a vector by its norm is called _normalizing_ it.) Thus, each vector in an orthonormal collection of vectors is normalized, and two different vectors from the collection are orthogonal. These two conditions can be combined into one statement about the inner products of pairs of vectors in the collection: \\(a_{1},\\ldots,a_{k}\\) is orthonormal means that\n\\[a_{i}^{T}a_{j}=\\left\\{\\begin{array}{ll}1&i=j\\\\ 0&i\\neq j.\\end{array}\\right.\\]\nOrthonormality, like linear dependence and independence, is an attribute of a collection of vectors, and not an attribute of vectors individually. By convention, though, we say \"The vectors \\(a_{1},\\ldots,a_{k}\\) are orthonormal\" to mean \"The collection of vectors \\(a_{1},\\ldots,a_{k}\\) is orthonormal\".\nExamples.The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are orthonormal. As another example, the 3-vectors\n\\[\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right],\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ 1\\\\ 0\\end{array}\\right],\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right],\\] (5.3)\nare orthonormal. Figure 5.2 shows a set of two orthonormal 2-vectors.\nLinear independence of orthonormal vectors.Orthonormal vectors are linearly independent. To see this, suppose \\(a_{1},\\ldots,a_{k}\\) are orthonormal, and\n\\[\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}=0.\\]\nFigure 5.2: Orthonormal vectors in a plane.\nTaking the inner product of this equality with \\(a_{i}\\) yields\n\\[0 = a_{i}^{T}(\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k})\\] \\[= \\beta_{1}(a_{i}^{T}a_{1})+\\cdots+\\beta_{k}(a_{i}^{T}a_{k})\\] \\[= \\beta_{i},\\]\nsince \\(a_{i}^{T}a_{j}=0\\) for \\(j\\neq i\\) and \\(a_{i}^{T}a_{i}=1\\). Thus, the only linear combination of \\(a_{1},\\ldots,a_{k}\\) that is zero is the one with all coefficients zero.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Are the vectors v_1 = [1,2,3], v_2 = [4,5,6], v_3 = [7,8,9] linearly independent?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_2.json",
            "explanation": "solutions/math_algebra_3_2.png",
            "theorem": "linear independence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "elainewan/math_algebra_additional_1.json",
        "question": "Let f_1, ..., f_n be polynomials. Do they span the space P of all polynomials?",
        "prediction": "Therefore, the answer depends on the specific polynomials \\(f_1, ..., f_n\\) provided in the question",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet f_1, ..., f_n be polynomials. Do they span the space P of all polynomials?\n\nDocument 1:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\nBalancing chemical reactions.(See page 154 for background.) We consider the problem of balancing the chemical reaction\n\\[a_{1}\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+a_{2}\\mathrm{Fe}^{2+}+a_{3}\\mathrm{H} ^{+}\\longrightarrow b_{1}\\mathrm{Cr}^{3+}+b_{2}\\mathrm{Fe}^{3+}+b_{3} \\mathrm{H}_{2}\\mathrm{O},\\]\n\nDocument 2:\n----\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\nDocument 3:\n----\n* _Portfolio return time series._ Suppose that \\(R\\) is a \\(T\\times n\\) asset return matrix, that gives the returns of \\(n\\) assets over \\(T\\) periods. A common trading strategy maintains constant investment weights given by the \\(n\\)-vector \\(w\\) over the \\(T\\) periods. For example, \\(w_{4}=0.15\\) means that \\(15\\%\\) of the total portfolio value is held in asset 4. (Short positions are denoted by negative entries in \\(w\\).) Then \\(Rw\\), which is a \\(T\\)-vector, is the time series of the portfolio returns over the periods \\(1,\\ldots,T\\). As an example, consider a portfolio of the 4 assets in table 6.1, with weights \\(w=(0.4,0.3,-0.2,0.5)\\). The product \\(Rw=(0.00213,-0.00201,0.00241)\\) gives the portfolio returns over the three periods in the example.\n* _Polynomial evaluation at multiple points._ Suppose the entries of the \\(n\\)-vector \\(c\\) are the coefficients of a polynomial \\(p\\) of degree \\(n-1\\) or less: \\[p(t)=c_{1}+c_{2}t+\\cdots+c_{n-1}t^{n-2}+c_{n}t^{n-1}.\\] Let \\(t_{1},\\ldots,t_{m}\\) be \\(m\\) numbers, and define the \\(m\\)-vector \\(y\\) as \\(y_{i}=p(t_{i})\\). Then we have \\(y=Ac\\), where \\(A\\) is the \\(m\\times n\\) matrix \\[A=\\left[\\begin{array}{ccccc}1&t_{1}&\\cdots&t_{1}^{n-2}&t_{1}^{n-1}\\\\ 1&t_{2}&\\cdots&t_{2}^{n-2}&t_{2}^{n-1}\\\\ \\vdots&\\vdots&&\\vdots&\\vdots\\\\ 1&t_{m}&\\cdots&t_{m}^{n-2}&t_{m}^{n-1}\\end{array}\\right].\\] (6.7)\nSo multiplying a vector \\(c\\) by the matrix \\(A\\) is the same as evaluating a polynomial with coefficients \\(c\\) at \\(m\\) points. The matrix \\(A\\) in (6.7) comes up often, and is called a _Vandermonde matrix_ (of degree \\(n-1\\), at the points \\(t_{1},\\ldots,t_{m}\\)), named for the mathematician Alexandre-Theophile Vandermonde.\n\nDocument 4:\n----\n\\[2 = (1)(2)\\] \\[1 = (1)(1)+(0)(2)\\] \\[-3 = (1)(-1)+(0)(1)+(-1)(2)\\] \\[-1 = (0)(-1)+(-1)(1)\\] \\[1 = (-1)(-1).\\]\nPolynomial multiplication.If \\(a\\) and \\(b\\) represent the coefficients of two polynomials\n\\[p(x)=a_{1}+a_{2}x+\\cdots+a_{n}x^{n-1},\\qquad q(x)=b_{1}+b_{2}x+\\cdots+b_{m}x^{ m-1},\\]\nthen the coefficients of the product polynomial \\(p(x)q(x)\\) are represented by \\(c=a*b\\):\n\\[p(x)q(x)=c_{1}+c_{2}x+\\cdots+c_{n+m-1}x^{n+m-2}.\\]\nTo see this we will show that \\(c_{k}\\) is the coefficient of \\(x^{k-1}\\) in \\(p(x)q(x)\\). We expand the product polynomial into \\(mn\\) terms, and collect those terms associated with \\(x^{k-1}\\). These terms have the form \\(a_{i}b_{j}x^{i+j-2}\\), for \\(i\\) and \\(j\\) that satisfy \\(i+j-2=k-1\\), _i.e._, \\(i+j=k-1\\). It follows that \\(c_{k}=\\sum_{i+j=k+1}a_{i}b_{j}\\), which agrees with the convolution formula (7.2).\nProperties of convolution.Convolution is symmetric: We have \\(a*b=b*a\\). It is also associative: We have \\((a*b)*c=a*(b*c)\\), so we can write both as \\(a*b*c\\). Another property is that \\(a*b=0\\) implies that either \\(a=0\\) or \\(b=0\\). These properties follow from the polynomial coefficient property above, and can also be directly shown. As an example, let us show that \\(a*b=b*a\\). Suppose \\(p\\) is the polynomial with coefficients \\(a\\), and \\(q\\) is the polynomial with coefficients \\(b\\). The two polynomials \\(p(x)q(x)\\) and \\(q(x)p(x)\\) are the same (since multiplication of numbers is commutative), so they have the same coefficients. The coefficients of \\(p(x)q(x)\\) are \\(a*b\\) and the coefficients of \\(q(x)p(x)\\) are \\(b*a\\). These must be the same.\n\nDocument 5:\n----\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\nBalancing chemical reactions.A chemical reaction involves \\(p\\) reactants (molecules) and \\(q\\) products, and can be written as\n\\[a_{1}R_{1}+\\cdots+a_{p}R_{p}\\longrightarrow b_{1}P_{1}+\\cdots+b_{q}P_{q}.\\]\nHere \\(R_{1},\\ldots,R_{p}\\) are the reactants, \\(P_{1},\\ldots,P_{q}\\) are the products, and the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\) are positive numbers that tell us how many of each of these molecules is involved in the reaction. They are typically integers, but can be scaled arbitrarily; we could double all of these numbers, for example, and we still have the same reaction. As a simple example, we have the electrolysis of water,\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Let f_1, ..., f_n be polynomials. Do they span the space P of all polynomials?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "self",
            "id": "elainewan/math_algebra_additional_1.json",
            "explanation": "NONE",
            "theorem": "linear span",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "elainewan/math_algebra_6_5.json",
        "question": "What is the determinant of the matrix A = [[1, 0, 0, 0, 0, 0], [2, 7, 0, 0, 0, 0], [3, 8, 6, 0, 0, 0], [4, 9, 5, 2, 1, 4], [5, 8, 4, 0, 2, 5], [6, 7, 3, 0, 3, 6]]?",
        "prediction": "0",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the determinant of the matrix A = [[1, 0, 0, 0, 0, 0], [2, 7, 0, 0, 0, 0], [3, 8, 6, 0, 0, 0], [4, 9, 5, 2, 1, 4], [5, 8, 4, 0, 2, 5], [6, 7, 3, 0, 3, 6]]?\n\nDocument 1:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\nDocument 2:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\nDocument 3:\n----\n\\[y=x_{1}a_{1}+x_{2}a_{2}+\\cdots+x_{n}a_{n}.\\]\nThis shows that \\(y=Ax\\) is a linear combination of the columns of \\(A\\); the coefficients in the linear combination are the elements of \\(x\\).\nGeneral examples.In the examples below, \\(A\\) is an \\(m\\times n\\) matrix and \\(x\\) is an \\(n\\)-vector.\n* _Zero matrix._ When \\(A=0\\), we have \\(Ax=0\\). In other words, \\(0x=0\\). (The left-hand \\(0\\) is an \\(m\\times n\\) matrix, and the right-hand zero is an \\(m\\)-vector.)\n* _Identity._ We have \\(Ix=x\\) for any vector \\(x\\). (The identity matrix here has dimension \\(n\\times n\\).) In other words, multiplying a vector by the identity matrix gives the same vector.\n* _Picking out columns and rows._ An important identity is \\(Ae_{j}=a_{j}\\), the \\(j\\)th column of \\(A\\). Multiplying a unit vector by a matrix 'picks out' one of the columns of the matrix. \\(A^{T}e_{i}\\), which is an \\(n\\)-vector, is the \\(i\\)th row of \\(A\\), transposed. (In other words, \\((A^{T}e_{i})^{T}\\) is the \\(i\\)th row of \\(A\\).)\n* _Summing or averaging columns or rows._ The \\(m\\)-vector \\(A\\mathbf{1}\\) is the sum of the columns of \\(A\\); its \\(i\\)th entry is the sum of the entries in the \\(i\\)th row of \\(A\\). The \\(m\\)-vector \\(A(\\mathbf{1}/n)\\) is the average of the columns of \\(A\\); its \\(i\\)th entry is the average of the entries in the \\(i\\)th row of \\(A\\). In a similar way, \\(A^{T}\\mathbf{1}\\) is an \\(n\\)-vector, whose \\(j\\)th entry is the sum of the entries in the \\(j\\)th column of \\(A\\).\n\nDocument 4:\n----\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n**10.11**: _Trace of matrix-matrix product_. The sum of the diagonal entries of a square matrix is called the _trace_ of the matrix, denoted \\(\\mathbf{tr}(A)\\).\n1. Suppose \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices. Show that \\[\\mathbf{tr}(A^{T}B)=\\sum_{i=1}^{m}\\sum_{j=1}^{n}A_{ij}B_{ij}.\\] What is the complexity of calculating \\(\\mathbf{tr}(A^{T}B)\\)?\n2. The number \\(\\mathbf{tr}(A^{T}B)\\) is sometimes referred to as the inner product of the matrices \\(A\\) and \\(B\\). (This allows us to extend concepts like angle to matrices.) Show that \\(\\mathbf{tr}(A^{T}B)=\\mathbf{tr}(B^{T}A)\\).\n3. Show that \\(\\mathbf{tr}(A^{T}A)=\\left\\|A\\right\\|^{2}\\). In other words, the square of the norm of a matrix is the trace of its Gram matrix.\n\nDocument 5:\n----\nDiagonal matrices.A square \\(n\\times n\\) matrix \\(A\\) is _diagonal_ if \\(A_{ij}=0\\) for \\(i\\neq j\\). (The entries of a matrix with \\(i=j\\) are called the _diagonal entries_; those with \\(i\\neq j\\) are its _off-diagonal_ entries.) A diagonal matrix is one for which all off-diagonal entries are zero. Examples of diagonal matrices we have already seen are square zero matrices and identity matrices. Other examples are\n\\[\\left[\\begin{array}{rrr}-3&0\\\\ 0&0\\end{array}\\right],\\qquad\\left[\\begin{array}{rrr}0.2&0&0\\\\ 0&-3&0\\\\ 0&0&1.2\\end{array}\\right].\\]\n(Note that in the first example, one of the diagonal elements is also zero.)\nThe notation \\(\\mathbf{diag}(a_{1},\\ldots,a_{n})\\) is used to compactly describe the \\(n\\times n\\) diagonal matrix \\(A\\) with diagonal entries \\(A_{11}=a_{1}\\), ..., \\(A_{nn}=a_{n}\\). This notation is not yet standard, but is coming into more prevalent use. As examples, the matrices above would be expressed as\n\\[\\mathbf{diag}(-3,0),\\qquad\\mathbf{diag}(0.2,-3,1.2),\\]\nrespectively. We also allow \\(\\mathbf{diag}\\) to take one \\(n\\)-vector argument, as in \\(I=\\mathbf{diag}(\\mathbf{1})\\).\nTriangular matrices.A square \\(n\\times n\\) matrix \\(A\\) is _upper triangular_ if \\(A_{ij}=0\\) for \\(i>j\\), and it is _lower triangular_ if \\(A_{ij}=0\\) for \\(i<j\\). (So a diagonal matrix is one that is both lower and upper triangular.) If a matrix is either lower or upper triangular, it is called _triangular_. For example, the matrices\n\\[\\left[\\begin{array}{rrr}1&-1&0.7\\\\ 0&1.2&-1.1\\\\ 0&0&3.2\\end{array}\\right],\\qquad\\left[\\begin{array}{rrr}-0.6&0\\\\ -0.3&3.5\\end{array}\\right],\\]\nare upper and lower triangular, respectively.\nA triangular \\(n\\times n\\) matrix \\(A\\) has up to \\(n(n+1)/2\\) nonzero entries, _i.e._, around half its entries are zero. Triangular matrices are generally not considered sparse matrices, since their density is around 50%, but their special sparsity pattern will be important in the sequel.\n\n\n",
        "answer": -252,
        "rationale": {
            "Question": "What is the determinant of the matrix A = [[1, 0, 0, 0, 0, 0], [2, 7, 0, 0, 0, 0], [3, 8, 6, 0, 0, 0], [4, 9, 5, 2, 1, 4], [5, 8, 4, 0, 2, 5], [6, 7, 3, 0, 3, 6]]?",
            "Answer": -252,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_5.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "wenhuchen/cramer's_rule2.json",
        "question": "For the two linear equations $2 * x + 3 * y + z = 8$ and $4 * x + 4 * y + 4z = 12$ and $x + y + 8z = 10$ with variables x, y and z. Use cramer's rule to solve these three variables.",
        "prediction": "$x = 7, y = -7, z = -1$",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor the two linear equations $2 * x + 3 * y + z = 8$ and $4 * x + 4 * y + 4z = 12$ and $x + y + 8z = 10$ with variables x, y and z. Use cramer's rule to solve these three variables.\n\nDocument 1:\n----\nIn summary, a right inverse can be used to find \\(a\\) solution of a square or under-determined set of linear equations, for any vector \\(b\\).\nExamples.Consider the matrix appearing in the example above on page 199,\n\\[A=\\left[\\begin{array}{rr}-3&-4\\\\ 4&6\\\\ 1&1\\end{array}\\right]\\]\nand the two left inverses\n\\[B=\\frac{1}{9}\\left[\\begin{array}{rr}-11&-10&16\\\\ 7&8&-11\\end{array}\\right],\\qquad C=\\frac{1}{2}\\left[\\begin{array}{rr}0&-1&6 \\\\ 0&1&-4\\end{array}\\right].\\]\n* The over-determined linear equations \\(Ax=(1,-2,0)\\) have the unique solution \\(x=(1,-1)\\), which can be obtained from _either_ left inverse: \\[x=B(1,-2,0)=C(1,-2,0).\\]\n* The over-determined linear equations \\(Ax=(1,-1,0)\\) do not have a solution, since \\(x=C(1,-1,0)=(1/2,-1/2)\\) does not satisfy \\(Ax=(1,-1,0)\\).\n* The under-determined linear equations \\(A^{T}y=(1,2)\\) has (different) solutions \\[B^{T}(1,2)=(1/3,2/3,38/9),\\qquad C^{T}(1,2)=(0,1/2,-1).\\] (Recall that \\(B^{T}\\) and \\(C^{T}\\) are both right inverses of \\(A^{T}\\).) We can find a solution of \\(A^{T}y=b\\) for any vector \\(b\\).\nLeft and right inverse of matrix product.Suppose \\(A\\) and \\(D\\) are compatible for the matrix product \\(AD\\) (_i.e._, the number of columns in \\(A\\) is equal to the number of rows in \\(D\\).) If \\(A\\) has a right inverse \\(B\\) and \\(D\\) has a right inverse \\(E\\), then \\(EB\\) is a right inverse of \\(AD\\). This follows from \\[(AD)(EB)=A(DE)B=A(IB)=AB=I.\\] If \\(A\\) has a left inverse \\(C\\) and \\(D\\) has a left inverse \\(F\\), then \\(FC\\) is a left inverse of \\(AD\\). This follows from \\[(FC)(AD)=F(CA)D=FD=I.\\]\n\nDocument 2:\n----\n**8.13**: _Portfolio sector exposures._ (See exercise 1.14.) The \\(n\\)-vector \\(h\\) denotes a portfolio of investments in \\(n\\) assets, with \\(h_{i}\\) the dollar value invested in asset \\(i\\). We consider a set of \\(m\\) industry sectors, such as pharmaceuticals or consumer electronics. Each asset is assigned to one of these sectors. (More complex models allow for an asset to be assigned to more than one sector.) The _exposure_ of the portfolio to sector \\(i\\) is defined as the sum of investments in the assets in that sector. We denote the sector exposures using the \\(m\\)-vector \\(s\\), where \\(s_{i}\\) is the portfolio exposure to sector \\(i\\). (When \\(s_{i}=0\\), the portfolio is said to be _neutral_ to sector \\(i\\).) An investment advisor specifies a set of desired sector exposures, given as the \\(m\\)-vector \\(s^{\\rm des}\\). Express the requirement \\(s=s^{\\rm des}\\) as a set of linear equations of the form \\(Ah=b\\). (You must describe the matrix \\(A\\) and the vector \\(b\\).) _Remark._ A typical practical case involves \\(n=1000\\) assets and \\(m=50\\) sectors. An advisor might specify \\(s_{i}^{\\rm des}=0\\) if she does not have an opinion as how companies in that sector will do in the future; she might specify a positive value for \\(s_{i}^{\\rm des}\\) if she thinks the companies in that sector will do well (_i.e._, generate positive returns) in the future, and a negative value if she thinks they will do poorly.\n**8.14**: _Affine combinations of solutions of linear equations._ Consider the set of \\(m\\) linear equations in \\(n\\) variables \\(Ax=b\\), where \\(A\\) is an \\(m\\times n\\) matrix, \\(b\\) is an \\(m\\)-vector, and \\(x\\) is the \\(n\\)-vector of variables. Suppose that the \\(n\\)-vectors \\(z_{1},\\ldots,z_{k}\\) are solutions of this set of equations, _i.e._, satisfy \\(Az_{i}=b\\). Show that if the coefficients \\(\\alpha_{1},\\ldots,\\alpha_{k}\\) satisfy \\(\\alpha_{1}+\\cdots+\\alpha_{k}=1\\), then the affine combination\n\nDocument 3:\n----\n* A list consisting of a single vector is linearly dependent only if the vector is zero. It is linearly independent only if the vector is nonzero.\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\n\nDocument 4:\n----\nIn chapter 11 we will address the question of how to determine if a system of linear equations has a solution, and how to find one when it does. For now, we give a few interesting examples.\n#### Examples\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\nBalancing chemical reactions.A chemical reaction involves \\(p\\) reactants (molecules) and \\(q\\) products, and can be written as\n\\[a_{1}R_{1}+\\cdots+a_{p}R_{p}\\longrightarrow b_{1}P_{1}+\\cdots+b_{q}P_{q}.\\]\nHere \\(R_{1},\\ldots,R_{p}\\) are the reactants, \\(P_{1},\\ldots,P_{q}\\) are the products, and the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\) are positive numbers that tell us how many of each of these molecules is involved in the reaction. They are typically integers, but can be scaled arbitrarily; we could double all of these numbers, for example, and we still have the same reaction. As a simple example, we have the electrolysis of water,\n\nDocument 5:\n----\n* _Recursive least squares_ is the following algorithm. For \\(k=n,\\ldots,m\\), compute \\(G^{(k+1)}\\) and \\(h^{(k+1)}\\) using (b); then compute \\(\\hat{x}^{(k)}\\) using (a). Work out the total flop count for this method, keeping only dominant terms. (You can include the cost of computing \\(G^{(n)}\\) and \\(h^{(n)}\\), which should be negligible in the total.) Compare to the flop count for the naive method.\n_Remark_.: A further trick called the matrix inversion lemma (which is beyond the scope of this book) can be used to reduce the complexity of recursive least squares to order \\(mn^{2}\\).\n**12.15**: _Minimizing a squared norm plus an affine function._ A generalization of the least squares problem (12.1) adds an affine function to the least squares objective,\n\\[\\text{minimize}\\quad\\|Ax-b\\|^{2}+c^{T}x+d,\\]\nwhere the \\(n\\)-vector \\(x\\) is the variable to be chosen, and the (given) data are the \\(m\\times n\\) matrix \\(A\\), the \\(m\\)-vector \\(b\\), the \\(n\\)-vector \\(c\\), and the number \\(d\\). We will use the same assumption we use in least squares: The columns of \\(A\\) are linearly independent. This generalized problem can be solved by reducing it to a standard least squares problem, using a trick called _completing the square_.\nShow that the objective of the problem above can be expressed in the form\n\\[\\|Ax-b\\|^{2}+c^{T}x+d=\\|Ax-b+f\\|^{2}+g,\\]\nfor some \\(m\\)-vector \\(f\\) and some constant \\(g\\). It follows that we can solve the generalized least squares problem by minimizing \\(\\|Ax-(b-f)\\|\\), an ordinary least squares problem with solution \\(\\hat{x}=A^{\\dagger}(b-f)\\).\n\n\n",
        "answer": [
            -1,
            3,
            1
        ],
        "rationale": {
            "Question": "For the two linear equations $2 * x + 3 * y + z = 8$ and $4 * x + 4 * y + 4z = 12$ and $x + y + 8z = 10$ with variables x, y and z. Use cramer's rule to solve these three variables.",
            "Answer": [
                -1,
                3,
                1
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/cramer's_rule2.json",
            "explanation": "NONE",
            "theorem": "cramer's rule",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "jianyu_xu/integer_programming_1.json",
        "question": "30 students from 5 classes solved 40 math problems. Each student must answer at least one question. Every two students in the same class solved the same number of questions. The number of questions answered by any two students in different classes is also different. Question: What's maximum possible number of students who only answered one question?",
        "prediction": "\\(e = 1\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\n30 students from 5 classes solved 40 math problems. Each student must answer at least one question. Every two students in the same class solved the same number of questions. The number of questions answered by any two students in different classes is also different. Question: What's maximum possible number of students who only answered one question?\n\nDocument 1:\n----\n3. \\(S^{T}S\\).\n4. \\(SS^{T}\\).\n**10.18**: _Students, classes, and majors._ We consider \\(m\\) students, \\(n\\) classes, and \\(p\\) majors. Each student can be in any number of the classes (although we'd expect the number to range from 3 to 6), and can have any number of the majors (although the common values would be 0, 1, or 2). The data about the students' classes and majors are given by an \\(m\\times n\\) matrix \\(C\\) and an \\(m\\times p\\) matrix \\(M\\), where \\[C_{ij}=\\left\\{\\begin{array}{ll}1&\\text{student $i$ is in class $j$}\\\\ 0&\\text{student $i$ is not in class $j$},\\end{array}\\right.\\] and \\[M_{ij}=\\left\\{\\begin{array}{ll}1&\\text{student $i$ is in major $j$}\\\\ 0&\\text{student $i$ is not in major $j$}.\\end{array}\\right.\\] 1. Let \\(E\\) be the \\(n\\)-vector with \\(E_{i}\\) being the enrollment in class \\(i\\). Express \\(E\\) using matrix notation, in terms of the matrices \\(C\\) and \\(M\\). 2. Define the \\(n\\times p\\) matrix \\(S\\) where \\(S_{ij}\\) is the total number of students in class \\(i\\) with major \\(j\\). Express \\(S\\) using matrix notation, in terms of the matrices \\(C\\) and \\(M\\).\n**10.19**: _Student group membership._ Let \\(G\\in{\\bf R}^{m\\times n}\\) represent a contingency matrix of \\(m\\) students who are members of \\(n\\) groups:\n\\[G_{ij}=\\left\\{\\begin{array}{ll}1&\\mbox{student $i$ is in group $j$}\\\\ 0&\\mbox{student $i$ is not in group $j$.}\\end{array}\\right.\\]\n\nDocument 2:\n----\n1. What can you say about \\(\\psi(1,-1)\\)? Either give the value of \\(\\psi(1,-1)\\), or state that it cannot be determined.\n2. What can you say about \\(\\psi(2,-2)\\)? Either give the value of \\(\\psi(2,-2)\\), or state that it cannot be determined. Justify your answers.\n**2.6**: _Questionnaire scoring._ A questionnaire in a magazine has 30 questions, broken into two sets of 15 questions. Someone taking the questionnaire answers each question with 'Rarely', 'Sometimes', or 'Often'. The answers are recorded as a 30-vector \\(a\\), with \\(a_{i}=1,2,3\\) if question \\(i\\) is answered Rarely, Sometimes, or Often, respectively. The total score on a completed questionnaire is found by adding up 1 point for every question answered Sometimes and 2 points for every question answered Often on questions 1-15, and by adding 2 points and 4 points for those responses on questions 16-30. (Nothing is added to the score for Rarely responses.) Express the total score \\(s\\) in the form of an affine function \\(s=w^{T}a+v\\), where \\(w\\) is a 30-vector and \\(v\\) is a scalar (number).\n**2.7**: _General formula for affine functions._ Verify that formula (2.4) holds for any affine function \\(f:{\\bf R}^{n}\\to{\\bf R}\\). You can use the fact that \\(f(x)=a^{T}x+b\\) for some \\(n\\)-vector \\(a\\) and scalar \\(b\\).\n\nDocument 3:\n----\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.23)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.24)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}[]\nImposing the condition that \\(a_{1}=1\\) we obtain a square set of 6 linear equations,\n\\[\\left[\\begin{array}{rrrrrr}2&0&0&-1&0&0\\\\ 7&0&0&0&0&-1\\\\ 0&1&0&0&-1&0\\\\ 0&0&1&0&0&-2\\\\ -2&2&1&-3&-3&0\\\\ 1&0&0&0&0&0\\end{array}\\right]\\left[\\begin{array}{c}a_{1}\\\\ a_{2}\\\\ a_{3}\\\\ b_{1}\\\\ b_{2}\\\\ b_{3}\\end{array}\\right]=\\left[\\begin{array}{c}0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\end{array}\\right].\\]\nSolving these equations we obtain\n\\[a_{1}=1,\\quad a_{2}=6,\\quad a_{3}=14,\\qquad b_{1}=2,\\quad b_{2}=6,\\quad b_{3}=7.\\]\n(Setting \\(a_{1}=1\\) could have yielded fractional values for the other coefficients, but in this case, it did not.) The balanced reaction is\n\\[\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+6\\mathrm{Fe}^{2+}+14\\mathrm{H}^{+} \\longrightarrow 2\\mathrm{Cr}^{3+}+6\\mathrm{Fe}^{3+}+7\\mathrm{H}_{2}\\mathrm{O}.\\]\nHeat diffusion.We consider a diffusion system as described on page 155. Some of the nodes have fixed potential, _i.e._, \\(e_{i}\\) is given; for the other nodes, the associated external source \\(s_{i}\\) is zero. This would model a thermal system in which some nodes are in contact with the outside world or a heat source, which maintains their temperatures (via external heat flows) at constant values; the other nodes are internal, and have no heat sources. This gives us a set of \\(n\\) additional equations:\n\nDocument 4:\n----\n**10.19**: _Student group membership._ Let \\(G\\in{\\bf R}^{m\\times n}\\) represent a contingency matrix of \\(m\\) students who are members of \\(n\\) groups:\n\\[G_{ij}=\\left\\{\\begin{array}{ll}1&\\mbox{student $i$ is in group $j$}\\\\ 0&\\mbox{student $i$ is not in group $j$.}\\end{array}\\right.\\]\n(A student can be in any number of the groups.)\n1. What is the meaning of the 3rd column of \\(G\\)?\n2. What is the meaning of the 15th row of \\(G\\)?\n3. Give a simple formula (using matrices, vectors, etc.) for the \\(n\\)-vector \\(M\\), where \\(M_{i}\\) is the total membership (_i.e._, number of students) in group \\(i\\).\n4. Interpret \\((GG^{T})_{ij}\\) in simple English.\n5. Interpret \\((G^{T}G)_{ij}\\) in simple English.\n**10.20**: _Products, materials, and locations._\\(P\\) different products each require some amounts of \\(M\\) different materials, and are manufactured in \\(L\\) different locations, which have different material costs. We let \\(C_{lm}\\) denote the cost of material \\(m\\) in location \\(l\\), for \\(l=1,\\ldots,L\\) and \\(m=1,\\ldots,M\\). We let \\(Q_{mp}\\) denote the amount of material \\(m\\) required to manufacture one unit of product \\(p\\), for \\(m=1,\\ldots,M\\) and \\(p=1,\\ldots,P\\). Let \\(T_{pl}\\) denote the total cost to manufacture product \\(p\\) in location \\(l\\), for \\(p=1,\\ldots,P\\) and \\(l=1,\\ldots,L\\). Give an expression for the matrix \\(T\\).\n**10.21**: _Integral of product of polynomials._ Let \\(p\\) and \\(q\\) be two quadratic polynomials, given by\n\nDocument 5:\n----\n* _ZIP code clustering._ Suppose that \\(x_{i}\\) is a vector giving \\(n\\) quantities or statistics for the residents of ZIP code \\(i\\), such as numbers of residents in various age groups, household size, education statistics, and income statistics. (In this example \\(N\\) is around 40000.) A clustering algorithm might be used to cluster the 40000 ZIP codes into, say, \\(k=100\\) groups of ZIP codes with similar statistics.\n* _Student clustering._ Suppose the vector \\(x_{i}\\) gives the detailed grading record of student \\(i\\) in a course, _i.e._, her grades on each question in the quizzes, homework assignments, and exams. A clustering algorithm might be used to cluster the students into \\(k=10\\) groups of students who performed similarly.\n* _Survey response clustering._ A group of \\(N\\) people respond to a survey with \\(n\\) questions. Each question contains a statement, such as 'The movie was too long', followed by some ordered options such as Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree. (This is called a _Likert scale_, named after the psychologist Rensis Likert.) Suppose the \\(n\\)-vector \\(x_{i}\\) encodes the selections of respondent \\(i\\) on the \\(n\\) questions, using the numerical coding \\(-2\\), \\(-1\\), \\(0\\), \\(+1\\), \\(+2\\) for the responses above. A clustering algorithm can be used to cluster the respondents into \\(k\\) groups, each with similar responses to the survey.\n\n\n",
        "answer": 26,
        "rationale": {
            "Question": "30 students from 5 classes solved 40 math problems. Each student must answer at least one question. Every two students in the same class solved the same number of questions. The number of questions answered by any two students in different classes is also different. Question: What's maximum possible number of students who only answered one question?",
            "Answer": 26,
            "Answer_type": "integer",
            "Picture": null,
            "source": "self",
            "id": "jianyu_xu/integer_programming_1.json",
            "explanation": "NONE",
            "theorem": "integer programming",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "mingyin/gaussian-elimination1.json",
        "question": "for the matrix $A=(\\begin{array}{rrrrr} 1 & 2 & 3 & 4 & -3 \\1 & 2 & 0 & -5 & 1 \\2 & 4 & -3 & -19 & 6 \\3 & 6 & -3 & -24 & 7\\end{array})$, what is its row rank and column rank? return the two numbers as a list.",
        "prediction": "\\([3, 3]\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nfor the matrix $A=(\\begin{array}{rrrrr} 1 & 2 & 3 & 4 & -3 \\1 & 2 & 0 & -5 & 1 \\2 & 4 & -3 & -19 & 6 \\3 & 6 & -3 & -24 & 7\\end{array})$, what is its row rank and column rank? return the two numbers as a list.\n\nDocument 1:\n----\nAn important attribute of a matrix is its _size_ or _dimensions_, _i.e._, the numbers of rows and columns. The matrix above has 3 rows and 4 columns, so its size is \\(3\\times 4\\). A matrix of size \\(m\\times n\\) is called an \\(m\\times n\\) matrix.\nThe _elements_ (or _entries_ or _coefficients_) of a matrix are the values in the array. The \\(i,j\\) element is the value in the \\(i\\)th row and \\(j\\)th column, denoted by double subscripts: the \\(i,j\\) element of a matrix \\(A\\) is denoted \\(A_{ij}\\) (or \\(A_{i,j}\\), when \\(i\\) or \\(j\\) is more than one digit or character). The positive integers \\(i\\) and \\(j\\) are called the (row and column) _indices_. If \\(A\\) is an \\(m\\times n\\) matrix, then the row index \\(i\\) runs from 1 to \\(m\\) and the column index \\(j\\) runs from 1 to \\(n\\). Row indices go from top to bottom, so row 1 is the top row and row \\(m\\) is the bottom row. Column indices go from left to right, so column 1 is the left column and column \\(n\\) is the right column.\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\n\nDocument 2:\n----\nAn important attribute of a matrix is its _size_ or _dimensions_, _i.e._, the numbers of rows and columns. The matrix above has 3 rows and 4 columns, so its size is \\(3\\times 4\\). A matrix of size \\(m\\times n\\) is called an \\(m\\times n\\) matrix.\nThe _elements_ (or _entries_ or _coefficients_) of a matrix are the values in the array. The \\(i,j\\) element is the value in the \\(i\\)th row and \\(j\\)th column, denoted by double subscripts: the \\(i,j\\) element of a matrix \\(A\\) is denoted \\(A_{ij}\\) (or \\(A_{i,j}\\), when \\(i\\) or \\(j\\) is more than one digit or character). The positive integers \\(i\\) and \\(j\\) are called the (row and column) _indices_. If \\(A\\) is an \\(m\\times n\\) matrix, then the row index \\(i\\) runs from 1 to \\(m\\) and the column index \\(j\\) runs from 1 to \\(n\\). Row indices go from top to bottom, so row 1 is the top row and row \\(m\\) is the bottom row. Column indices go from left to right, so column 1 is the left column and column \\(n\\) is the right column.\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\nTwo matrices are equal if they have the same size, and the corresponding entries are all equal. As with vectors, we normally deal with matrices with entries that\n\nDocument 3:\n----\n\\[A=\\left[\\begin{array}{cccc}a_{1}&a_{2}&\\cdots&a_{n}\\end{array}\\right],\\]\nwhere \\(a_{j}\\), which is an \\(m\\)-vector, is the \\(j\\)th column of \\(A\\). Thus, an \\(m\\times n\\) matrix can be viewed as its \\(n\\) columns, concatenated.\nSimilarly, an \\(m\\times n\\) matrix \\(A\\) can be written as a block matrix with one block column and \\(m\\) block rows:\n\\[A=\\left[\\begin{array}{c}b_{1}\\\\ b_{2}\\\\ \\vdots\\\\ b_{m}\\end{array}\\right],\\]\nwhere \\(b_{i}\\), which is a row \\(n\\)-vector, is the \\(i\\)th row of \\(A\\). In this notation, the matrix \\(A\\) is interpreted as its \\(m\\) rows, stacked.\n## Examples\nTable interpretation.The most direct interpretation of a matrix is as a table of numbers that depend on two indices, \\(i\\) and \\(j\\). (A vector is a list of numbers that depend on only one index.) In this case the rows and columns of the matrix usually have some simple interpretation. Some examples are given below.\n* _Images._ A black and white image with \\(M\\times N\\) pixels is naturally represented as an \\(M\\times N\\) matrix. The row index \\(i\\) gives the vertical position of the pixel, the column index \\(j\\) gives the horizontal position of the pixel, and the \\(i,j\\) entry gives the pixel value.\n* _Rainfall data._ An \\(m\\times n\\) matrix \\(A\\) gives the rainfall at \\(m\\) different locations on \\(n\\) consecutive days, so \\(A_{42}\\) (which is a number) is the rainfall at location 4 on day 2. The \\(j\\)th column of \\(A\\), which is an \\(m\\)-vector, gives the rainfall at the \\(m\\) locations on day \\(j\\). The \\(i\\)th row of \\(A\\), which is an \\(n\\)-row-vector, is the time series of rainfall at location \\(i\\).\n\nDocument 4:\n----\nTwo matrices are equal if they have the same size, and the corresponding entries are all equal. As with vectors, we normally deal with matrices with entries that\nare real numbers, which will be our assumption unless we state otherwise. The set of real \\(m\\times n\\) matrices is denoted \\(\\mathbf{R}^{m\\times n}\\). But matrices with complex entries, for example, do arise in some applications.\nMatrix indexing.As with vectors, standard mathematical notation indexes the rows and columns of a matrix starting from 1. In computer languages, matrices are often (but not always) stored as 2-dimensional arrays, which can be indexed in a variety of ways, depending on the language. Lower level languages typically use indices starting from 0; higher level languages and packages that support matrix operations usually use standard mathematical indexing, starting from 1.\nSquare, tall, and wide matrices.A _square_ matrix has an equal number of rows and columns. A square matrix of size \\(n\\times n\\) is said to be of _order_\\(n\\). A _tall_ matrix has more rows than columns (size \\(m\\times n\\) with \\(m>n\\)). A _wide_ matrix has more columns than rows (size \\(m\\times n\\) with \\(n>m\\)).\nColumn and row vectors.An \\(n\\)-vector can be interpreted as an \\(n\\times 1\\) matrix; we do not distinguish between vectors and matrices with one column. A matrix with only one row, _i.e._, with size \\(1\\times n\\), is called a _row vector_; to give its size, we can refer to it as an _\\(n\\)-row-vector_. As an example,\n\nDocument 5:\n----\n_Remark_.: Suppose the vertices represent all people on earth, and the graph edges represent acquaintance, _i.e._, \\(A_{ij}=1\\) if person \\(j\\) and person \\(i\\) are acquainted. (This graph is symmetric.) Even though \\(n\\) is measured in billions, the diameter of this acquaintance graph is thought to be quite small, perhaps 6 or 7. In other words, any two people on earth can be connected though a set of 6 or 7 (or fewer) acquaintances. This idea, originally conjectured in the 1920s, is sometimes called _six degrees of separation_.\n**10.32**: _Matrix exponential._ You may know that for any real number \\(a\\), the sequence \\((1+a/k)^{k}\\) converges as \\(k\\to\\infty\\) to the exponential of \\(a\\), denoted \\(\\exp a\\) or \\(e^{a}\\). The _matrix exponential_ of a square matrix \\(A\\) is defined as the limit of the matrix sequence \\((I+A/k)^{k}\\) as \\(k\\to\\infty\\). (It can shown that this sequence always converges.) The matrix exponential arises in many applications, and is covered in more advanced courses on linear algebra.\n1. Find \\(\\exp 0\\) (the zero matrix) and \\(\\exp I\\).\n2. Find \\(\\exp A\\), for \\(A=\\left[\\begin{array}{cc}0&1\\\\ 0&0\\end{array}\\right]\\).\n**10.33**: _Matrix equations._ Consider two \\(m\\times n\\) matrices \\(A\\) and \\(B\\). Suppose that for \\(j=1,\\ldots,n\\), the \\(j\\)th column of \\(A\\) is a linear combination of the first \\(j\\) columns of \\(B\\). How do we express this as a matrix equation? Choose one of the matrix equations below and justify your choice.\n1. \\(A=GB\\) for some upper triangular matrix \\(G\\).\n2. \\(A=BH\\) for some upper triangular matrix \\(H\\).\n3. \\(A=FB\\) for some lower triangular matrix \\(F\\).\n\n\n",
        "answer": [
            2,
            2
        ],
        "rationale": {
            "Question": "for the matrix $A=(\\begin{array}{rrrrr} 1 & 2 & 3 & 4 & -3 \\1 & 2 & 0 & -5 & 1 \\2 & 4 & -3 & -19 & 6 \\3 & 6 & -3 & -24 & 7\\end{array})$, what is its row rank and column rank? return the two numbers as a list.",
            "Answer": [
                2,
                2
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "linear algebra 2.2 example 3",
            "id": "mingyin/gaussian-elimination1.json",
            "explanation": "NONE",
            "theorem": "gaussian elimination",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "wenhuchen/gauss_lemma.json",
        "question": "Is 7 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
        "prediction": "False",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs 7 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.\n\nDocument 1:\n----\nThe constrained least squares problem can also be thought of as a limit of a bi-objective least squares problem, with primary objective \\(\\|Ax-b\\|^{2}\\) and secondary objective \\(\\|Cx-d\\|^{2}\\). Roughly speaking, we put infinite weight on the second objective, so that any nonzero value is unacceptable (which forces \\(x\\) to satisfy \\(Cx=d\\)). So we would expect (and it can be verified) that minimizing the weighted objective\n\\[\\|Ax-b\\|^{2}+\\lambda\\|Cx-d\\|^{2},\\]\nfor a very large value of \\(\\lambda\\) yields a vector close to a solution of the constrained least squares problem (16.1). We will encounter this idea again in chapter 19, when we consider the nonlinear constrained least squares problem.\nExample.In figure 16.1 we fit a _piecewise-polynomial_ function \\(\\hat{f}(x)\\) to a set of \\(N=140\\) points \\((x_{i},y_{i})\\) in the plane. The function \\(\\hat{f}(x)\\) is defined as\n\\[\\hat{f}(x)=\\left\\{\\begin{array}{ll}p(x)&x\\leq a\\\\ q(x)&x>a,\\end{array}\\right.\\]\nwith \\(a\\) given, and \\(p(x)\\) and \\(q(x)\\) polynomials of degree three or less,\n\\[p(x)=\\theta_{1}+\\theta_{2}x+\\theta_{3}x^{2}+\\theta_{4}x^{3},\\qquad q(x)=\\theta _{5}+\\theta_{6}x+\\theta_{7}x^{2}+\\theta_{8}x^{3}.\\]\nWe also impose the condition that \\(p(a)=q(a)\\) and \\(p^{\\prime}(a)=q^{\\prime}(a)\\), so that \\(\\hat{f}(x)\\) is continuous and has a continuous first derivative at \\(x=a\\). Suppose the \\(N\\) data\nFigure 16.1: Least squares fit of two cubic polynomials to 140 points, with continuity constraints \\(p(a)=q(a)\\) and \\(p^{\\prime}(a)=q^{\\prime}(a)\\).\npoints \\((x_{i},y_{i})\\) are numbered so that \\(x_{1},\\ldots,x_{M}\\leq a\\) and \\(x_{M+1},\\ldots,x_{N}>a\\). The sum of squares of the prediction errors is\n\\[\\sum_{i=1}^{M}(\\theta_{1}+\\theta_{2}x_{i}+\\theta_{3}x_{i}^{2}+\\theta_{4}x_{i}^{ 3}-y_{i})^{2}+\\sum_{i=M+1}^{N}(\\theta_{5}+\\theta_{6}x_{i}+\\theta_{7}x_{i}^{2}+ \\theta_{8}x_{i}^{3}-y_{i})^{2}.\\]\nThe conditions \\(p(a)-q(a)=0\\) and \\(p^{\\prime}(a)-q^{\\prime}(a)=0\\) are two linear equations\n\\[\\theta_{1}+\\theta_{2}a+\\theta_{3}a^{2}+\\theta_{4}a^{3}-\\theta_{5} -\\theta_{6}a-\\theta_{7}a^{2}-\\theta_{8}a^{3} = 0\\] \\[\\theta_{2}+2\\theta_{3}a+3\\theta_{4}a^{2}-\\theta_{6}-2\\theta_{7}a- 3\\theta_{8}a^{2} = 0.\\]\nWe can determine the coefficients \\(\\hat{\\theta}=(\\hat{\\theta}_{1},\\ldots,\\hat{\\theta}_{8})\\) that minimize the sum of squares of the prediction errors, subject to the continuity constraints, by solving a constrained least squares problem\n\nDocument 2:\n----\nwhere \\(\\lambda_{i}>0\\) are weights, and \\(J_{2},\\ldots,J_{p}\\) are the regularization terms. This is called _regularized inversion_ or _regularized estimation_. We may repeat this for several choices of the weights, and choose the best estimate for the particular application.\nTikhonov regularized inversion.Choosing \\(\\hat{x}\\) to minimize\n\\[\\|Ax-y\\|^{2}+\\lambda\\|x\\|^{2}\\]\nfor some choice of \\(\\lambda>0\\) is called _Tikhonov regularized inversion_, after the mathematician Andrey Tikhonov. Here we seek a guess \\(\\hat{x}\\) that is consistent with the measurements (_i.e._, \\(\\|A\\hat{x}-y\\|^{2}\\) is small), but not too big.\nThe stacked matrix in this case,\n\\[\\tilde{A}=\\left[\\begin{array}{c}A\\\\ \\sqrt{\\lambda}I\\end{array}\\right],\\]\nalways has linearly independent columns, without any assumption about \\(A\\), which can have any dimensions, and need not have linearly independent columns. To see this we note that \\(\\tilde{A}x=(Ax,\\sqrt{\\lambda}x)=0\\) implies that \\(\\sqrt{\\lambda}x=0\\), which implies \\(x=0\\). The Gram matrix associated with \\(\\tilde{A}\\),\n\\[\\tilde{A}^{T}\\tilde{A}=A^{T}A+\\lambda I,\\]\nis therefore always invertible (provided \\(\\lambda>0\\)). The Tikhonov regularized approximate solution is then\n\\[\\hat{x}=(A^{T}A+\\lambda I)^{-1}A^{T}b.\\]\nEqualization.The vector \\(x\\) represents a transmitted signal or message, consisting of \\(n\\) real values. The matrix \\(A\\) represents the mapping from the transmitted signal to what is received (called the _channel_); \\(y=Ax+v\\) includes noise as well as the action of the channel. Guessing what \\(x\\) is, given \\(y\\), can be thought of as un-doing the effects of the channel. In this context, estimation is called _equalization_.\n#### Estimating a periodic time series\n\nDocument 3:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\nBalancing chemical reactions.(See page 154 for background.) We consider the problem of balancing the chemical reaction\n\\[a_{1}\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+a_{2}\\mathrm{Fe}^{2+}+a_{3}\\mathrm{H} ^{+}\\longrightarrow b_{1}\\mathrm{Cr}^{3+}+b_{2}\\mathrm{Fe}^{3+}+b_{3} \\mathrm{H}_{2}\\mathrm{O},\\]\n\nDocument 4:\n----\n1. \\(\\tilde{q}_{i}\\neq 0\\), so the linear dependence test in step 2 is not satisfied, and we do not have a divide-by-zero error in step 3.\n2. \\(q_{1},\\ldots,q_{i}\\) are orthonormal.\n3. \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i}\\).\n4. \\(q_{i}\\) is a linear combination of \\(a_{1},\\ldots,a_{i}\\).\nWe show this by induction. For \\(i=1\\), we have \\(\\tilde{q}_{1}=a_{1}\\). Since \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we must have \\(a_{1}\\neq 0\\), and therefore \\(\\tilde{q}_{1}\\neq 0\\), so assertion 1 holds. The single vector \\(q_{1}\\) (considered as a list with one element) is evidently orthonormal, since \\(\\|q_{1}\\|=1\\), so assertion 2 holds. We have \\(a_{1}=\\|\\tilde{q}_{1}\\|q_{1}\\), and \\(q_{1}=(1/\\|\\tilde{q}_{1}\\|)a_{1}\\), so assertions 3 and 4 hold.\nSuppose our assertion holds for some \\(i-1\\), with \\(i<k\\); we will show it holds for \\(i\\). If \\(\\tilde{q}_{i}=0\\), then \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i-1}\\) (from the first step in the algorithm); but each of these is (by the induction hypothesis) a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), so it follows that \\(a_{i}\\) is a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), which contradicts our assumption that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. So assertion 1 holds for \\(i\\).\nStep 3 of the algorithm ensures that \\(q_{1},\\ldots,q_{i}\\) are normalized; to show they are orthogonal we will show that \\(q_{i}\\perp q_{j}\\) for \\(j=1,\\ldots,i-1\\). (Our induction hypothesis tells us that \\(q_{r}\\perp q_{s}\\) for \\(r,s<i\\).) For any \\(j=1,\\ldots,i-1\\), we have (using step 1)\n\\[q_{j}^{T}\\tilde{q}_{i} = q_{j}^{T}a_{i}-(q_{1}^{T}a_{i})(q_{j}^{T}q_{1})-\\cdots-(q_{i-1}^ {T}a_{i})(q_{j}^{T}q_{i-1})\\] \\[= q_{j}^{T}a_{i}-q_{j}^{T}a_{i}=0,\\]\n\nDocument 5:\n----\n\\[f(\\alpha x+\\beta y) = a^{T}(\\alpha x+\\beta y)+b\\] \\[= \\alpha a^{T}x+\\beta a^{T}y+(\\alpha+\\beta)b\\] \\[= \\alpha(a^{T}x+b)+\\beta(a^{T}y+b)\\] \\[= \\alpha f(x)+\\beta f(y).\\]\n(In the second line we use \\(\\alpha+\\beta=1\\).)\nThis restricted superposition property for affine functions is useful in showing that a function \\(f\\) is _not_ affine: We find vectors \\(x\\), \\(y\\), and numbers \\(\\alpha\\) and \\(\\beta\\) with \\(\\alpha+\\beta=1\\), and verify that \\(f(\\alpha x+\\beta y)\\neq\\alpha f(x)+\\beta f(y)\\). This shows that \\(f\\) cannot be affine. As an example, we verified above that superposition does not hold for the maximum function (with \\(n>1\\)); the coefficients in our counterexample are \\(\\alpha=\\beta=1/2\\), which sum to one, which allows us to conclude that the maximum function is not affine.\nThe converse is also true: Any scalar-valued function that satisfies the restricted superposition property is affine. An analog of the formula (2.3) is\n\\[f(x)=f(0)+x_{1}\\left(f(e_{1})-f(0)\\right)+\\cdots+x_{n}\\left(f(e_{n})-f(0)\\right),\\] (2.4)\nwhich holds when \\(f\\) is affine, and \\(x\\) is any \\(n\\)-vector. (See exercise 2.7.) This formula shows that for an affine function, once we know the \\(n+1\\) numbers \\(f(0)\\), \\(f(e_{1})\\), ..., \\(f(e_{n})\\), we can predict (or reconstruct or evaluate) \\(f(x)\\) for any \\(n\\)-vector \\(x\\). It also shows how the vector \\(a\\) and constant \\(b\\) in the representation \\(f(x)=a^{T}x+b\\) can be found from the function \\(f\\): \\(a_{i}=f(e_{i})-f(0)\\), and \\(b=f(0)\\).\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Is 7 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | http://mathonline.wikidot.com/example-questions-regarding-gauss-s-lemma",
            "id": "wenhuchen/gauss_lemma.json",
            "explanation": "NONE",
            "theorem": "gauss's lemma",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "mingyin/Sylveete-rank-inequality1.json",
        "question": "Both A, B are n-by-n matrices with rank(A)=n, rank(A*B)=0. What is rank(B)?",
        "prediction": "0",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nBoth A, B are n-by-n matrices with rank(A)=n, rank(A*B)=0. What is rank(B)?\n\nDocument 1:\n----\nwhere \\(x_{j}\\) is the \\(j\\)th column of \\(X\\) and \\(b_{j}\\) is the \\(j\\)th column of \\(B\\). (Here we use the property that the square of the matrix norm is the sum of the squared norms of the columns of the matrix.) So the objective is a sum of \\(k\\) terms, with each term depending on only one column of \\(X\\). It follows that we can choose the columns \\(x_{j}\\) independently, each one by minimizing its associated term \\(\\|Ax_{j}-b_{j}\\|^{2}\\). Assuming that \\(A\\) has linearly independent columns, the solution is \\(\\hat{x}_{j}=A^{\\dagger}b_{j}\\). The solution of the matrix least squares problem is therefore\n\\[\\hat{X} = \\left[\\begin{array}{cccc}\\hat{x}_{1}&\\cdots&\\hat{x}_{k}\\end{array}\\right]\\] (12.12) \\[= \\left[\\begin{array}{cccc}A^{\\dagger}b_{1}&\\cdots&A^{\\dagger}b_{ k}\\end{array}\\right]\\] \\[= A^{\\dagger}\\left[\\begin{array}{cccc}b_{1}&\\cdots&b_{k}\\end{array}\\right]\\] \\[= A^{\\dagger}B.\\]\nThe very simple solution \\(\\hat{X}=A^{\\dagger}B\\) of the matrix least squares problem agrees with the solution of the ordinary least squares problem when \\(k=1\\) (as it must). Many software packages for linear algebra use the backslash operator \\(A\\backslash B\\) to denote \\(A^{\\dagger}B\\), but this is not standard mathematical notation.\nThe matrix least squares problem can be solved efficiently by exploiting the fact that algorithm 12.1 is another example of a factor-solve algorithm. To compute \\(\\hat{X}=A^{\\dagger}B\\) we carry out the QR factorization of \\(A\\) once; we carry out steps 2 and 3 of algorithm 12.1 for each of the \\(k\\) columns of \\(B\\). The total cost is \\(2mn^{2}+k(2mn+n^{2})\\) flops. When \\(k\\) is small compared to \\(n\\) this is roughly \\(2mn^{2}\\) flops, the same cost as solving a single least squares problem (_i.e._, one with a vector right-hand side).\n\nDocument 2:\n----\nThus, the columns of \\(AB\\) are the matrix-vector products of \\(A\\) and the columns of \\(B\\). The product \\(AB\\) can be interpreted as the matrix obtained by 'applying' \\(A\\) to each of the columns of \\(B\\).\nMultiple sets of linear equations.We can use the column interpretation of matrix multiplication to express a set of \\(k\\) linear equations with the same \\(m\\times n\\) coefficient matrix \\(A\\),\n\\[Ax_{i}=b_{i},\\quad i=1,\\ldots,k,\\]\nin the compact form\n\\[AX=B,\\]\nwhere \\(X=[x_{1}\\;\\cdots\\;x_{k}]\\) and \\(B=[b_{1}\\;\\cdots\\;b_{k}]\\). The matrix equation \\(AX=B\\) is sometimes called a _linear equation with matrix right-hand side_, since it looks like \\(Ax=b\\), but \\(X\\) (the variable) and \\(B\\) (the right-hand side) are now \\(n\\times k\\) matrices, instead of \\(n\\)-vectors (which are \\(n\\times 1\\) matrices).\nRow interpretation of matrix-matrix product.We can give an analogous row interpretation of the product \\(AB\\), by partitioning \\(A\\) and \\(AB\\) as block matrices with row vector blocks. Let \\(a_{1}^{T},\\ldots,a_{m}^{T}\\) be the rows of \\(A\\). Then we have\n\\[AB=\\left[\\begin{array}{c}a_{1}^{T}\\\\ a_{2}^{T}\\\\ \\vdots\\\\ a_{m}^{T}\\end{array}\\right]B=\\left[\\begin{array}{c}a_{1}^{T}B\\\\ a_{2}^{T}B\\\\ \\vdots\\\\ a_{m}^{T}B\\end{array}\\right]=\\left[\\begin{array}{c}(B^{T}a_{1})^{T}\\\\ (B^{T}a_{2})^{T}\\\\ \\vdots\\\\ (B^{T}a_{m})^{T}\\end{array}\\right].\\]\nThis shows that the rows of \\(AB\\) are obtained by applying \\(B^{T}\\) to the transposed row vectors \\(a_{k}\\) of \\(A\\), and transposing the result.\nInner product representation.From the definition of the \\(i,j\\) element of \\(AB\\) in (10.1), we also see that the elements of \\(AB\\) are the inner products of the rows of \\(A\\) with the columns of \\(B\\):\n\nDocument 3:\n----\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n**10.11**: _Trace of matrix-matrix product_. The sum of the diagonal entries of a square matrix is called the _trace_ of the matrix, denoted \\(\\mathbf{tr}(A)\\).\n1. Suppose \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices. Show that \\[\\mathbf{tr}(A^{T}B)=\\sum_{i=1}^{m}\\sum_{j=1}^{n}A_{ij}B_{ij}.\\] What is the complexity of calculating \\(\\mathbf{tr}(A^{T}B)\\)?\n2. The number \\(\\mathbf{tr}(A^{T}B)\\) is sometimes referred to as the inner product of the matrices \\(A\\) and \\(B\\). (This allows us to extend concepts like angle to matrices.) Show that \\(\\mathbf{tr}(A^{T}B)=\\mathbf{tr}(B^{T}A)\\).\n3. Show that \\(\\mathbf{tr}(A^{T}A)=\\left\\|A\\right\\|^{2}\\). In other words, the square of the norm of a matrix is the trace of its Gram matrix.\n\nDocument 4:\n----\nIn chapter 11 we will address the question of how to determine if a system of linear equations has a solution, and how to find one when it does. For now, we give a few interesting examples.\n#### Examples\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\nBalancing chemical reactions.A chemical reaction involves \\(p\\) reactants (molecules) and \\(q\\) products, and can be written as\n\\[a_{1}R_{1}+\\cdots+a_{p}R_{p}\\longrightarrow b_{1}P_{1}+\\cdots+b_{q}P_{q}.\\]\nHere \\(R_{1},\\ldots,R_{p}\\) are the reactants, \\(P_{1},\\ldots,P_{q}\\) are the products, and the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\) are positive numbers that tell us how many of each of these molecules is involved in the reaction. They are typically integers, but can be scaled arbitrarily; we could double all of these numbers, for example, and we still have the same reaction. As a simple example, we have the electrolysis of water,\n\nDocument 5:\n----\nFactor-solve methods with multiple right-hand sides.Now suppose that we must solve several sets of linear equations,\n\\[Ax_{1}=b_{1},\\ \\ \\ldots,\\ Ax_{k}=b_{k},\\]\nall with the same coefficient matrix \\(A\\), but different right-hand sides. We can express this as the matrix equation \\(AX=B\\), where \\(X\\) is the \\(n\\times k\\) matrix with columns \\(x_{1},\\ldots,x_{k}\\), and \\(B\\) is the \\(n\\times k\\) matrix with columns \\(b_{1},\\ldots,b_{k}\\) (see page 180). Assuming \\(A\\) is invertible, the solution of \\(AX=B\\) is \\(X=A^{-1}B\\).\nA naive way to solve the \\(k\\) problems \\(Ax_{i}=b_{i}\\) (or in matrix notation, compute \\(X=A^{-1}B\\)) is to apply algorithm 11.2\\(k\\) times, which costs \\(2kn^{3}\\) flops. A more efficient method exploits the fact that \\(A\\) is the same matrix in each problem, so we can re-use the matrix factorization in step 1 and only need to repeat steps 2 and 3 to compute \\(\\tilde{x}_{k}=R^{-1}Q^{T}b_{k}\\) for \\(l=1,\\ldots,k\\). (This is sometimes referred to as _factorization caching_, since we save or cache the factorization after carrying it out, for later use.) The cost of this method is \\(2n^{3}+3kn^{2}\\) flops, or approximately \\(2n^{3}\\) flops if \\(k\\ll n\\). The (surprising) conclusion is that we can solve _multiple_ sets of linear equations, with the same coefficient matrix \\(A\\), at essentially the same cost as solving _one_ set of linear equations.\n\n\n",
        "answer": 0.0,
        "rationale": {
            "Question": "Both A, B are n-by-n matrices with rank(A)=n, rank(A*B)=0. What is rank(B)?",
            "Answer": 0.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 4.6 example 6",
            "id": "mingyin/Sylveete-rank-inequality1.json",
            "explanation": "NONE",
            "theorem": "sylveeter rank inequality",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float"
    },
    {
        "id": "elainewan/math_algebra_7.json",
        "question": "Let A be an invertible n * n matrix and v and eigenvector of both A and B, is v necesarily an eigenvector of A + B?",
        "prediction": "False",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet A be an invertible n * n matrix and v and eigenvector of both A and B, is v necesarily an eigenvector of A + B?\n\nDocument 1:\n----\n\\[x=A^{-1}b\\] (11.1)\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\nInvertibility conditions.For square matrices, left-invertibility, right-invertibility, and invertibility are equivalent: If a matrix is square and left-invertible, then it is also right-invertible (and therefore invertible) and vice-versa.\nTo see this, suppose \\(A\\) is an \\(n\\times n\\) matrix and left-invertible. This implies that the \\(n\\) columns of \\(A\\) are linearly independent. Therefore they form a basis and so any \\(n\\)-vector can be expressed as a linear combination of the columns of \\(A\\). In particular, each of the \\(n\\) unit vectors \\(e_{i}\\) can be expressed as \\(e_{i}=Ab_{i}\\) for some \\(n\\)-vector \\(b_{i}\\). The matrix \\(B=\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\) satisfies\n\\[AB=\\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right]= \\left[\\begin{array}{cccc}e_{1}&e_{2}&\\cdots&e_{n}\\end{array}\\right]=I.\\]\nSo \\(B\\) is a right inverse of \\(A\\).\nWe have just shown that for a square matrix \\(A\\),\n\nDocument 2:\n----\n**11.10**: _Reverse-time linear dynamical system._ A linear dynamical system has the form \\[x_{t+1}=Ax_{t},\\] where \\(x_{t}\\) in the (\\(n\\)-vector) state in period \\(t\\), and \\(A\\) is the \\(n\\times n\\) dynamics matrix. This formula gives the state in the next period as a function of the current state. We want to derive a recursion of the form \\[x_{t-1}=A^{\\rm rev}x_{t},\\] which gives the previous state as a function of the current state. We call this the _reverse time linear dynamical system_. 1. When is this possible? When it is possible, what is \\(A^{\\rm rev}\\)? 2. For the specific linear dynamical system with dynamics matrix \\[A=\\left[\\begin{array}{cc}3&2\\\\ -1&4\\end{array}\\right],\\] find \\(A^{\\rm rev}\\), or explain why the reverse time linear dynamical system doesn't exist.\n**11.11**: _Interpolation of rational functions._ (Continuation of exercise 8.8.) Find a rational function \\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}}\\] that satisfies the following interpolation conditions: \\[f(1)=2,\\qquad f(2)=5,\\qquad f(3)=9,\\qquad f(4)=-1,\\qquad f(5)=-4.\\] In exercise 8.8 these conditions were expressed as a set of linear equations in the coefficients \\(c_{1}\\), \\(c_{2}\\), \\(c_{3}\\), \\(d_{1}\\) and \\(d_{2}\\); here we are asking you to form and (numerically) solve the system of equations. Plot the rational function you find over the range \\(x=0\\) to \\(x=6\\). Your plot should include markers at the interpolation points \\((1,2),\\ldots,(5,-4)\\). (Your rational function graph should pass through these points.)\n**11.12**: _Combinations of invertible matrices._ Suppose the \\(n\\times n\\) matrices \\(A\\) and \\(B\\) are both invertible. Determine whether each of the matrices given below is invertible, without any further assumptions about \\(A\\) and \\(B\\).\n1. \\(A+B\\).\n\nDocument 3:\n----\n* A diagonal matrix \\(A\\) is invertible if and only if its diagonal entries are nonzero. The inverse of an \\(n\\times n\\) diagonal matrix \\(A\\) with nonzero diagonal entries is \\[A^{-1}=\\left[\\begin{array}{cccc}1/A_{11}&0&\\cdots&0\\\\ 0&1/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&1/A_{nn}\\end{array}\\right],\\] since \\[AA^{-1}=\\left[\\begin{array}{cccc}A_{11}/A_{11}&0&\\cdots&0\\\\ 0&A_{22}/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&A_{nn}/A_{nn}\\end{array}\\right]=I.\\] In compact notation, we have \\[\\mathbf{diag}(A_{11},\\ldots,A_{nn})^{-1}=\\mathbf{diag}(A_{11}^{-1},\\ldots,A_{ nn}^{-1}).\\] Note that the inverse on the left-hand side of this equation is the matrix inverse, while the inverses appearing on the right-hand side are scalar inverses.\n* As a non-obvious example, the matrix \\[A=\\left[\\begin{array}{cccc}1&-2&3\\\\ 0&2&2\\\\ -3&-4&-4\\end{array}\\right]\\] is invertible, with inverse \\[A^{-1}=\\frac{1}{30}\\left[\\begin{array}{cccc}0&-20&-10\\\\ -6&5&-2\\\\ 6&10&2\\end{array}\\right].\\] This can be verified by checking that \\(AA^{-1}=I\\) (or that \\(A^{-1}A=I\\), since either of these implies the other).\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\nInverse of matrix transpose.If \\(A\\) is invertible, its transpose \\(A^{T}\\) is also invertible and its inverse is \\((A^{-1})^{T}\\):\n\\[(A^{T})^{-1}=(A^{-1})^{T}.\\]\nSince the order of the transpose and inverse operations does not matter, this matrix is sometimes written as \\(A^{-T}\\).\n\nDocument 4:\n----\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\nInverse of matrix transpose.If \\(A\\) is invertible, its transpose \\(A^{T}\\) is also invertible and its inverse is \\((A^{-1})^{T}\\):\n\\[(A^{T})^{-1}=(A^{-1})^{T}.\\]\nSince the order of the transpose and inverse operations does not matter, this matrix is sometimes written as \\(A^{-T}\\).\nInverse of matrix product.If \\(A\\) and \\(B\\) are invertible (hence, square) and of the same size, then \\(AB\\) is invertible, and\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\n\nDocument 5:\n----\n1. \\(A\\) is tall with linearly independent columns.\n2. \\(A\\) is wide with linearly independent rows.\n3. \\(A\\) is square and invertible.\n**11.23**: _Product of pseudo-inverses._ Suppose \\(A\\) and \\(D\\) are right-invertible matrices and the product \\(AD\\) exists. We have seen that if \\(B\\) is a right inverse of \\(A\\) and \\(E\\) is a right inverse of \\(D\\), then \\(EB\\) is a right inverse of \\(AD\\). Now suppose \\(B\\) is the pseudo-inverse of \\(A\\) and \\(E\\) is the pseudo-inverse of \\(D\\). Is \\(EB\\) the pseudo-inverse of \\(AD\\)? Prove that this is always true or give an example for which it is false.\n**11.24**: _Simultaneous left inverse._ The two matrices\nand both left-invertible, and have multiple left inverses. Do they have a common left inverse? Explain how to find a \\(2\\times 4\\) matrix \\(C\\) that satisfies \\(CA=CB=I\\), or determine that no such matrix exists. (You can use numerical computing to find \\(C\\).) _Hint._ Set up a set of linear equations for the entries of \\(C\\). _Remark._ There is nothing special about the particular entries of the two matrices \\(A\\) and \\(B\\).\n* _Checking the computed solution of linear equations._ One of your colleagues says that whenever you compute the solution \\(x\\) of a square set of \\(n\\) equations \\(Ax=b\\) (say, using QR factorization), you should compute the number \\(\\|Ax-b\\|\\) and check that it is small. (It is not exactly zero due to the small rounding errors made in floating point computations.) Another colleague says that this would be nice to do, but the additional cost of computing \\(\\|Ax-b\\|\\) is too high. Briefly comment on your colleagues' advice. Who is right?\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Let A be an invertible n * n matrix and v and eigenvector of both A and B, is v necesarily an eigenvector of A + B?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "wenhuchen/vieta's_formula2.json",
        "question": "The polynomial $x^3 - Ax + 15$ has three real roots. Two of these roots sum to 5. What is |A|?",
        "prediction": "$\\boxed{5}$",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nThe polynomial $x^3 - Ax + 15$ has three real roots. Two of these roots sum to 5. What is |A|?\n\nDocument 1:\n----\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\nDocument 2:\n----\n1. Show that every vector \\(x\\) can be decomposed in a unique way as a sum \\(x=x_{\\rm s}+x_{\\rm a}\\) of a symmetric vector \\(x_{\\rm s}\\) and an anti-symmetric vector \\(x_{\\rm a}\\).\n2. Show that the symmetric and anti-symmetric parts \\(x_{\\rm s}\\) and \\(x_{\\rm a}\\) are linear functions of \\(x\\). Give matrices \\(A_{\\rm s}\\) and \\(A_{\\rm a}\\) such that \\(x_{\\rm s}=A_{\\rm s}x\\) and \\(x_{\\rm a}=A_{\\rm a}x\\) for all \\(x\\).\n**8.6**: _Linear functions._ For each description of \\(y\\) below, express it as \\(y=Ax\\) for some \\(A\\). (You should specify \\(A\\).)\n1. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average of \\(x_{1},\\ldots,x_{i-1}\\). (We take \\(y_{1}=x_{1}\\).)\n2. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average value of all other \\(x_{j}\\)s, _i.e._, the average of \\(x_{1},\\ldots,x_{i-1},x_{i+1},\\ldots,x_{n}\\).\n**8.7**: _Interpolation of polynomial values and derivatives._ The 5-vector \\(c\\) represents the coefficients of a quartic polynomial \\(p(x)=c_{1}+c_{2}x+c_{3}x^{2}+c_{4}x^{3}+c_{5}x^{4}\\). Express the conditions \\[p(0)=0,\\quad p^{\\prime}(0)=0,\\quad p(1)=1,\\quad p^{\\prime}(1)=0,\\] as a set of linear equations of the form \\(Ac=b\\). Is the system of equations underdetermined, over-determined, or square?\n**8.8**: _Interpolation of rational functions._ A _rational function_ of degree two has the form\n\\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}},\\]\nwhere \\(c_{1},c_{2},c_{3},d_{1},d_{2}\\) are coefficients. ('Rational' refers to the fact that \\(f\\) is a ratio of polynomials. Another name for \\(f\\) is _bi-quadratic_.) Consider the interpolation conditions\n\\[f(t_{i})=y_{i},\\quad i=1,\\ldots,K,\\]\n\nDocument 3:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\nDocument 4:\n----\n* _Portfolio return time series._ Suppose that \\(R\\) is a \\(T\\times n\\) asset return matrix, that gives the returns of \\(n\\) assets over \\(T\\) periods. A common trading strategy maintains constant investment weights given by the \\(n\\)-vector \\(w\\) over the \\(T\\) periods. For example, \\(w_{4}=0.15\\) means that \\(15\\%\\) of the total portfolio value is held in asset 4. (Short positions are denoted by negative entries in \\(w\\).) Then \\(Rw\\), which is a \\(T\\)-vector, is the time series of the portfolio returns over the periods \\(1,\\ldots,T\\). As an example, consider a portfolio of the 4 assets in table 6.1, with weights \\(w=(0.4,0.3,-0.2,0.5)\\). The product \\(Rw=(0.00213,-0.00201,0.00241)\\) gives the portfolio returns over the three periods in the example.\n* _Polynomial evaluation at multiple points._ Suppose the entries of the \\(n\\)-vector \\(c\\) are the coefficients of a polynomial \\(p\\) of degree \\(n-1\\) or less: \\[p(t)=c_{1}+c_{2}t+\\cdots+c_{n-1}t^{n-2}+c_{n}t^{n-1}.\\] Let \\(t_{1},\\ldots,t_{m}\\) be \\(m\\) numbers, and define the \\(m\\)-vector \\(y\\) as \\(y_{i}=p(t_{i})\\). Then we have \\(y=Ac\\), where \\(A\\) is the \\(m\\times n\\) matrix \\[A=\\left[\\begin{array}{ccccc}1&t_{1}&\\cdots&t_{1}^{n-2}&t_{1}^{n-1}\\\\ 1&t_{2}&\\cdots&t_{2}^{n-2}&t_{2}^{n-1}\\\\ \\vdots&\\vdots&&\\vdots&\\vdots\\\\ 1&t_{m}&\\cdots&t_{m}^{n-2}&t_{m}^{n-1}\\end{array}\\right].\\] (6.7)\nSo multiplying a vector \\(c\\) by the matrix \\(A\\) is the same as evaluating a polynomial with coefficients \\(c\\) at \\(m\\) points. The matrix \\(A\\) in (6.7) comes up often, and is called a _Vandermonde matrix_ (of degree \\(n-1\\), at the points \\(t_{1},\\ldots,t_{m}\\)), named for the mathematician Alexandre-Theophile Vandermonde.\n\nDocument 5:\n----\nThe constrained least squares problem can also be thought of as a limit of a bi-objective least squares problem, with primary objective \\(\\|Ax-b\\|^{2}\\) and secondary objective \\(\\|Cx-d\\|^{2}\\). Roughly speaking, we put infinite weight on the second objective, so that any nonzero value is unacceptable (which forces \\(x\\) to satisfy \\(Cx=d\\)). So we would expect (and it can be verified) that minimizing the weighted objective\n\\[\\|Ax-b\\|^{2}+\\lambda\\|Cx-d\\|^{2},\\]\nfor a very large value of \\(\\lambda\\) yields a vector close to a solution of the constrained least squares problem (16.1). We will encounter this idea again in chapter 19, when we consider the nonlinear constrained least squares problem.\nExample.In figure 16.1 we fit a _piecewise-polynomial_ function \\(\\hat{f}(x)\\) to a set of \\(N=140\\) points \\((x_{i},y_{i})\\) in the plane. The function \\(\\hat{f}(x)\\) is defined as\n\\[\\hat{f}(x)=\\left\\{\\begin{array}{ll}p(x)&x\\leq a\\\\ q(x)&x>a,\\end{array}\\right.\\]\nwith \\(a\\) given, and \\(p(x)\\) and \\(q(x)\\) polynomials of degree three or less,\n\\[p(x)=\\theta_{1}+\\theta_{2}x+\\theta_{3}x^{2}+\\theta_{4}x^{3},\\qquad q(x)=\\theta _{5}+\\theta_{6}x+\\theta_{7}x^{2}+\\theta_{8}x^{3}.\\]\nWe also impose the condition that \\(p(a)=q(a)\\) and \\(p^{\\prime}(a)=q^{\\prime}(a)\\), so that \\(\\hat{f}(x)\\) is continuous and has a continuous first derivative at \\(x=a\\). Suppose the \\(N\\) data\nFigure 16.1: Least squares fit of two cubic polynomials to 140 points, with continuity constraints \\(p(a)=q(a)\\) and \\(p^{\\prime}(a)=q^{\\prime}(a)\\).\npoints \\((x_{i},y_{i})\\) are numbered so that \\(x_{1},\\ldots,x_{M}\\leq a\\) and \\(x_{M+1},\\ldots,x_{N}>a\\). The sum of squares of the prediction errors is\n\n\n",
        "answer": 22.0,
        "rationale": {
            "Question": "The polynomial $x^3 - Ax + 15$ has three real roots. Two of these roots sum to 5. What is |A|?",
            "Answer": 22.0,
            "Answer_type": "float",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula2.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float"
    },
    {
        "id": "mingyin/linear-dependence2.json",
        "question": "suppose a,b,c,\\alpha,\\beta,\\gamma are six real numbers with a^2+b^2+c^2>0.  In addition, $a=b*cos(\\gamma)+c*cos(\\beta), b=c*cos(\\alpha)+a*cos(\\gamma), c=a*cos(\\beta)+b*cos(\\alpha)$. What is the value of $cos^2(\\alpha)+cos^2(\\beta)+cos^2(\\gamma)+2*cos(\\alpha)*cos(\\beta)*cos(\\gamma)? return the numeric.",
        "prediction": "",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nsuppose a,b,c,\\alpha,\\beta,\\gamma are six real numbers with a^2+b^2+c^2>0.  In addition, $a=b*cos(\\gamma)+c*cos(\\beta), b=c*cos(\\alpha)+a*cos(\\gamma), c=a*cos(\\beta)+b*cos(\\alpha)$. What is the value of $cos^2(\\alpha)+cos^2(\\beta)+cos^2(\\gamma)+2*cos(\\alpha)*cos(\\beta)*cos(\\gamma)? return the numeric.\n\nDocument 1:\n----\nProperties.The inner product satisfies some simple properties that are easily verified from the definition. If \\(a\\), \\(b\\), and \\(c\\) are vectors of the same size, and \\(\\gamma\\) is a scalar, we have the following.\n* _Commutativity_. \\(a^{T}b=b^{T}a\\). The order of the two vector arguments in the inner product does not matter.\n* _Associativity with scalar multiplication_. \\((\\gamma a)^{T}b=\\gamma(a^{T}b)\\), so we can write both as \\(\\gamma a^{T}b\\).\n* _Distributivity with vector addition_. \\((a+b)^{T}c=a^{T}c+b^{T}c\\). The inner product can be distributed across vector addition.\nThese can be combined to obtain other identities, such as \\(a^{T}(\\gamma b)=\\gamma(a^{T}b)\\), or \\(a^{T}(b+\\gamma c)=a^{T}b+\\gamma a^{T}c\\). As another useful example, we have, for any vectors \\(a,b,c,d\\) of the same size,\n\\[(a+b)^{T}(c+d)=a^{T}c+a^{T}d+b^{T}c+b^{T}d.\\]\nFigure 1.12: The affine combination \\((1-\\theta)a+\\theta b\\) for different values of \\(\\theta\\). These points are on the line passing through \\(a\\) and \\(b\\); for \\(\\theta\\) between \\(0\\) and \\(1\\), the points are on the line segment between \\(a\\) and \\(b\\).\nThis formula expresses an inner product on the left-hand side as a sum of four inner products on the right-hand side, and is analogous to expanding a product of sums in algebra. Note that on the left-hand side, the two addition symbols refer to vector addition, whereas on the right-hand side, the three addition symbols refer to scalar (number) addition.\n\nDocument 2:\n----\nSuppose you have observed the values \\(z_{1},\\ldots,z_{T}\\), and wish to choose the sinusoid amplitudes \\(a_{1},\\ldots,a_{K}\\) and phases \\(\\phi_{1},\\ldots,\\phi_{K}\\) so as to minimize the RMS value of the approximation error \\((\\hat{z}_{1}-z_{1},\\ldots,\\hat{z}_{T}-z_{T})\\). (We assume that the frequencies are given.) Explain how to solve this using least squares model fitting.\n_Hint._ A sinusoid with amplitude \\(a\\), frequency \\(\\omega\\), and phase \\(\\phi\\) can be described by its cosine and sine coefficients \\(\\alpha\\) and \\(\\beta\\), where\n\\[a\\cos(\\omega t-\\phi)=\\alpha\\cos(\\omega t)+\\beta\\sin(\\omega t),\\]\nwhere (using the cosine of sum formula) \\(\\alpha=a\\cos\\phi\\), \\(\\beta=a\\sin\\phi\\). We can recover the amplitude and phase from the cosine and sine coefficients as\n\\[a=\\sqrt{\\alpha^{2}+\\beta^{2}},\\qquad\\phi=\\arctan(\\beta/\\alpha).\\]\nExpress the problem in terms of the cosine and sine coefficients.\n* 13.20_Fitting with continuous and discontinuous piecewise-linear functions._ Consider a fitting problem with \\(n=1\\), so \\(x^{(1)},\\ldots,x^{(N)}\\) and \\(y^{(1)},\\ldots,y^{(N)}\\) are numbers. We consider two types of closely related models. The first is a piecewise-linear model with knot points at \\(-1\\) and \\(1\\), as described on page 256, and illustrated in figure 13.8. The second is a stratified model (see page 272), with three independent affine models, one for \\(x<-1\\), one for \\(-1\\leq x\\leq 1\\), and one for \\(x>1\\). (In other words, we stratify on \\(x\\) taking low, middle, or high values.) Are these two models the same? Is one more general than the other? How many parameters does each model have? _Hint._ See problem title. What can you say about the training set RMS error and test set RMS error that would be achieved using least squares with these two models?\n\nDocument 3:\n----\n### 10.3 Matrix power\nIt makes sense to multiply a square matrix \\(A\\) by itself to form \\(AA\\). We refer to this matrix as \\(A^{2}\\). Similarly, if \\(k\\) is a positive integer, then \\(k\\) copies of \\(A\\) multiplied together is denoted \\(A^{k}\\). If \\(k\\) and \\(l\\) are positive integers, and \\(A\\) is square, then \\(A^{k}A^{l}=A^{k+l}\\) and \\((A^{k})^{l}=A^{kl}\\). By convention we take \\(A^{0}=I\\), which makes the formulas above hold for all nonnegative integer values of \\(k\\) and \\(l\\).\nWe should mention one ambiguity in matrix power notation that occasionally arises. When \\(A\\) is a square matrix and \\(T\\) is a nonnegative integer, \\(A^{T}\\) can mean either the transpose of the matrix \\(A\\) or its \\(T\\)th power. Usually which is meant is clear from the context, or the author explicitly states which meaning is intended. To avoid this ambiguity, some authors use a different symbol for the transpose, such as \\(A^{\\mathrm{T}}\\) (with the superscript in roman font) or \\(A^{\\prime}\\), or avoid referring to the \\(T\\)th power of a matrix. When \\(A\\) is not square there is no ambiguity, since \\(A^{T}\\) can only be the transpose in this case.\nOther matrix powers.Matrix powers \\(A^{k}\\) with \\(k\\) a negative integer will be discussed in SS11.2. Non-integer powers, such as \\(A^{1/2}\\) (the matrix squareroot), need not make sense, or can be ambiguous, unless certain conditions on \\(A\\) hold. This is an advanced topic in linear algebra that we will not pursue in this book.\n\nDocument 4:\n----\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\\[l_{i}=\\left[\\begin{array}{c}0_{i-1}\\\\ 1\\\\ -(1+r)\\\\ 0_{n-i-1}\\end{array}\\right],\\quad i=1,\\ldots,n-1,\\]\nwhere \\(r\\geq 0\\) is the per-period interest rate. The cash flow \\(l_{i}\\) represents a loan of $1 in period \\(i\\), which is paid back in period \\(i+1\\) with interest \\(r\\). (The subscripts on the zero vectors above give their dimensions.) Scaling \\(l_{i}\\) changes the loan amount; scaling \\(l_{i}\\) by a negative coefficient converts it into a loan _to_ another entity (which is paid back in period \\(i+1\\) with interest).\n\nDocument 5:\n----\nthen (evidently) \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); conversely, if \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\), the identity (5.4) holds for \\(y\\).\nOrthonormal basis.If the \\(n\\)-vectors \\(a_{1},\\ldots,a_{n}\\) are orthonormal, they are linearly independent, and therefore also a basis. In this case they are called an _orthonormal basis_. The three examples above (on page 5.2) are orthonormal bases.\nIf \\(a_{1},\\ldots,a_{n}\\) is an orthonormal basis, then we have, for any \\(n\\)-vector \\(x\\), the identity\n\\[x=(a_{1}^{T}x)a_{1}+\\cdots+(a_{n}^{T}x)a_{n}.\\] (5.5)\nTo see this, we note that since \\(a_{1},\\ldots,a_{n}\\) are a basis, \\(x\\) can be expressed as a linear combination of them; hence the identity (5.4) above holds. The equation above is sometimes called the _orthonormal expansion formula_; the right-hand side is called the _expansion of \\(x\\) in the basis \\(a_{1},\\ldots,a_{n}\\)_. It shows that any \\(n\\)-vector can be expressed as a linear combination of the basis elements, with the coefficients given by taking the inner product of \\(x\\) with the elements of the basis.\nAs an example, we express the 3-vector \\(x=(1,2,3)\\) as a linear combination of the orthonormal basis given in (5.3). The inner products of \\(x\\) with these vectors\nare\n\\[\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right]^{T}x=-3,\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ 1\\\\ 0\\end{array}\\right]^{T}x=\\frac{3}{\\sqrt{2}},\\qquad\\frac{1}{\\sqrt{2}}\\left[ \\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right]^{T}x=\\frac{-1}{\\sqrt{2}}.\\]\nIt can be verified that the expansion of \\(x\\) in this basis is\n\n\n",
        "answer": 1.0,
        "rationale": {
            "Question": "suppose a,b,c,\\alpha,\\beta,\\gamma are six real numbers with a^2+b^2+c^2>0.  In addition, $a=b*cos(\\gamma)+c*cos(\\beta), b=c*cos(\\alpha)+a*cos(\\gamma), c=a*cos(\\beta)+b*cos(\\alpha)$. What is the value of $cos^2(\\alpha)+cos^2(\\beta)+cos^2(\\gamma)+2*cos(\\alpha)*cos(\\beta)*cos(\\gamma)? return the numeric.",
            "Answer": 1.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 3.5 example 7",
            "id": "mingyin/linear-dependence2.json",
            "explanation": "NONE",
            "theorem": "linear dependence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float"
    },
    {
        "id": "wenhuchen/determinant2.json",
        "question": "For matrix A = [[2, 4, 3], [3, 3, 1], [42, 20, 51]], what is its determinant?",
        "prediction": "\\(-120\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[2, 4, 3], [3, 3, 1], [42, 20, 51]], what is its determinant?\n\nDocument 1:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\nDocument 2:\n----\n* _Entries of matrix triple product_. (See page 10.) Suppose \\(A\\) has dimensions \\(m\\times n\\), \\(B\\) has dimensions \\(n\\times p\\), \\(C\\) has dimensions \\(p\\times q\\), and let \\(D=ABC\\). Show that \\[D_{ij}=\\sum_{k=1}^{n}\\sum_{l=1}^{p}A_{ik}B_{kl}C_{lj}.\\] This is the formula analogous to (10.1) for the product of two matrices.\n* _Multiplication by a diagonal matrix_. Suppose that \\(A\\) is an \\(m\\times n\\) matrix, \\(D\\) is a diagonal matrix, and \\(B=DA\\). Describe \\(B\\) in terms of \\(A\\) and the entries of \\(D\\). You can refer to the rows or columns or entries of \\(A\\).\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n\nDocument 3:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\nDocument 4:\n----\nDiagonal matrices.A square \\(n\\times n\\) matrix \\(A\\) is _diagonal_ if \\(A_{ij}=0\\) for \\(i\\neq j\\). (The entries of a matrix with \\(i=j\\) are called the _diagonal entries_; those with \\(i\\neq j\\) are its _off-diagonal_ entries.) A diagonal matrix is one for which all off-diagonal entries are zero. Examples of diagonal matrices we have already seen are square zero matrices and identity matrices. Other examples are\n\\[\\left[\\begin{array}{rrr}-3&0\\\\ 0&0\\end{array}\\right],\\qquad\\left[\\begin{array}{rrr}0.2&0&0\\\\ 0&-3&0\\\\ 0&0&1.2\\end{array}\\right].\\]\n(Note that in the first example, one of the diagonal elements is also zero.)\nThe notation \\(\\mathbf{diag}(a_{1},\\ldots,a_{n})\\) is used to compactly describe the \\(n\\times n\\) diagonal matrix \\(A\\) with diagonal entries \\(A_{11}=a_{1}\\), ..., \\(A_{nn}=a_{n}\\). This notation is not yet standard, but is coming into more prevalent use. As examples, the matrices above would be expressed as\n\\[\\mathbf{diag}(-3,0),\\qquad\\mathbf{diag}(0.2,-3,1.2),\\]\nrespectively. We also allow \\(\\mathbf{diag}\\) to take one \\(n\\)-vector argument, as in \\(I=\\mathbf{diag}(\\mathbf{1})\\).\nTriangular matrices.A square \\(n\\times n\\) matrix \\(A\\) is _upper triangular_ if \\(A_{ij}=0\\) for \\(i>j\\), and it is _lower triangular_ if \\(A_{ij}=0\\) for \\(i<j\\). (So a diagonal matrix is one that is both lower and upper triangular.) If a matrix is either lower or upper triangular, it is called _triangular_. For example, the matrices\n\\[\\left[\\begin{array}{rrr}1&-1&0.7\\\\ 0&1.2&-1.1\\\\ 0&0&3.2\\end{array}\\right],\\qquad\\left[\\begin{array}{rrr}-0.6&0\\\\ -0.3&3.5\\end{array}\\right],\\]\nare upper and lower triangular, respectively.\nA triangular \\(n\\times n\\) matrix \\(A\\) has up to \\(n(n+1)/2\\) nonzero entries, _i.e._, around half its entries are zero. Triangular matrices are generally not considered sparse matrices, since their density is around 50%, but their special sparsity pattern will be important in the sequel.\n\nDocument 5:\n----\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n1. Find all diagonal squareroots of \\(I_{n}\\). How many are there? (For \\(n=1\\), you should get \\(2\\).)\n* Find a nondiagonal \\(2\\times 2\\) matrix \\(A\\) that satisfies \\(A^{2}=I\\). This means that in general there are even more squareroots of \\(I_{n}\\) than you found in part (a).\n* _Circular shift matrices._ Let \\(A\\) be the \\(5\\times 5\\) matrix \\[A=\\left[\\begin{array}{cccc}0&0&0&0&1\\\\ 1&0&0&0&0\\\\ 0&1&0&0&0\\\\ 0&0&1&0&0\\\\ 0&0&0&1&0\\end{array}\\right].\\] 1. How is \\(Ax\\) related to \\(x\\)? Your answer should be in English. _Hint._ See exercise title. 2. What is \\(A^{5}\\)? _Hint._ The answer should make sense, given your answer to part (a).\n* _Dynamics of an economy._ Let \\(x_{1},x_{2},\\ldots\\) be \\(n\\)-vectors that give the level of economic activity of a country in years \\(1,2,\\ldots\\), in \\(n\\) different sectors (like energy, defense, manufacturing). Specifically, \\((x_{t})_{i}\\) is the level of economic activity in economic sector \\(i\\) (say, in billions of dollars) in year \\(t\\). A common model that connects these economic activity vectors is \\(x_{t+1}=Bx_{t}\\), where \\(B\\) is an \\(n\\times n\\) matrix. (See exercise 9.2.) Five a matrix expression for the total economic activity across all sectors in year \\(t=6\\), in terms of the matrix \\(B\\) and the vector of initial activity levels \\(x_{1}\\). Suppose you can increase economic activity in year \\(t=1\\) by some fixed amount (say, one billion dollars) in _one_ sector, by government spending. How should you choose which sector to stimulate so as to maximize the total economic output in year \\(t=6\\)?\n\n\n",
        "answer": -376,
        "rationale": {
            "Question": "For matrix A = [[2, 4, 3], [3, 3, 1], [42, 20, 51]], what is its determinant?",
            "Answer": -376,
            "Picture": null,
            "Answer_type": "integer",
            "source": "self",
            "id": "wenhuchen/determinant2.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "wenhuchen/vieta's_formula3.json",
        "question": "Let $x_1$ and $x_2$ be the roots of the equation $x^2 + 3x + 1 =0$. Compute $(x_1/(x_2 + 1))^2 + (x_2 / (x_1 + 1))^2$.",
        "prediction": "$\\boxed{6}$",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet $x_1$ and $x_2$ be the roots of the equation $x^2 + 3x + 1 =0$. Compute $(x_1/(x_2 + 1))^2 + (x_2 / (x_1 + 1))^2$.\n\nDocument 1:\n----\n* A list consisting of a single vector is linearly dependent only if the vector is zero. It is linearly independent only if the vector is nonzero.\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\n\nDocument 2:\n----\nTriangular matrix.A triangular matrix with nonzero diagonal elements is invertible. We first discuss this for a lower triangular matrix. Let \\(L\\) be \\(n\\times n\\) and lower triangular with nonzero diagonal elements. We show that the columns are linearly independent, _i.e._, \\(Lx=0\\) is only possible if \\(x=0\\). Expanding the matrix-vector product, we can write \\(Lx=0\\) as\n\\[L_{11}x_{1} = 0\\] \\[L_{21}x_{1}+L_{22}x_{2} = 0\\] \\[L_{31}x_{1}+L_{32}x_{2}+L_{33}x_{3} = 0\\] \\[\\vdots\\] \\[L_{n1}x_{1}+L_{n2}x_{2}+\\cdots+L_{n,n-1}x_{n-1}+L_{nn}x_{n} = 0.\\]\nSince \\(L_{11}\\neq 0\\), the first equation implies \\(x_{1}=0\\). Using \\(x_{1}=0\\), the second equation reduces to \\(L_{22}x_{2}=0\\). Since \\(L_{22}\\neq 0\\), we conclude that \\(x_{2}=0\\). Using \\(x_{1}=x_{2}=0\\), the third equation now reduces to \\(L_{33}x_{3}=0\\), and since \\(L_{33}\\) is assumed to be nonzero, we have \\(x_{3}=0\\). Continuing this argument, we find that all entries of \\(x\\) are zero, and this shows that the columns of \\(L\\) are linearly independent. It follows that \\(L\\) is invertible.\nA similar argument can be followed to show that an upper triangular matrix with nonzero diagonal elements is invertible. One can also simply note that if \\(R\\) is upper triangular, then \\(L=R^{T}\\) is lower triangular with the same diagonal, and use the formula \\((L^{T})^{-1}=(L^{-1})^{T}\\) for the inverse of the transpose.\nInverse via QR factorization.The QR factorization gives a simple expression for the inverse of an invertible matrix. If \\(A\\) is square and invertible, its columns are linearly independent, so it has a QR factorization \\(A=QR\\). The matrix \\(Q\\) is orthogonal and \\(R\\) is upper triangular with positive diagonal entries. Hence \\(Q\\) and \\(R\\) are invertible, and the formula for the inverse product gives\n\nDocument 3:\n----\n**2.7**: _General formula for affine functions._ Verify that formula (2.4) holds for any affine function \\(f:{\\bf R}^{n}\\to{\\bf R}\\). You can use the fact that \\(f(x)=a^{T}x+b\\) for some \\(n\\)-vector \\(a\\) and scalar \\(b\\).\n**2.8**: _Integral and derivative of polynomial._ Suppose the \\(n\\)-vector \\(c\\) gives the coefficients of a polynomial \\(p(x)=c_{1}+c_{2}x+\\cdots+c_{n}x^{n-1}\\).\n1. Let \\(\\alpha\\) and \\(\\beta\\) be numbers with \\(\\alpha<\\beta\\). Find an \\(n\\)-vector \\(a\\) for which \\[a^{T}c=\\int_{\\alpha}^{\\beta}p(x)\\;dx\\] always holds. This means that the integral of a polynomial over an interval is a linear function of its coefficients.\n2. Let \\(\\alpha\\) be a number. Find an \\(n\\)-vector \\(b\\) for which \\[b^{T}c=p^{\\prime}(\\alpha).\\] This means that the derivative of the polynomial at a given point is a linear function of its coefficients.\n**2.9**: _Taylor approximation._ Consider the function \\(f:{\\bf R}^{2}\\to{\\bf R}\\) given by \\(f(x_{1},x_{2})=x_{1}x_{2}\\). Find the Taylor approximation \\(\\hat{f}\\) at the point \\(z=(1,1)\\). Compare \\(f(x)\\) and \\(\\hat{f}(x)\\) for the following values of \\(x\\): \\[x=(1,1),\\quad x=(1.05,0.95),\\quad x=(0.85,1.25),\\quad x=(-1,2).\\] Make a brief comment about the accuracy of the Taylor approximation in each case.\n**2.10**: _Regression model._ Consider the regression model \\(\\hat{y}=x^{T}\\beta+v\\), where \\(\\hat{y}\\) is the predicted response, \\(x\\) is an 8-vector of features, \\(\\beta\\) is an 8-vector of coefficients, and \\(v\\) is the offset term. Determine whether each of the following statements is true or false.\n\nDocument 4:\n----\nThe first condition requires that \\(C\\) is wide (or square), _i.e._, that there are fewer constraints than variables. The second condition depends on both \\(A\\) and \\(C\\), and it can be satisfied even when the columns of \\(A\\) are linearly dependent. The condition (16.5) is the generalization of our assumption (12.2) for unconstrained least squares (_i.e._, that \\(A\\) has linearly independent columns).\nBefore proceeding, let us verify that the KKT matrix is invertible if and only if (16.5) holds. First suppose that the KKT matrix is not invertible. This means that there is a nonzero vector \\((\\bar{x},\\bar{z})\\) with\n\\[\\left[\\begin{array}{cc}2A^{T}A&C^{T}\\\\ C&0\\end{array}\\right]\\left[\\begin{array}{c}\\bar{x}\\\\ \\bar{z}\\end{array}\\right]=0.\\]\nMultiply the top block equation \\(2A^{T}A\\bar{x}+C^{T}\\bar{z}=0\\) on the left by \\(\\bar{x}^{T}\\) to get\n\\[2\\|A\\bar{x}\\|^{2}+\\bar{x}^{T}C^{T}\\bar{z}=0.\\]\nThe second block equation, \\(C\\bar{x}=0\\), implies (by taking the transpose) \\(\\bar{x}^{T}C^{T}=0\\), so the equation above becomes \\(2\\|A\\bar{x}\\|^{2}=0\\), _i.e._, \\(A\\bar{x}=0\\). We also have \\(C\\bar{x}=0\\), so\n\\[\\left[\\begin{array}{c}A\\\\ C\\end{array}\\right]\\bar{x}=0.\\]\nSince the matrix on the left has linearly independent columns (by assumption), we conclude that \\(\\bar{x}=0\\). The first block equation above then becomes \\(C^{T}\\bar{z}=0\\). But by our assumption that the columns of \\(C^{T}\\) are linearly independent, we have \\(\\bar{z}=0\\). So \\((\\bar{x},\\bar{z})=0\\), which is a contradiction.\nThe converse is also true. First suppose that the rows of \\(C\\) are linearly dependent. Then there is a nonzero vector \\(\\bar{z}\\) with \\(C^{T}\\bar{z}=0\\). Then\n\\[\\left[\\begin{array}{cc}2A^{T}A&C^{T}\\\\ C&0\\end{array}\\right]\\left[\\begin{array}{c}0\\\\ \\bar{z}\\end{array}\\right]=0,\\]\n\nDocument 5:\n----\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\n\n",
        "answer": 18.0,
        "rationale": {
            "Question": "Let $x_1$ and $x_2$ be the roots of the equation $x^2 + 3x + 1 =0$. Compute $(x_1/(x_2 + 1))^2 + (x_2 / (x_1 + 1))^2$.",
            "Answer": 18.0,
            "Answer_type": "float",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula3.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float"
    },
    {
        "id": "elainewan/math_algebra_3_3.json",
        "question": "What is the vector that spans the kernel of A = [[1, 0, 2, 4], [0, 1, -3, -1], [3, 4, -6, 8], [0, -1, 3, 4]]?",
        "prediction": "the vector \\([2, 3, 1, 0]\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the vector that spans the kernel of A = [[1, 0, 2, 4], [0, 1, -3, -1], [3, 4, -6, 8], [0, -1, 3, 4]]?\n\nDocument 1:\n----\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\nNow suppose that \\(x\\) is any \\(n\\)-vector. It can be expressed as a linear combination of the basis vectors \\(a_{1},\\ldots,a_{n}\\):\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{n}a_{n}.\\]\nThe dual basis gives us a simple way to find the coefficients \\(\\beta_{1},\\ldots,\\beta_{n}\\).\nWe start with \\(AB=I\\), and multiply by \\(x\\) to get\n\\[x=ABx=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right]\\left[ \\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right]x=(b_{1}^{T}x)a_{1}+\\cdots+(b_{n}^{T}x)a_{n}.\\]\nThis means (since the vectors \\(a_{1},\\ldots,a_{n}\\) are linearly independent) that \\(\\beta_{i}=b_{i}^{T}x\\). In words: The coefficients in the expansion of a vector in a basis are given by the inner products with the dual basis vectors. Using matrix notation, we can say that \\(\\beta=B^{T}x=(A^{-1})^{T}x\\) is the vector of coefficients of \\(x\\) in the basis given by the columns of \\(A\\).\n\nDocument 2:\n----\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\\[l_{i}=\\left[\\begin{array}{c}0_{i-1}\\\\ 1\\\\ -(1+r)\\\\ 0_{n-i-1}\\end{array}\\right],\\quad i=1,\\ldots,n-1,\\]\nwhere \\(r\\geq 0\\) is the per-period interest rate. The cash flow \\(l_{i}\\) represents a loan of $1 in period \\(i\\), which is paid back in period \\(i+1\\) with interest \\(r\\). (The subscripts on the zero vectors above give their dimensions.) Scaling \\(l_{i}\\) changes the loan amount; scaling \\(l_{i}\\) by a negative coefficient converts it into a loan _to_ another entity (which is paid back in period \\(i+1\\) with interest).\n\nDocument 3:\n----\nThe identity matrix, and the _reverser matrix_\n\\[A=\\left[\\begin{array}{c}e_{n}^{T}\\\\ \\vdots\\\\ e_{1}^{T}\\end{array}\\right]=\\left[\\begin{array}{ccccc}0&0&\\cdots&0&1\\\\ 0&0&\\cdots&1&0\\\\ \\vdots&\\vdots&\\iddots&\\vdots&\\vdots\\\\ 0&1&\\cdots&0&0\\\\ 1&0&\\cdots&0&0\\end{array}\\right]\\]\nare special cases of selector matrices. (The reverser matrix reverses the order of the entries of a vector: \\(Ax=(x_{n},x_{n-1},\\ldots,x_{2},x_{1})\\).) Another one is the _\\(r\\!:\\!s\\) slicing matrix_, which can be described as the block matrix\n\\[A=\\left[\\begin{array}{cc}0_{m\\times(r-1)}&I_{m\\times m}&0_{m\\times(n-s)}\\end{array} \\right],\\]\nwhere \\(m=s-r+1\\). (We show the dimensions of the blocks for clarity.) We have \\(Ax=x_{r\\!:\\!s}\\), _i.e._, multiplying by \\(A\\) gives the \\(r\\!:\\!s\\) slice of a vector.\nDown-sampling.Another example is the \\((n/2)\\times n\\) matrix (with \\(n\\) even)\n\\[A=\\left[\\begin{array}{cccccccccc}1&0&0&0&0&0&\\cdots&0&0&0&0\\\\ 0&0&1&0&0&0&\\cdots&0&0&0&0\\\\ 0&0&0&0&1&0&\\cdots&0&0&0&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&&\\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&0&0&0&0&\\cdots&1&0&0&0\\\\ 0&0&0&0&0&0&\\cdots&0&0&1&0\\end{array}\\right].\\]\nIf \\(y=Ax\\), we have \\(y=(x_{1},x_{3},x_{5},\\ldots,x_{n-3},x_{n-1})\\). When \\(x\\) is a time series, \\(y\\) is called the \\(2\\times\\)_down-sampled_ version of \\(x\\). If \\(x\\) is a quantity sampled every hour, then \\(y\\) is the same quantity, sampled every \\(2\\) hours.\nImage cropping.As a more interesting example, suppose that \\(x\\) is an image with \\(M\\times N\\) pixels, with \\(M\\) and \\(N\\) even. (That is, \\(x\\) is an \\(MN\\)-vector, with its entries giving the pixel values in some specific order.) Let \\(y\\) be the \\((M/2)\\times(N/2)\\) image that is the upper left corner of the image \\(x\\), _i.e._, a cropped version. Then we have \\(y=Ax\\), where \\(A\\) is an \\((MN/4)\\times(MN)\\) selector matrix. The \\(i\\)th row of \\(A\\) is \\(e_{k_{i}}^{T}\\), where \\(k_{i}\\) is the index of the pixel in \\(x\\) that corresponds to the \\(i\\)th pixel in \\(y\\).\n\nDocument 4:\n----\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nTaking the inner product of the left-hand and right-hand sides of this equation with \\(a_{i}\\) yields\n\\[a_{i}^{T}x=a_{i}^{T}(\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k})=\\beta_{i},\\]\nusing the same argument as above. So if a vector \\(x\\) is a linear combination of orthonormal vectors, we can easily find the coefficients of the linear combination by taking the inner products with the vectors.\nFor any \\(x\\) that is a linear combination of orthonormal vectors \\(a_{1},\\ldots,a_{k}\\), we have the identity\n\\[x=(a_{1}^{T}x)a_{1}+\\cdots+(a_{k}^{T}x)a_{k}.\\] (5.4)\nThis identity gives us a simple way to check if an \\(n\\)-vector \\(y\\) is a linear combination of the orthonormal vectors \\(a_{1},\\ldots,a_{k}\\). If the identity (5.4) holds for \\(y\\), _i.e._,\n\\[y=(a_{1}^{T}y)a_{1}+\\cdots+(a_{k}^{T}y)a_{k},\\]\nthen (evidently) \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); conversely, if \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\), the identity (5.4) holds for \\(y\\).\nOrthonormal basis.If the \\(n\\)-vectors \\(a_{1},\\ldots,a_{n}\\) are orthonormal, they are linearly independent, and therefore also a basis. In this case they are called an _orthonormal basis_. The three examples above (on page 5.2) are orthonormal bases.\nIf \\(a_{1},\\ldots,a_{n}\\) is an orthonormal basis, then we have, for any \\(n\\)-vector \\(x\\), the identity\n\\[x=(a_{1}^{T}x)a_{1}+\\cdots+(a_{n}^{T}x)a_{n}.\\] (5.5)\nTo see this, we note that since \\(a_{1},\\ldots,a_{n}\\) are a basis, \\(x\\) can be expressed as a linear combination of them; hence the identity (5.4) above holds. The equation above is sometimes called the _orthonormal expansion formula_; the right-hand side is called the _expansion of \\(x\\) in the basis \\(a_{1},\\ldots,a_{n}\\)_. It shows that any \\(n\\)-vector can be expressed as a linear combination of the basis elements, with the coefficients given by taking the inner product of \\(x\\) with the elements of the basis.\n\nDocument 5:\n----\nas the _expansion of \\(b\\) in the \\(a_{1},\\ldots,a_{n}\\) basis_. The numbers \\(\\alpha_{1},\\ldots,\\alpha_{n}\\) are called the _coefficients_ of the expansion of \\(b\\) in the basis \\(a_{1},\\ldots,a_{n}\\). (We will see later how to find the coefficients in the expansion of a vector in a basis.)\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\n\n",
        "answer": [
            -2,
            3,
            1,
            0
        ],
        "rationale": {
            "Question": "What is the vector that spans the kernel of A = [[1, 0, 2, 4], [0, 1, -3, -1], [3, 4, -6, 8], [0, -1, 3, 4]]?",
            "Answer": [
                -2,
                3,
                1,
                0
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_3.json",
            "explanation": "NONE",
            "theorem": "kernel of linear transformations",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "mingyin/gaussian-elimination2.json",
        "question": "V is a vector space over the real field R. It is known that the vector group u_1, u_2, u_3 in V are linearly independent. Finding the rank of vector group ${u_1-\\lambda u_2, u_2-\\lambda u_3, u_3-\\lambda u_1}$ for $\\lambda=\\sqrt{5}$ and $\\lambda=1$ separately. Return the answer as a list.",
        "prediction": "[3, 3, 3]",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nV is a vector space over the real field R. It is known that the vector group u_1, u_2, u_3 in V are linearly independent. Finding the rank of vector group ${u_1-\\lambda u_2, u_2-\\lambda u_3, u_3-\\lambda u_1}$ for $\\lambda=\\sqrt{5}$ and $\\lambda=1$ separately. Return the answer as a list.\n\nDocument 1:\n----\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\nDocument 2:\n----\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\nNow suppose that \\(x\\) is any \\(n\\)-vector. It can be expressed as a linear combination of the basis vectors \\(a_{1},\\ldots,a_{n}\\):\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{n}a_{n}.\\]\nThe dual basis gives us a simple way to find the coefficients \\(\\beta_{1},\\ldots,\\beta_{n}\\).\nWe start with \\(AB=I\\), and multiply by \\(x\\) to get\n\\[x=ABx=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right]\\left[ \\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right]x=(b_{1}^{T}x)a_{1}+\\cdots+(b_{n}^{T}x)a_{n}.\\]\nThis means (since the vectors \\(a_{1},\\ldots,a_{n}\\) are linearly independent) that \\(\\beta_{i}=b_{i}^{T}x\\). In words: The coefficients in the expansion of a vector in a basis are given by the inner products with the dual basis vectors. Using matrix notation, we can say that \\(\\beta=B^{T}x=(A^{-1})^{T}x\\) is the vector of coefficients of \\(x\\) in the basis given by the columns of \\(A\\).\n\nDocument 3:\n----\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\\[x=\\gamma_{1}a_{1}+\\cdots+\\gamma_{k}a_{k},\\]\nthen \\(\\beta_{i}=\\gamma_{i}\\) for \\(i=1,\\ldots,k\\). This tells us that, in principle at least, we can find the coefficients that form a vector \\(x\\) as a linear combination of linearly independent vectors.\nTo see this, we subtract the two equations above to get\n\\[0=(\\beta_{1}-\\gamma_{1})a_{1}+\\cdots+(\\beta_{k}-\\gamma_{k})a_{k}.\\]\nSince \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we conclude that \\(\\beta_{i}-\\gamma_{i}\\) are all zero.\nThe converse is also true: If each linear combination of a list of vectors can only be expressed as a linear combination with one set of coefficients, then the list of vectors is linearly independent. This gives a nice interpretation of linear independence: A list of vectors is linearly independent if and only if for any linear combination of them, we can infer or deduce the associated coefficients. (We will see later how to do this.)\nSupersets and subsets.If a collection of vectors is linearly dependent, then any superset of it is linearly dependent. In other words: If we add vectors to a linearly dependent collection of vectors, the new collection is also linearly dependent. Any nonempty subset of a linearly independent collection of vectors is linearly independent. In other words: Removing vectors from a collection of vectors preserves linear independence.\n\nDocument 4:\n----\nwhere \\(\\lambda_{i}>0\\) are weights, and \\(J_{2},\\ldots,J_{p}\\) are the regularization terms. This is called _regularized inversion_ or _regularized estimation_. We may repeat this for several choices of the weights, and choose the best estimate for the particular application.\nTikhonov regularized inversion.Choosing \\(\\hat{x}\\) to minimize\n\\[\\|Ax-y\\|^{2}+\\lambda\\|x\\|^{2}\\]\nfor some choice of \\(\\lambda>0\\) is called _Tikhonov regularized inversion_, after the mathematician Andrey Tikhonov. Here we seek a guess \\(\\hat{x}\\) that is consistent with the measurements (_i.e._, \\(\\|A\\hat{x}-y\\|^{2}\\) is small), but not too big.\nThe stacked matrix in this case,\n\\[\\tilde{A}=\\left[\\begin{array}{c}A\\\\ \\sqrt{\\lambda}I\\end{array}\\right],\\]\nalways has linearly independent columns, without any assumption about \\(A\\), which can have any dimensions, and need not have linearly independent columns. To see this we note that \\(\\tilde{A}x=(Ax,\\sqrt{\\lambda}x)=0\\) implies that \\(\\sqrt{\\lambda}x=0\\), which implies \\(x=0\\). The Gram matrix associated with \\(\\tilde{A}\\),\n\\[\\tilde{A}^{T}\\tilde{A}=A^{T}A+\\lambda I,\\]\nis therefore always invertible (provided \\(\\lambda>0\\)). The Tikhonov regularized approximate solution is then\n\\[\\hat{x}=(A^{T}A+\\lambda I)^{-1}A^{T}b.\\]\nEqualization.The vector \\(x\\) represents a transmitted signal or message, consisting of \\(n\\) real values. The matrix \\(A\\) represents the mapping from the transmitted signal to what is received (called the _channel_); \\(y=Ax+v\\) includes noise as well as the action of the channel. Guessing what \\(x\\) is, given \\(y\\), can be thought of as un-doing the effects of the channel. In this context, estimation is called _equalization_.\n#### Estimating a periodic time series\n\nDocument 5:\n----\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\nInvertibility conditions.For square matrices, left-invertibility, right-invertibility, and invertibility are equivalent: If a matrix is square and left-invertible, then it is also right-invertible (and therefore invertible) and vice-versa.\nTo see this, suppose \\(A\\) is an \\(n\\times n\\) matrix and left-invertible. This implies that the \\(n\\) columns of \\(A\\) are linearly independent. Therefore they form a basis and so any \\(n\\)-vector can be expressed as a linear combination of the columns of \\(A\\). In particular, each of the \\(n\\) unit vectors \\(e_{i}\\) can be expressed as \\(e_{i}=Ab_{i}\\) for some \\(n\\)-vector \\(b_{i}\\). The matrix \\(B=\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\) satisfies\n\\[AB=\\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right]= \\left[\\begin{array}{cccc}e_{1}&e_{2}&\\cdots&e_{n}\\end{array}\\right]=I.\\]\nSo \\(B\\) is a right inverse of \\(A\\).\nWe have just shown that for a square matrix \\(A\\),\n\n\n",
        "answer": [
            3,
            2
        ],
        "rationale": {
            "Question": "V is a vector space over the real field R. It is known that the vector group u_1, u_2, u_3 in V are linearly independent. Finding the rank of vector group ${u_1-\\lambda u_2, u_2-\\lambda u_3, u_3-\\lambda u_1}$ for $\\lambda=\\sqrt{5}$ and $\\lambda=1$ separately. Return the answer as a list.",
            "Answer": [
                3,
                2
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "linear algebra 2.6 example 1(2)",
            "id": "mingyin/gaussian-elimination2.json",
            "explanation": "NONE",
            "theorem": "gaussian elimination",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "elainewan/math_algebra_7_2.json",
        "question": "Is there an eigenbasis for the identity matrix I_n?",
        "prediction": "True",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs there an eigenbasis for the identity matrix I_n?\n\nDocument 1:\n----\nThis identity gives us a simple way to check if an \\(n\\)-vector \\(y\\) is a linear combination of the orthonormal vectors \\(a_{1},\\ldots,a_{k}\\). If the identity (5.4) holds for \\(y\\), _i.e._,\n\\[y=(a_{1}^{T}y)a_{1}+\\cdots+(a_{k}^{T}y)a_{k},\\]\nthen (evidently) \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); conversely, if \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\), the identity (5.4) holds for \\(y\\).\nOrthonormal basis.If the \\(n\\)-vectors \\(a_{1},\\ldots,a_{n}\\) are orthonormal, they are linearly independent, and therefore also a basis. In this case they are called an _orthonormal basis_. The three examples above (on page 5.2) are orthonormal bases.\nIf \\(a_{1},\\ldots,a_{n}\\) is an orthonormal basis, then we have, for any \\(n\\)-vector \\(x\\), the identity\n\\[x=(a_{1}^{T}x)a_{1}+\\cdots+(a_{n}^{T}x)a_{n}.\\] (5.5)\nTo see this, we note that since \\(a_{1},\\ldots,a_{n}\\) are a basis, \\(x\\) can be expressed as a linear combination of them; hence the identity (5.4) above holds. The equation above is sometimes called the _orthonormal expansion formula_; the right-hand side is called the _expansion of \\(x\\) in the basis \\(a_{1},\\ldots,a_{n}\\)_. It shows that any \\(n\\)-vector can be expressed as a linear combination of the basis elements, with the coefficients given by taking the inner product of \\(x\\) with the elements of the basis.\nAs an example, we express the 3-vector \\(x=(1,2,3)\\) as a linear combination of the orthonormal basis given in (5.3). The inner products of \\(x\\) with these vectors\n\nDocument 2:\n----\n* The rows of \\(A\\) are linearly independent.\n* \\(A\\) has a left inverse.\n* \\(A\\) has a right inverse.\n**Examples.**\n* The identity matrix \\(I\\) is invertible, with inverse \\(I^{-1}=I\\), since \\(II=I\\).\n* A diagonal matrix \\(A\\) is invertible if and only if its diagonal entries are nonzero. The inverse of an \\(n\\times n\\) diagonal matrix \\(A\\) with nonzero diagonal entries is \\[A^{-1}=\\left[\\begin{array}{cccc}1/A_{11}&0&\\cdots&0\\\\ 0&1/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&1/A_{nn}\\end{array}\\right],\\] since \\[AA^{-1}=\\left[\\begin{array}{cccc}A_{11}/A_{11}&0&\\cdots&0\\\\ 0&A_{22}/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&A_{nn}/A_{nn}\\end{array}\\right]=I.\\] In compact notation, we have \\[\\mathbf{diag}(A_{11},\\ldots,A_{nn})^{-1}=\\mathbf{diag}(A_{11}^{-1},\\ldots,A_{ nn}^{-1}).\\] Note that the inverse on the left-hand side of this equation is the matrix inverse, while the inverses appearing on the right-hand side are scalar inverses.\n* As a non-obvious example, the matrix \\[A=\\left[\\begin{array}{cccc}1&-2&3\\\\ 0&2&2\\\\ -3&-4&-4\\end{array}\\right]\\] is invertible, with inverse \\[A^{-1}=\\frac{1}{30}\\left[\\begin{array}{cccc}0&-20&-10\\\\ -6&5&-2\\\\ 6&10&2\\end{array}\\right].\\] This can be verified by checking that \\(AA^{-1}=I\\) (or that \\(A^{-1}A=I\\), since either of these implies the other).\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\n\nDocument 3:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\nDocument 4:\n----\nwith state \\(x_{t}\\), input \\(u_{t}\\), and output \\(y_{t}\\). The second is given by\n\\[\\tilde{x}_{t+1}=\\tilde{A}\\tilde{x}_{t}+\\tilde{B}w_{t},\\qquad v_{t}=\\tilde{C} \\tilde{x}_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(\\tilde{x}_{t}\\), input \\(w_{t}\\), and output \\(v_{t}\\). We now connect the output of the first linear dynamical system to the input of the second one, which means we take \\(w_{t}=y_{t}\\). (This is called the _composition_ of the two systems.) Show that this composition can also be expressed as a linear dynamical system with state \\(z_{t}=(x_{t},\\tilde{x}_{t})\\), input \\(u_{t}\\), and output \\(v_{t}\\). (Give the state transition matrix, input matrix, and output matrix.)\n**10.23**: Suppose \\(A\\) is an \\(n\\times n\\) matrix that satisfies \\(A^{2}=0\\). Does this imply that \\(A=0\\)? (This is the case when \\(n=1\\).) If this is (always) true, explain why. If it is not, give a specific counterexample, _i.e._, a matrix \\(A\\) that is nonzero but satisfies \\(A^{2}=0\\).\n**10.24**: _Matrix power identity._ A student says that for any square matrix \\(A\\),\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n\nDocument 5:\n----\nwhere the variables to be chosen are \\(x_{1},\\ldots,x_{T}\\) and \\(u_{1},\\ldots,u_{T-1}\\).\nFormulation as constrained least squares problem.We can solve the linear quadratic control problem (17.8) by setting it up as a big linearly constrained least squares problem. We define the vector \\(z\\) of all these variables, stacked:\n\\[z=(x_{1},\\ldots,x_{T},u_{1},\\ldots,u_{T-1}).\\]\nThe dimension of \\(z\\) is \\(Tn+(T-1)m\\). The control objective can be expressed as \\(\\|\\tilde{A}z-\\tilde{b}\\|^{2}\\), where \\(\\tilde{b}=0\\) and \\(\\tilde{A}\\) is the block matrix\n\\[\\tilde{A}=\\left[\\begin{array}{cccc|cccc}C_{1}&&&&&&&\\\\ &C_{2}&&&&\\\\ &&\\ddots&&&&&\\\\ &&&C_{T}&&&\\\\ \\hline&&&&\\sqrt{\\rho}I&&\\\\ &&&&\\ddots&\\\\ &&&&&&&\\sqrt{\\rho}I\\end{array}\\right].\\]\nIn this matrix, (block) entries not shown are zero, and the identity matrices in the lower right corner have dimension \\(m\\). (The lines in the matrix delineate the portions related to the states and the inputs.) The dynamics constraints, and the initial and final state constraints, can be expressed as \\(\\tilde{C}z=\\tilde{d}\\), with\n\\[\\tilde{C}=\\left[\\begin{array}{cccc|cccc}A_{1}&-I&&&&B_{1}&&&&\\\\ &A_{2}&-I&&&&B_{2}&&\\\\ &&\\ddots&\\ddots&&&&\\ddots&\\\\ &&&A_{T-1}&-I&&&&B_{T-1}\\\\ \\hline I&&&&I&&\\\\ \\end{array}\\right],\\qquad\\tilde{d}=\\left[\\begin{array}{c}0\\\\ 0\\\\ \\vdots\\\\ 0\\\\ \\hline x^{\\rm init}\\\\ x^{\\rm des}\\end{array}\\right],\\]\nwhere (block) entries not shown are zero. (The vertical line separates the portions of the matrix associated with the states and the inputs, and the horizontal lines separate the dynamics equations and the initial and final state constraints.)\nThe solution \\(\\hat{z}\\) of the constrained least squares problem\n\\[\\begin{array}{ll}\\mbox{minimize}&\\|\\tilde{A}z-\\tilde{b}\\|^{2}\\\\ \\mbox{subject to}&\\tilde{C}z=\\tilde{d}\\end{array}\\] (17.9)\ngives us the optimal input trajectory and the associated optimal state (and output) trajectory. The solution \\(\\hat{z}\\) is a linear function of \\(\\tilde{b}\\) and \\(\\tilde{d}\\); since here \\(\\tilde{b}=0\\), it is a linear function of \\(x^{\\rm init}\\) and \\(x^{\\rm des}\\).\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Is there an eigenbasis for the identity matrix I_n?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_2.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "elainewan/math_algebra_7_5.json",
        "question": "For which 2 * 2 matrices A does there exist a nonzero matrix M such that AM = MD, where D = [[2, 0], [0, 3]]? Give your answer in terms of eigenvalues of A.",
        "prediction": "[2, 3]",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor which 2 * 2 matrices A does there exist a nonzero matrix M such that AM = MD, where D = [[2, 0], [0, 3]]? Give your answer in terms of eigenvalues of A.\n\nDocument 1:\n----\n\\[x_{t+1}=Ax_{t}+Bu_{t},\\qquad y_{t}=Cx_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(x_{t}\\), input \\(u_{t}\\), and output \\(y_{t}\\). The second is given by\n\\[\\tilde{x}_{t+1}=\\tilde{A}\\tilde{x}_{t}+\\tilde{B}w_{t},\\qquad v_{t}=\\tilde{C} \\tilde{x}_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(\\tilde{x}_{t}\\), input \\(w_{t}\\), and output \\(v_{t}\\). We now connect the output of the first linear dynamical system to the input of the second one, which means we take \\(w_{t}=y_{t}\\). (This is called the _composition_ of the two systems.) Show that this composition can also be expressed as a linear dynamical system with state \\(z_{t}=(x_{t},\\tilde{x}_{t})\\), input \\(u_{t}\\), and output \\(v_{t}\\). (Give the state transition matrix, input matrix, and output matrix.)\n**10.23**: Suppose \\(A\\) is an \\(n\\times n\\) matrix that satisfies \\(A^{2}=0\\). Does this imply that \\(A=0\\)? (This is the case when \\(n=1\\).) If this is (always) true, explain why. If it is not, give a specific counterexample, _i.e._, a matrix \\(A\\) that is nonzero but satisfies \\(A^{2}=0\\).\n**10.24**: _Matrix power identity._ A student says that for any square matrix \\(A\\),\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n\nDocument 2:\n----\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\nTwo matrices are equal if they have the same size, and the corresponding entries are all equal. As with vectors, we normally deal with matrices with entries that\nare real numbers, which will be our assumption unless we state otherwise. The set of real \\(m\\times n\\) matrices is denoted \\(\\mathbf{R}^{m\\times n}\\). But matrices with complex entries, for example, do arise in some applications.\nMatrix indexing.As with vectors, standard mathematical notation indexes the rows and columns of a matrix starting from 1. In computer languages, matrices are often (but not always) stored as 2-dimensional arrays, which can be indexed in a variety of ways, depending on the language. Lower level languages typically use indices starting from 0; higher level languages and packages that support matrix operations usually use standard mathematical indexing, starting from 1.\nSquare, tall, and wide matrices.A _square_ matrix has an equal number of rows and columns. A square matrix of size \\(n\\times n\\) is said to be of _order_\\(n\\). A _tall_ matrix has more rows than columns (size \\(m\\times n\\) with \\(m>n\\)). A _wide_ matrix has more columns than rows (size \\(m\\times n\\) with \\(n>m\\)).\n\nDocument 3:\n----\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n1. Find all diagonal squareroots of \\(I_{n}\\). How many are there? (For \\(n=1\\), you should get \\(2\\).)\n* Find a nondiagonal \\(2\\times 2\\) matrix \\(A\\) that satisfies \\(A^{2}=I\\). This means that in general there are even more squareroots of \\(I_{n}\\) than you found in part (a).\n* _Circular shift matrices._ Let \\(A\\) be the \\(5\\times 5\\) matrix \\[A=\\left[\\begin{array}{cccc}0&0&0&0&1\\\\ 1&0&0&0&0\\\\ 0&1&0&0&0\\\\ 0&0&1&0&0\\\\ 0&0&0&1&0\\end{array}\\right].\\] 1. How is \\(Ax\\) related to \\(x\\)? Your answer should be in English. _Hint._ See exercise title. 2. What is \\(A^{5}\\)? _Hint._ The answer should make sense, given your answer to part (a).\n* _Dynamics of an economy._ Let \\(x_{1},x_{2},\\ldots\\) be \\(n\\)-vectors that give the level of economic activity of a country in years \\(1,2,\\ldots\\), in \\(n\\) different sectors (like energy, defense, manufacturing). Specifically, \\((x_{t})_{i}\\) is the level of economic activity in economic sector \\(i\\) (say, in billions of dollars) in year \\(t\\). A common model that connects these economic activity vectors is \\(x_{t+1}=Bx_{t}\\), where \\(B\\) is an \\(n\\times n\\) matrix. (See exercise 9.2.) Five a matrix expression for the total economic activity across all sectors in year \\(t=6\\), in terms of the matrix \\(B\\) and the vector of initial activity levels \\(x_{1}\\). Suppose you can increase economic activity in year \\(t=1\\) by some fixed amount (say, one billion dollars) in _one_ sector, by government spending. How should you choose which sector to stimulate so as to maximize the total economic output in year \\(t=6\\)?\n\nDocument 4:\n----\nfor \\(i=1,\\ldots,L\\), where \\(A\\) is \\(m\\times n\\). The cost of the simple method is \\(2Lmn^{2}\\) flops; using Gram caching the cost is \\(mn^{2}+2Ln^{3}=(m+2Ln)n^{2}\\) flops. (We drop the term \\(Lkn^{2}\\), since \\(k=2\\) here.) With \\(m=100n\\) and \\(L=100\\), Gram caching reduces the computational cost by more than a factor of 50. This means that the entire regularization path (_i.e._, the solution for 100 values of \\(\\lambda\\)) can be computed in not much more time than it takes to compute the solution for one value of \\(\\lambda\\).\n#### The kernel trick\nIn this section we focus on another special case, which arises in many applications:\n\\[J=\\|Ax-b\\|^{2}+\\lambda\\|x-x^{\\rm des}\\|^{2},\\] (15.9)\nwhere the \\(m\\times n\\) matrix \\(A\\) is wide, _i.e._, \\(m<n\\), and \\(\\lambda>0\\). (Here we drop the subscripts on \\(A\\), \\(b\\), and \\(m\\) since we have only one matrix in this problem.) The associated \\((m+n)\\times n\\) stacked matrix (see (15.2))\n\\[\\tilde{A}=\\left[\\begin{array}{c}A\\\\ \\sqrt{\\lambda}I\\end{array}\\right]\\]\nalways has linearly independent columns. Using the QR factorization to solve the stacked least squares problem requires \\(2(m+n)n^{2}\\) flops, which grows like \\(n^{3}\\). We\nwill show now how this special problem can be solved far more efficiently when \\(m\\) is much smaller than \\(n\\), using something called the _kernel trick_. Recall that the minimizer of \\(J\\) is given by (see (15.3))\n\nDocument 5:\n----\nThe complexity of this algorithm is dominated by the cost of the QR factorization in step 1, _i.e._, \\(2np^{2}\\) flops.\n## 16 Constrained least squares\n### 16.1 Smallest right inverse\nSuppose the \\(m\\times n\\) matrix \\(A\\) is wide, with linearly independent rows. Its pseudo-inverse \\(A^{\\dagger}\\) is a right inverse of \\(A\\). In fact, there are many right inverses of \\(A\\) and it turns out that \\(A^{\\dagger}\\) is the smallest one among them, as measured by the matrix norm. In other words, if \\(X\\) satisfies \\(AX=I\\), then \\(\\|X\\|\\geq\\|A^{\\dagger}\\|\\). You will show this in this problem.\n1. Suppose \\(AX=I\\), and let \\(x_{1},\\ldots,x_{m}\\) denote the columns of \\(X\\). Let \\(b_{j}\\) denote the \\(j\\)th column of \\(A^{\\dagger}\\). Explain why \\(\\|x_{j}\\|^{2}\\geq\\|b_{j}\\|^{2}\\). _Hint_. Show that \\(z=b_{j}\\) is the vector of smallest norm that satisfies \\(Az=e_{j}\\), for \\(j=1,\\ldots,m\\).\n2. Use the inequalities from part (a) to establish \\(\\|X\\|\\geq\\|A^{\\dagger}\\|\\).\n3. _Matrix least norm problem_. The matrix least norm problem is minimize \\[\\|X\\|^{2}\\] subject to \\[CX=D,\\] where the variable to be chosen is the \\(n\\times k\\) matrix \\(X\\); the \\(p\\times n\\) matrix \\(C\\) and the \\(p\\times k\\) matrix \\(D\\) are given. Show that the solution of this problem is \\(\\hat{X}=C^{\\dagger}D\\), assuming the rows of \\(C\\) are linearly independent. _Hint_. Show that we can find the columns of \\(X\\) independently, by solving a least norm problem for each one.\n\n\n",
        "answer": [
            2,
            3
        ],
        "rationale": {
            "Question": "For which 2 * 2 matrices A does there exist a nonzero matrix M such that AM = MD, where D = [[2, 0], [0, 3]]? Give your answer in terms of eigenvalues of A.",
            "Answer": [
                2,
                3
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_5.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "elainewan/math_algebra_2.json",
        "question": "Is the transformation [[-1, 0], [0, -1]] invertible?",
        "prediction": "invertible",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs the transformation [[-1, 0], [0, -1]] invertible?\n\nDocument 1:\n----\n### 11.5 Pseudo-inverse\nLinearly independent columns and Gram invertibility.We first show that an \\(m\\times n\\) matrix \\(A\\) has linearly independent columns if and only if its \\(n\\times n\\) Gram matrix \\(A^{T}A\\) is invertible.\nFirst suppose that the columns of \\(A\\) are linearly independent. Let \\(x\\) be an \\(n\\)-vector which satisfies \\((A^{T}A)x=0\\). Multiplying on the left by \\(x^{T}\\) we get\n\\[0=x^{T}0=x^{T}(A^{T}Ax)=x^{T}A^{T}Ax=\\|Ax\\|^{2},\\]\nwhich implies that \\(Ax=0\\). Since the columns of \\(A\\) are linearly independent, we conclude that \\(x=0\\). Since the only solution of \\((A^{T}A)x=0\\) is \\(x=0\\), we conclude that \\(A^{T}A\\) is invertible.\nNow let's show the converse. Suppose the columns of \\(A\\) are linearly dependent, which means there is a nonzero \\(n\\)-vector \\(x\\) which satisfies \\(Ax=0\\). Multiply on the left by \\(A^{T}\\) to get \\((A^{T}A)x=0\\). This shows that the Gram matrix \\(A^{T}A\\) is singular.\nPseudo-inverse of square or tall matrix.We show here that if \\(A\\) has linearly independent columns (and therefore, is square or tall) then it has a left inverse. (We already have observed the converse, that a matrix with a left inverse has linearly independent columns.) Assuming \\(A\\) has linearly independent columns, we know that \\(A^{T}A\\) is invertible. We now observe that the matrix \\((A^{T}A)^{-1}A^{T}\\) is a left inverse of \\(A\\):\n\\[\\left((A^{T}A)^{-1}A^{T}\\right)A=(A^{T}A)^{-1}(A^{T}A)=I.\\]\n\nDocument 2:\n----\n**Examples.**\n* The identity matrix \\(I\\) is invertible, with inverse \\(I^{-1}=I\\), since \\(II=I\\).\n* A diagonal matrix \\(A\\) is invertible if and only if its diagonal entries are nonzero. The inverse of an \\(n\\times n\\) diagonal matrix \\(A\\) with nonzero diagonal entries is \\[A^{-1}=\\left[\\begin{array}{cccc}1/A_{11}&0&\\cdots&0\\\\ 0&1/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&1/A_{nn}\\end{array}\\right],\\] since \\[AA^{-1}=\\left[\\begin{array}{cccc}A_{11}/A_{11}&0&\\cdots&0\\\\ 0&A_{22}/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&A_{nn}/A_{nn}\\end{array}\\right]=I.\\] In compact notation, we have \\[\\mathbf{diag}(A_{11},\\ldots,A_{nn})^{-1}=\\mathbf{diag}(A_{11}^{-1},\\ldots,A_{ nn}^{-1}).\\] Note that the inverse on the left-hand side of this equation is the matrix inverse, while the inverses appearing on the right-hand side are scalar inverses.\n* As a non-obvious example, the matrix \\[A=\\left[\\begin{array}{cccc}1&-2&3\\\\ 0&2&2\\\\ -3&-4&-4\\end{array}\\right]\\] is invertible, with inverse \\[A^{-1}=\\frac{1}{30}\\left[\\begin{array}{cccc}0&-20&-10\\\\ -6&5&-2\\\\ 6&10&2\\end{array}\\right].\\] This can be verified by checking that \\(AA^{-1}=I\\) (or that \\(A^{-1}A=I\\), since either of these implies the other).\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\nInverse of matrix transpose.If \\(A\\) is invertible, its transpose \\(A^{T}\\) is also invertible and its inverse is \\((A^{-1})^{T}\\):\n\nDocument 3:\n----\nwill show now how this special problem can be solved far more efficiently when \\(m\\) is much smaller than \\(n\\), using something called the _kernel trick_. Recall that the minimizer of \\(J\\) is given by (see (15.3))\n\\[\\hat{x} = (A^{T}A+\\lambda I)^{-1}(A^{T}b+\\lambda x^{\\rm des})\\] \\[= (A^{T}A+\\lambda I)^{-1}(A^{T}b+(\\lambda I+A^{T}A)x^{\\rm des}-(A^{T }A)x^{\\rm des})\\] \\[= (A^{T}A+\\lambda I)^{-1}A^{T}(b-Ax^{\\rm des})+x^{\\rm des}.\\]\nThe matrix inverse here has size \\(n\\times n\\).\nWe will use the identity\n\\[(A^{T}A+\\lambda I)^{-1}A^{T}=A^{T}(AA^{T}+\\lambda I)^{-1}\\] (15.10)\nwhich holds for any matrix \\(A\\) and any \\(\\lambda>0\\). Note that the left-hand side of the identity involves the inverse of an \\(n\\times n\\) matrix, whereas the right-hand side involves the inverse of a (smaller) \\(m\\times m\\) matrix. (This is a variation on the push-through identity from exercise 11.9.)\nTo show the identity (15.10), we first observe that the matrices \\(A^{T}A+\\lambda I\\) and \\(AA^{T}+\\lambda I\\) are invertible. We start with the equation\n\\[A^{T}(AA^{T}+\\lambda I)=(A^{T}A+\\lambda I)A^{T},\\]\nand multiply each side by \\((A^{T}A+\\lambda I)^{-1}\\) on the left and \\((AA^{T}+\\lambda I)^{-1}\\) on the right, which yields the identity above.\nUsing (15.10) we can express the minimizer of \\(J\\) as\n\\[\\hat{x}=A^{T}(AA^{T}+\\lambda I)^{-1}(b-Ax^{\\rm des})+x^{\\rm des}.\\]\nWe can compute the term \\((AA^{T}+\\lambda I)^{-1}(b-Ax^{\\rm des})\\) by computing the QR factorization of the \\((m+n)\\times m\\) matrix\n\\[\\bar{A}=\\left[\\begin{array}{c}A^{T}\\\\ \\sqrt{\\lambda}I\\end{array}\\right],\\]\n\nDocument 4:\n----\nPseudo-inverse via QR factorization.The QR factorization gives a simple formula for the pseudo-inverse. If \\(A\\) is left-invertible, its columns are linearly independent and the QR factorization \\(A=QR\\) exists. We have\n\\[A^{T}A=(QR)^{T}(QR)=R^{T}Q^{T}QR=R^{T}R,\\]\nso\n\\[A^{\\dagger}=(A^{T}A)^{-1}A^{T}=(R^{T}R)^{-1}(QR)^{T}=R^{-1}R^{-T}R^{T}Q^{T}=R^ {-1}Q^{T}.\\]\nWe can compute the pseudo-inverse using the QR factorization, followed by back substitution on the columns of \\(Q^{T}\\). (This is exactly the same as algorithm 11.3 when \\(A\\) is square and invertible.) The complexity of this method is \\(2n^{2}m\\) flops (for the QR factorization), and \\(mn^{2}\\) flops for the \\(m\\) back substitutions. So the total is \\(3mn^{2}\\) flops.\nSimilarly, if \\(A\\) is right-invertible, the QR factorization \\(A^{T}=QR\\) of its transpose exists. We have \\(AA^{T}=(QR)^{T}(QR)=R^{T}Q^{T}QR=R^{T}R\\) and\n\\[A^{\\dagger}=A^{T}(AA^{T})^{-1}=QR(R^{T}R)^{-1}=QRR^{-1}R^{-T}=QR^{-T}.\\]\nWe can compute it using the method described above, using the formula\n\\[(A^{T})^{\\dagger}=(A^{\\dagger})^{T}.\\]\nSolving over- and under-determined systems of linear equations.The pseudo-inverse gives us a method for solving over-determined and under-determined systems of linear equations, provided the columns of the coefficient matrix are linearly independent (in the over-determined case), or the rows are linearly independent (in the under-determined case). If the columns of \\(A\\) are linearly independent, and the over-determined equations \\(Ax=b\\) have a solution, then \\(x=A^{\\dagger}b\\) is it. If the rows of \\(A\\) are linearly independent, the under-determined equations \\(Ax=b\\) have a solution for any vector \\(b\\), and \\(x=A^{\\dagger}b\\) is a solution.\n\nDocument 5:\n----\n* A diagonal matrix \\(A\\) is invertible if and only if its diagonal entries are nonzero. The inverse of an \\(n\\times n\\) diagonal matrix \\(A\\) with nonzero diagonal entries is \\[A^{-1}=\\left[\\begin{array}{cccc}1/A_{11}&0&\\cdots&0\\\\ 0&1/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&1/A_{nn}\\end{array}\\right],\\] since \\[AA^{-1}=\\left[\\begin{array}{cccc}A_{11}/A_{11}&0&\\cdots&0\\\\ 0&A_{22}/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&A_{nn}/A_{nn}\\end{array}\\right]=I.\\] In compact notation, we have \\[\\mathbf{diag}(A_{11},\\ldots,A_{nn})^{-1}=\\mathbf{diag}(A_{11}^{-1},\\ldots,A_{ nn}^{-1}).\\] Note that the inverse on the left-hand side of this equation is the matrix inverse, while the inverses appearing on the right-hand side are scalar inverses.\n* As a non-obvious example, the matrix \\[A=\\left[\\begin{array}{cccc}1&-2&3\\\\ 0&2&2\\\\ -3&-4&-4\\end{array}\\right]\\] is invertible, with inverse \\[A^{-1}=\\frac{1}{30}\\left[\\begin{array}{cccc}0&-20&-10\\\\ -6&5&-2\\\\ 6&10&2\\end{array}\\right].\\] This can be verified by checking that \\(AA^{-1}=I\\) (or that \\(A^{-1}A=I\\), since either of these implies the other).\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\nInverse of matrix transpose.If \\(A\\) is invertible, its transpose \\(A^{T}\\) is also invertible and its inverse is \\((A^{-1})^{T}\\):\n\\[(A^{T})^{-1}=(A^{-1})^{T}.\\]\nSince the order of the transpose and inverse operations does not matter, this matrix is sometimes written as \\(A^{-T}\\).\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Is the transformation [[-1, 0], [0, -1]] invertible?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_2.json",
            "explanation": "solutions/math_algebra_2.txt",
            "theorem": "invertible matrix theorem",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "wenhuchen/eigen_value2.json",
        "question": "For matrix A = [[3, 1, 1], [2, 4, 2], [1, 1, 3]], what are its eigen values?",
        "prediction": "[1, 4, 5]",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[3, 1, 1], [2, 4, 2], [1, 1, 3]], what are its eigen values?\n\nDocument 1:\n----\nAn important attribute of a matrix is its _size_ or _dimensions_, _i.e._, the numbers of rows and columns. The matrix above has 3 rows and 4 columns, so its size is \\(3\\times 4\\). A matrix of size \\(m\\times n\\) is called an \\(m\\times n\\) matrix.\nThe _elements_ (or _entries_ or _coefficients_) of a matrix are the values in the array. The \\(i,j\\) element is the value in the \\(i\\)th row and \\(j\\)th column, denoted by double subscripts: the \\(i,j\\) element of a matrix \\(A\\) is denoted \\(A_{ij}\\) (or \\(A_{i,j}\\), when \\(i\\) or \\(j\\) is more than one digit or character). The positive integers \\(i\\) and \\(j\\) are called the (row and column) _indices_. If \\(A\\) is an \\(m\\times n\\) matrix, then the row index \\(i\\) runs from 1 to \\(m\\) and the column index \\(j\\) runs from 1 to \\(n\\). Row indices go from top to bottom, so row 1 is the top row and row \\(m\\) is the bottom row. Column indices go from left to right, so column 1 is the left column and column \\(n\\) is the right column.\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\n\nDocument 2:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\nDocument 3:\n----\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\\[l_{i}=\\left[\\begin{array}{c}0_{i-1}\\\\ 1\\\\ -(1+r)\\\\ 0_{n-i-1}\\end{array}\\right],\\quad i=1,\\ldots,n-1,\\]\nwhere \\(r\\geq 0\\) is the per-period interest rate. The cash flow \\(l_{i}\\) represents a loan of $1 in period \\(i\\), which is paid back in period \\(i+1\\) with interest \\(r\\). (The subscripts on the zero vectors above give their dimensions.) Scaling \\(l_{i}\\) changes the loan amount; scaling \\(l_{i}\\) by a negative coefficient converts it into a loan _to_ another entity (which is paid back in period \\(i+1\\) with interest).\n\nDocument 4:\n----\n* \\(A\\), \\(B\\), and \\(C\\) have the same number of rows.\n* \\(B\\) is a tall matrix.\n* _Block matrix notation_. Consider the block matrix \\[A=\\left[\\begin{array}{ccc}I&B&0\\\\ B^{T}&0&0\\\\ 0&0&BB^{T}\\end{array}\\right],\\] where \\(B\\) is \\(10\\times 5\\). What are the dimensions of the four zero matrices and the identity matrix in the definition of \\(A\\)? What are the dimensions of \\(A\\)?\n* _When is the outer product symmetric?_ Let \\(a\\) and \\(b\\) be \\(n\\)-vectors. The inner product is symmetric, _i.e._, we have \\(a^{T}b=b^{T}a\\). The outer product of the two vectors is generally _not_ symmetric; that is, we generally have \\(ab^{T}\\neq ba^{T}\\). What are the conditions on \\(a\\) and \\(b\\) under which \\(ab=ba^{T}\\)? You can assume that all the entries of \\(a\\) and \\(b\\) are nonzero. (The conclusion you come to will hold even when some entries of \\(a\\) or \\(b\\) are zero.) _Hint_. Show that \\(ab^{T}=ba^{T}\\) implies that \\(a_{i}/b_{i}\\) is a constant (_i.e._, independent of \\(i\\)).\n* _Product of rotation matrices_. Let \\(A\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\theta\\) radians, defined in (7.1), and let \\(B\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\omega\\) radians. Show that \\(AB\\) is also a rotation matrix, and give the angle by which it rotates vectors. Verify that \\(AB=BA\\) in this case, and give a simple English explanation.\n\nDocument 5:\n----\n* _Entries of matrix triple product_. (See page 10.) Suppose \\(A\\) has dimensions \\(m\\times n\\), \\(B\\) has dimensions \\(n\\times p\\), \\(C\\) has dimensions \\(p\\times q\\), and let \\(D=ABC\\). Show that \\[D_{ij}=\\sum_{k=1}^{n}\\sum_{l=1}^{p}A_{ik}B_{kl}C_{lj}.\\] This is the formula analogous to (10.1) for the product of two matrices.\n* _Multiplication by a diagonal matrix_. Suppose that \\(A\\) is an \\(m\\times n\\) matrix, \\(D\\) is a diagonal matrix, and \\(B=DA\\). Describe \\(B\\) in terms of \\(A\\) and the entries of \\(D\\). You can refer to the rows or columns or entries of \\(A\\).\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n\n\n",
        "answer": [
            2,
            6
        ],
        "rationale": {
            "Question": "For matrix A = [[3, 1, 1], [2, 4, 2], [1, 1, 3]], what are its eigen values?",
            "Answer": [
                2,
                6
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/eigen_value2.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "wenhuchen/definite_matrix1.json",
        "question": "Consider the matrix of A=[[1, 4], [4, 1]], is this a positive definite matrix?",
        "prediction": "False",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider the matrix of A=[[1, 4], [4, 1]], is this a positive definite matrix?\n\nDocument 1:\n----\n\\[x_{t+1}=Ax_{t}+Bu_{t},\\qquad y_{t}=Cx_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(x_{t}\\), input \\(u_{t}\\), and output \\(y_{t}\\). The second is given by\n\\[\\tilde{x}_{t+1}=\\tilde{A}\\tilde{x}_{t}+\\tilde{B}w_{t},\\qquad v_{t}=\\tilde{C} \\tilde{x}_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(\\tilde{x}_{t}\\), input \\(w_{t}\\), and output \\(v_{t}\\). We now connect the output of the first linear dynamical system to the input of the second one, which means we take \\(w_{t}=y_{t}\\). (This is called the _composition_ of the two systems.) Show that this composition can also be expressed as a linear dynamical system with state \\(z_{t}=(x_{t},\\tilde{x}_{t})\\), input \\(u_{t}\\), and output \\(v_{t}\\). (Give the state transition matrix, input matrix, and output matrix.)\n**10.23**: Suppose \\(A\\) is an \\(n\\times n\\) matrix that satisfies \\(A^{2}=0\\). Does this imply that \\(A=0\\)? (This is the case when \\(n=1\\).) If this is (always) true, explain why. If it is not, give a specific counterexample, _i.e._, a matrix \\(A\\) that is nonzero but satisfies \\(A^{2}=0\\).\n**10.24**: _Matrix power identity._ A student says that for any square matrix \\(A\\),\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n\nDocument 2:\n----\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n**10.38**: _Orthogonal matrix with nonnegative entries._ Suppose the \\(n\\times n\\) matrix \\(A\\) is orthogonal, and all of its entries are nonnegative, _i.e._, \\(A_{ij}\\geq 0\\) for \\(i,j=1,\\ldots,n\\). Show that \\(A\\) must be a permutation matrix, _i.e._, each entry is either \\(0\\) or \\(1\\), each row has exactly one entry with value one, and each column has exactly one entry with value one. (See page 132.)\n**10.39**: _Gram matrix and QR factorization._ Suppose the matrix \\(A\\) has linearly independent columns and QR factorization \\(A=QR\\). What is the relationship between the Gram matrix of \\(A\\) and the Gram matrix of \\(R\\)? What can you say about the angles between the columns of \\(A\\) and the angles between the columns of \\(R\\)?\n\nDocument 3:\n----\nwhere \\(\\lambda^{(k)}\\) is a positive parameter. We add an iteration superscript to the parameter \\(\\lambda\\) since it can take different values in different iterations. For \\(\\lambda^{(k)}\\) small, we primarily minimize the first term, the squared norm of the approximation; for \\(\\lambda^{(k)}\\) large, we choose \\(x^{(k+1)}\\) near \\(x^{(k)}\\). (For \\(\\lambda^{(k)}=0\\), this coincides with the next iterate in the basic Gauss-Newton algorithm.) The second term in (18.11) is sometimes called a _trust penalty_ term, since it penalizes choices of \\(x\\) that are far from \\(x^{(k)}\\), where we cannot trust the affine approximation. The parameter \\(\\lambda^{(k)}\\) is sometimes called the _trust parameter_ (although 'distrust parameter' is perhaps more accurate).\nComputing the minimizer of (18.11) is a multi-objective least squares or regularized least squares problem, and equivalent to minimizing\n\\[\\left\\|\\left[\\begin{array}{c}Df(x^{(k)})\\\\ \\sqrt{\\lambda^{(k)}}I\\end{array}\\right]x-\\left[\\begin{array}{c}Df(x^{(k)})x ^{(k)}-f(x^{(k)})\\\\ \\sqrt{\\lambda^{(k)}}x^{(k)}\\end{array}\\right]\\right\\|^{2}.\\]\nSince \\(\\lambda^{(k)}\\) is positive, the stacked matrix in this least squares problem has linearly independent columns, even when \\(Df(x^{(k)})\\) does not. It follows that the solution of the least squares problem exists and is unique. From the normal equations of the least squares problem we can derive a useful expression for \\(x^{(k+1)}\\):\n\\[\\left(Df(x^{(k)})^{T}Df(x^{(k)})+\\lambda^{(k)}I\\right)x^{(k+1)}\\] \\[= Df(x^{(k)})^{T}\\left(Df(x^{(k)})x^{(k)}-f(x^{(k)})\\right)+\\lambda ^{(k)}x^{(k)}\\] \\[= \\left(Df(x^{(k)})^{T}Df(x^{(k)})+\\lambda^{(k)}I\\right)x^{(k)}-Df( x^{(k)})^{T}f(x^{(k)}),\\]\nand therefore \\[x^{(k+1)}=x^{(k)}-\\left(Df(x^{(k)})^{T}Df(x^{(k)})+\\lambda^{(k)}I\\right)^{-1}Df(x^ {(k)})^{T}f(x^{(k)}).\\] (18.12) The matrix inverse here always exists. From (18.12), we see that \\(x^{(k+1)}=x^{(k)}\\) only if \\(2Df(x^{(k)})^{T}f(x^{(k)})=0\\), _i.e._, only when the optimality condition (18.3) holds for \\(x^{(k)}\\). So like the Gauss-Newton algorithm, the Levenberg-Marquardt algorithm stops (or more accurately, repeats itself with \\(x^{(k+1)}=x^{(k)}\\)) only when the optimality condition (18.3) holds.\n\nDocument 4:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\nDocument 5:\n----\nwhich can be directly verified.\nNegative matrix powers.We can now give a meaning to matrix powers with negative integer exponents. Suppose \\(A\\) is a square invertible matrix and \\(k\\) is a positive integer. Then by repeatedly applying property (11.2), we get\n\\[(A^{k})^{-1}=(A^{-1})^{k}.\\]\nWe denote this matrix as \\(A^{-k}\\). For example, if \\(A\\) is square and invertible, then \\(A^{-2}=A^{-1}A^{-1}=(AA)^{-1}\\). With \\(A^{0}\\) defined as \\(A^{0}=I\\), the identity \\(A^{k+l}=A^{k}A^{l}\\) holds for all integers \\(k\\) and \\(l\\).\nTriangular matrix.A triangular matrix with nonzero diagonal elements is invertible. We first discuss this for a lower triangular matrix. Let \\(L\\) be \\(n\\times n\\) and lower triangular with nonzero diagonal elements. We show that the columns are linearly independent, _i.e._, \\(Lx=0\\) is only possible if \\(x=0\\). Expanding the matrix-vector product, we can write \\(Lx=0\\) as\n\\[L_{11}x_{1} = 0\\] \\[L_{21}x_{1}+L_{22}x_{2} = 0\\] \\[L_{31}x_{1}+L_{32}x_{2}+L_{33}x_{3} = 0\\] \\[\\vdots\\] \\[L_{n1}x_{1}+L_{n2}x_{2}+\\cdots+L_{n,n-1}x_{n-1}+L_{nn}x_{n} = 0.\\]\nSince \\(L_{11}\\neq 0\\), the first equation implies \\(x_{1}=0\\). Using \\(x_{1}=0\\), the second equation reduces to \\(L_{22}x_{2}=0\\). Since \\(L_{22}\\neq 0\\), we conclude that \\(x_{2}=0\\). Using \\(x_{1}=x_{2}=0\\), the third equation now reduces to \\(L_{33}x_{3}=0\\), and since \\(L_{33}\\) is assumed to be nonzero, we have \\(x_{3}=0\\). Continuing this argument, we find that all entries of \\(x\\) are zero, and this shows that the columns of \\(L\\) are linearly independent. It follows that \\(L\\) is invertible.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Consider the matrix of A=[[1, 4], [4, 1]], is this a positive definite matrix?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | https://www.math.usm.edu/lambers/mat419/lecture3.pdf",
            "id": "wenhuchen/definite_matrix1.json",
            "explanation": "NONE",
            "theorem": "definite matrix criteria",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "elainewan/math_algebra_6_4.json",
        "question": "What is the determinant of the matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]?",
        "prediction": "**0**",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the determinant of the matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]?\n\nDocument 1:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\nDocument 2:\n----\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n**10.11**: _Trace of matrix-matrix product_. The sum of the diagonal entries of a square matrix is called the _trace_ of the matrix, denoted \\(\\mathbf{tr}(A)\\).\n1. Suppose \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices. Show that \\[\\mathbf{tr}(A^{T}B)=\\sum_{i=1}^{m}\\sum_{j=1}^{n}A_{ij}B_{ij}.\\] What is the complexity of calculating \\(\\mathbf{tr}(A^{T}B)\\)?\n2. The number \\(\\mathbf{tr}(A^{T}B)\\) is sometimes referred to as the inner product of the matrices \\(A\\) and \\(B\\). (This allows us to extend concepts like angle to matrices.) Show that \\(\\mathbf{tr}(A^{T}B)=\\mathbf{tr}(B^{T}A)\\).\n3. Show that \\(\\mathbf{tr}(A^{T}A)=\\left\\|A\\right\\|^{2}\\). In other words, the square of the norm of a matrix is the trace of its Gram matrix.\n\nDocument 3:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\nDocument 4:\n----\n* _Entries of matrix triple product_. (See page 10.) Suppose \\(A\\) has dimensions \\(m\\times n\\), \\(B\\) has dimensions \\(n\\times p\\), \\(C\\) has dimensions \\(p\\times q\\), and let \\(D=ABC\\). Show that \\[D_{ij}=\\sum_{k=1}^{n}\\sum_{l=1}^{p}A_{ik}B_{kl}C_{lj}.\\] This is the formula analogous to (10.1) for the product of two matrices.\n* _Multiplication by a diagonal matrix_. Suppose that \\(A\\) is an \\(m\\times n\\) matrix, \\(D\\) is a diagonal matrix, and \\(B=DA\\). Describe \\(B\\) in terms of \\(A\\) and the entries of \\(D\\). You can refer to the rows or columns or entries of \\(A\\).\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n\nDocument 5:\n----\n* Find a nondiagonal \\(2\\times 2\\) matrix \\(A\\) that satisfies \\(A^{2}=I\\). This means that in general there are even more squareroots of \\(I_{n}\\) than you found in part (a).\n* _Circular shift matrices._ Let \\(A\\) be the \\(5\\times 5\\) matrix \\[A=\\left[\\begin{array}{cccc}0&0&0&0&1\\\\ 1&0&0&0&0\\\\ 0&1&0&0&0\\\\ 0&0&1&0&0\\\\ 0&0&0&1&0\\end{array}\\right].\\] 1. How is \\(Ax\\) related to \\(x\\)? Your answer should be in English. _Hint._ See exercise title. 2. What is \\(A^{5}\\)? _Hint._ The answer should make sense, given your answer to part (a).\n* _Dynamics of an economy._ Let \\(x_{1},x_{2},\\ldots\\) be \\(n\\)-vectors that give the level of economic activity of a country in years \\(1,2,\\ldots\\), in \\(n\\) different sectors (like energy, defense, manufacturing). Specifically, \\((x_{t})_{i}\\) is the level of economic activity in economic sector \\(i\\) (say, in billions of dollars) in year \\(t\\). A common model that connects these economic activity vectors is \\(x_{t+1}=Bx_{t}\\), where \\(B\\) is an \\(n\\times n\\) matrix. (See exercise 9.2.) Five a matrix expression for the total economic activity across all sectors in year \\(t=6\\), in terms of the matrix \\(B\\) and the vector of initial activity levels \\(x_{1}\\). Suppose you can increase economic activity in year \\(t=1\\) by some fixed amount (say, one billion dollars) in _one_ sector, by government spending. How should you choose which sector to stimulate so as to maximize the total economic output in year \\(t=6\\)?\n\n\n",
        "answer": -3,
        "rationale": {
            "Question": "What is the determinant of the matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]?",
            "Answer": -3,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_4.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "mingyin/orthogonal-similarity1.json",
        "question": "If A and B are both orthogonal square matrices, and det A = -det B. What is det(A+B)? Return the numerical value.",
        "prediction": "\\(-1 \\cdot \\det(I + \\pm 1)\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIf A and B are both orthogonal square matrices, and det A = -det B. What is det(A+B)? Return the numerical value.\n\nDocument 1:\n----\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\nInverse of matrix transpose.If \\(A\\) is invertible, its transpose \\(A^{T}\\) is also invertible and its inverse is \\((A^{-1})^{T}\\):\n\\[(A^{T})^{-1}=(A^{-1})^{T}.\\]\nSince the order of the transpose and inverse operations does not matter, this matrix is sometimes written as \\(A^{-T}\\).\nInverse of matrix product.If \\(A\\) and \\(B\\) are invertible (hence, square) and of the same size, then \\(AB\\) is invertible, and\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\n\nDocument 2:\n----\n4. Express \\(2x_{1}^{2}-3x_{1}x_{2}-x_{2}^{2}\\) as a quadratic form, with symmetric coefficient matrix \\(A\\).\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n**10.38**: _Orthogonal matrix with nonnegative entries._ Suppose the \\(n\\times n\\) matrix \\(A\\) is orthogonal, and all of its entries are nonnegative, _i.e._, \\(A_{ij}\\geq 0\\) for \\(i,j=1,\\ldots,n\\). Show that \\(A\\) must be a permutation matrix, _i.e._, each entry is either \\(0\\) or \\(1\\), each row has exactly one entry with value one, and each column has exactly one entry with value one. (See page 132.)\n**10.39**: _Gram matrix and QR factorization._ Suppose the matrix \\(A\\) has linearly independent columns and QR factorization \\(A=QR\\). What is the relationship between the Gram matrix of \\(A\\) and the Gram matrix of \\(R\\)? What can you say about the angles between the columns of \\(A\\) and the angles between the columns of \\(R\\)?\n\nDocument 3:\n----\nIn chapter 11 we will address the question of how to determine if a system of linear equations has a solution, and how to find one when it does. For now, we give a few interesting examples.\n#### Examples\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\nBalancing chemical reactions.A chemical reaction involves \\(p\\) reactants (molecules) and \\(q\\) products, and can be written as\n\\[a_{1}R_{1}+\\cdots+a_{p}R_{p}\\longrightarrow b_{1}P_{1}+\\cdots+b_{q}P_{q}.\\]\nHere \\(R_{1},\\ldots,R_{p}\\) are the reactants, \\(P_{1},\\ldots,P_{q}\\) are the products, and the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\) are positive numbers that tell us how many of each of these molecules is involved in the reaction. They are typically integers, but can be scaled arbitrarily; we could double all of these numbers, for example, and we still have the same reaction. As a simple example, we have the electrolysis of water,\n\nDocument 4:\n----\nwhere \\(x_{j}\\) is the \\(j\\)th column of \\(X\\) and \\(b_{j}\\) is the \\(j\\)th column of \\(B\\). (Here we use the property that the square of the matrix norm is the sum of the squared norms of the columns of the matrix.) So the objective is a sum of \\(k\\) terms, with each term depending on only one column of \\(X\\). It follows that we can choose the columns \\(x_{j}\\) independently, each one by minimizing its associated term \\(\\|Ax_{j}-b_{j}\\|^{2}\\). Assuming that \\(A\\) has linearly independent columns, the solution is \\(\\hat{x}_{j}=A^{\\dagger}b_{j}\\). The solution of the matrix least squares problem is therefore\n\\[\\hat{X} = \\left[\\begin{array}{cccc}\\hat{x}_{1}&\\cdots&\\hat{x}_{k}\\end{array}\\right]\\] (12.12) \\[= \\left[\\begin{array}{cccc}A^{\\dagger}b_{1}&\\cdots&A^{\\dagger}b_{ k}\\end{array}\\right]\\] \\[= A^{\\dagger}\\left[\\begin{array}{cccc}b_{1}&\\cdots&b_{k}\\end{array}\\right]\\] \\[= A^{\\dagger}B.\\]\nThe very simple solution \\(\\hat{X}=A^{\\dagger}B\\) of the matrix least squares problem agrees with the solution of the ordinary least squares problem when \\(k=1\\) (as it must). Many software packages for linear algebra use the backslash operator \\(A\\backslash B\\) to denote \\(A^{\\dagger}B\\), but this is not standard mathematical notation.\nThe matrix least squares problem can be solved efficiently by exploiting the fact that algorithm 12.1 is another example of a factor-solve algorithm. To compute \\(\\hat{X}=A^{\\dagger}B\\) we carry out the QR factorization of \\(A\\) once; we carry out steps 2 and 3 of algorithm 12.1 for each of the \\(k\\) columns of \\(B\\). The total cost is \\(2mn^{2}+k(2mn+n^{2})\\) flops. When \\(k\\) is small compared to \\(n\\) this is roughly \\(2mn^{2}\\) flops, the same cost as solving a single least squares problem (_i.e._, one with a vector right-hand side).\n\nDocument 5:\n----\n* A diagonal matrix \\(A\\) is invertible if and only if its diagonal entries are nonzero. The inverse of an \\(n\\times n\\) diagonal matrix \\(A\\) with nonzero diagonal entries is \\[A^{-1}=\\left[\\begin{array}{cccc}1/A_{11}&0&\\cdots&0\\\\ 0&1/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&1/A_{nn}\\end{array}\\right],\\] since \\[AA^{-1}=\\left[\\begin{array}{cccc}A_{11}/A_{11}&0&\\cdots&0\\\\ 0&A_{22}/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&A_{nn}/A_{nn}\\end{array}\\right]=I.\\] In compact notation, we have \\[\\mathbf{diag}(A_{11},\\ldots,A_{nn})^{-1}=\\mathbf{diag}(A_{11}^{-1},\\ldots,A_{ nn}^{-1}).\\] Note that the inverse on the left-hand side of this equation is the matrix inverse, while the inverses appearing on the right-hand side are scalar inverses.\n* As a non-obvious example, the matrix \\[A=\\left[\\begin{array}{cccc}1&-2&3\\\\ 0&2&2\\\\ -3&-4&-4\\end{array}\\right]\\] is invertible, with inverse \\[A^{-1}=\\frac{1}{30}\\left[\\begin{array}{cccc}0&-20&-10\\\\ -6&5&-2\\\\ 6&10&2\\end{array}\\right].\\] This can be verified by checking that \\(AA^{-1}=I\\) (or that \\(A^{-1}A=I\\), since either of these implies the other).\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\nInverse of matrix transpose.If \\(A\\) is invertible, its transpose \\(A^{T}\\) is also invertible and its inverse is \\((A^{-1})^{T}\\):\n\\[(A^{T})^{-1}=(A^{-1})^{T}.\\]\nSince the order of the transpose and inverse operations does not matter, this matrix is sometimes written as \\(A^{-T}\\).\n\n\n",
        "answer": 0.0,
        "rationale": {
            "Question": "If A and B are both orthogonal square matrices, and det A = -det B. What is det(A+B)? Return the numerical value.",
            "Answer": 0.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 9.3.3",
            "id": "mingyin/orthogonal-similarity1.json",
            "explanation": "NONE",
            "theorem": "orthogonal similarity",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float"
    },
    {
        "id": "wenhuchen/gauss_lemma2.json",
        "question": "Is 10 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
        "prediction": "False",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs 10 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.\n\nDocument 1:\n----\n1. \\(\\tilde{q}_{i}\\neq 0\\), so the linear dependence test in step 2 is not satisfied, and we do not have a divide-by-zero error in step 3.\n2. \\(q_{1},\\ldots,q_{i}\\) are orthonormal.\n3. \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i}\\).\n4. \\(q_{i}\\) is a linear combination of \\(a_{1},\\ldots,a_{i}\\).\nWe show this by induction. For \\(i=1\\), we have \\(\\tilde{q}_{1}=a_{1}\\). Since \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we must have \\(a_{1}\\neq 0\\), and therefore \\(\\tilde{q}_{1}\\neq 0\\), so assertion 1 holds. The single vector \\(q_{1}\\) (considered as a list with one element) is evidently orthonormal, since \\(\\|q_{1}\\|=1\\), so assertion 2 holds. We have \\(a_{1}=\\|\\tilde{q}_{1}\\|q_{1}\\), and \\(q_{1}=(1/\\|\\tilde{q}_{1}\\|)a_{1}\\), so assertions 3 and 4 hold.\nSuppose our assertion holds for some \\(i-1\\), with \\(i<k\\); we will show it holds for \\(i\\). If \\(\\tilde{q}_{i}=0\\), then \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i-1}\\) (from the first step in the algorithm); but each of these is (by the induction hypothesis) a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), so it follows that \\(a_{i}\\) is a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), which contradicts our assumption that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. So assertion 1 holds for \\(i\\).\nStep 3 of the algorithm ensures that \\(q_{1},\\ldots,q_{i}\\) are normalized; to show they are orthogonal we will show that \\(q_{i}\\perp q_{j}\\) for \\(j=1,\\ldots,i-1\\). (Our induction hypothesis tells us that \\(q_{r}\\perp q_{s}\\) for \\(r,s<i\\).) For any \\(j=1,\\ldots,i-1\\), we have (using step 1)\n\\[q_{j}^{T}\\tilde{q}_{i} = q_{j}^{T}a_{i}-(q_{1}^{T}a_{i})(q_{j}^{T}q_{1})-\\cdots-(q_{i-1}^ {T}a_{i})(q_{j}^{T}q_{i-1})\\] \\[= q_{j}^{T}a_{i}-q_{j}^{T}a_{i}=0,\\]\n\nDocument 2:\n----\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\nDocument 3:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\nBalancing chemical reactions.(See page 154 for background.) We consider the problem of balancing the chemical reaction\n\\[a_{1}\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+a_{2}\\mathrm{Fe}^{2+}+a_{3}\\mathrm{H} ^{+}\\longrightarrow b_{1}\\mathrm{Cr}^{3+}+b_{2}\\mathrm{Fe}^{3+}+b_{3} \\mathrm{H}_{2}\\mathrm{O},\\]\n\nDocument 4:\n----\nThe temperature \\(T\\) of an electronic device containing three processors is an affine function of the power dissipated by the three processors, \\(P=(P_{1},P_{2},P_{3})\\). When all three processors are idling, we have \\(P=(10,10,10)\\), which results in a temperature \\(T=30\\). When the first processor operates at full power and the other two are idling, we have \\(P=(100,10,10)\\), and the temperature rises to \\(T=60\\). When the second processor operates at full power and the other two are idling, we have \\(P=(10,100,10)\\) and \\(T=70\\). When the third processor operates at full power and the other two are idling, we have \\(P=(10,10,100)\\) and \\(T=65\\). Now suppose that all three processors are operated at the same power \\(P^{\\text{same}}\\). How large can \\(P^{\\text{same}}\\) be, if we require that \\(T\\leq 85\\)? _Hint._ From the given data, find the 3-vector \\(a\\) and number \\(b\\) for which \\(T=a^{T}P+b\\). 3. _Motion of a mass in response to applied force_. A unit mass moves on a straight line (in one dimension). The position of the mass at time \\(t\\) (in seconds) is denoted by \\(s(t)\\), and its derivatives (the velocity and acceleration) by \\(s^{\\prime}(t)\\) and \\(s^{\\prime\\prime}(t)\\). The position as a function of time can be determined from Newton's second law \\[s^{\\prime\\prime}(t)=F(t),\\] where \\(F(t)\\) is the force applied at time \\(t\\), and the initial conditions \\(s(0)\\), \\(s^{\\prime}(0)\\). We assume \\(F(t)\\) is piecewise-constant, and is kept constant in intervals of one second. The sequence of forces \\(F(t)\\), for \\(0\\leq t<10\\), can then be represented by a 10-vector \\(f\\), with \\[F(t)=f_{k},\\quad k-1\\leq t<k.\\] Derive expressions for the final velocity \\(s^{\\prime}(10)\\) and final position \\(s(10)\\). Show that \\(s(10)\\) and \\(s^{\\prime}(10)\\) are affine functions of \\(x\\), and give 10-vectors \\(a,c\\) and constants \\(b,d\\) for which \\[s^{\\prime}(10)=a^{T}f+b,\\qquad s(10)=c^{T}f+d.\\] This means that the mapping from the applied force sequence to the final position and velocity is affine. _Hint._ You can use \\[s^{\\prime}(t)=s^{\\prime}(0)+\\int_{0}^{t}F(\\tau)\\;d\\tau,\\qquad s(t)=s(0)+\\int_{ 0}^{t}s^{\\prime}(\\tau)\\;d\\tau.\\] You will find that the mass velocity \\(s^{\\prime}(t)\\) is piecewise-linear.\n\nDocument 5:\n----\nwhere \\(\\lambda_{i}>0\\) are weights, and \\(J_{2},\\ldots,J_{p}\\) are the regularization terms. This is called _regularized inversion_ or _regularized estimation_. We may repeat this for several choices of the weights, and choose the best estimate for the particular application.\nTikhonov regularized inversion.Choosing \\(\\hat{x}\\) to minimize\n\\[\\|Ax-y\\|^{2}+\\lambda\\|x\\|^{2}\\]\nfor some choice of \\(\\lambda>0\\) is called _Tikhonov regularized inversion_, after the mathematician Andrey Tikhonov. Here we seek a guess \\(\\hat{x}\\) that is consistent with the measurements (_i.e._, \\(\\|A\\hat{x}-y\\|^{2}\\) is small), but not too big.\nThe stacked matrix in this case,\n\\[\\tilde{A}=\\left[\\begin{array}{c}A\\\\ \\sqrt{\\lambda}I\\end{array}\\right],\\]\nalways has linearly independent columns, without any assumption about \\(A\\), which can have any dimensions, and need not have linearly independent columns. To see this we note that \\(\\tilde{A}x=(Ax,\\sqrt{\\lambda}x)=0\\) implies that \\(\\sqrt{\\lambda}x=0\\), which implies \\(x=0\\). The Gram matrix associated with \\(\\tilde{A}\\),\n\\[\\tilde{A}^{T}\\tilde{A}=A^{T}A+\\lambda I,\\]\nis therefore always invertible (provided \\(\\lambda>0\\)). The Tikhonov regularized approximate solution is then\n\\[\\hat{x}=(A^{T}A+\\lambda I)^{-1}A^{T}b.\\]\nEqualization.The vector \\(x\\) represents a transmitted signal or message, consisting of \\(n\\) real values. The matrix \\(A\\) represents the mapping from the transmitted signal to what is received (called the _channel_); \\(y=Ax+v\\) includes noise as well as the action of the channel. Guessing what \\(x\\) is, given \\(y\\), can be thought of as un-doing the effects of the channel. In this context, estimation is called _equalization_.\n#### Estimating a periodic time series\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is 10 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | http://mathonline.wikidot.com/example-questions-regarding-gauss-s-lemma",
            "id": "wenhuchen/gauss_lemma2.json",
            "explanation": "NONE",
            "theorem": "gauss's lemma",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "jianyu_xu/integer_programming_2.json",
        "question": "Solve the following linear programming problems: maximize 3x + y subject to (1) -x + y <= 1, (2) 2x + y <= 4, (3) x>= 0 and y >= 0. What's [x, y] for the optimal solution?",
        "prediction": "[1, 2]",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nSolve the following linear programming problems: maximize 3x + y subject to (1) -x + y <= 1, (2) 2x + y <= 4, (3) x>= 0 and y >= 0. What's [x, y] for the optimal solution?\n\nDocument 1:\n----\n* Show that \\(\\|x-\\tilde{x}\\|\\) does not depend on \\(b\\); it only depends on the matrix \\(A\\), \\(\\epsilon\\), and \\(j\\).\n* How would you find the index \\(j\\) that maximizes the value of \\(\\|x-\\tilde{x}\\|\\)? By part (a), your answer should be in terms of \\(A\\) (or quantities derived from \\(A\\)) and \\(\\epsilon\\) only. _Remark._ If a small change in the right-hand side vector \\(b\\) can lead to a large change in the solution, we say that the linear equations \\(Ax=b\\) are _poorly conditioned_ or _ill-conditioned_. As a practical matter it means that unless you are very confident in what the entries of \\(b\\) are, the solution \\(A^{-1}b\\) may not be useful in practice.\n* _Timing test._ Generate a random \\(n\\times n\\) matrix \\(A\\) and an \\(n\\)-vector \\(b\\), for \\(n=500\\), \\(n=1000\\), and \\(n=2000\\). For each of these, compute the solution \\(x=A^{-1}b\\) (for example using the backslash operator, if the software you are using supports it), and verify that \\(Ax-b\\) is (very) small. Report the time it takes to solve each of these three sets of linear equations, and for each one work out the implied speed of your processor in Gflop/s, based on the \\(2n^{3}\\) complexity of solving equations using the QR factorization.\n\nDocument 2:\n----\n## Chapter 18 Nonlinear least squares\nIn previous chapters we studied the problems of solving a set of linear equations or finding a least squares approximate solution to them. In this chapter we study extensions of these problems in which linear is replaced with nonlinear. These nonlinear problems are in general hard to solve exactly, but we describe a heuristic algorithm that often works well in practice.\n\nDocument 3:\n----\nwhich states that \\(\\hat{x}\\) satisfies the equality constraints \\(C\\hat{x}=d\\) (which we already knew). The first set of equations, however, is more informative. Expanding the objective \\(\\|Ax-b\\|^{2}\\) as a sum of terms involving the entries of \\(x\\) (as was done on page 229) and taking the partial derivative of \\(L\\) with respect to \\(x_{i}\\) we obtain\n\\[\\frac{\\partial L}{\\partial x_{i}}(\\hat{x},\\hat{z})=2\\sum_{j=1}^{n}(A^{T}A)_{ij }\\hat{x}_{j}-2(A^{T}b)_{i}+\\sum_{j=1}^{p}\\hat{z}_{j}(c_{j})_{i}=0.\\]\nThese equations can be written in compact matrix-vector form as\n\\[2(A^{T}A)\\hat{x}-2A^{T}b+C^{T}\\hat{z}=0.\\]\nCombining this set of linear equations with the feasibility conditions \\(C\\hat{x}=d\\), we can write the optimality conditions (16.3) as one set of \\(n+p\\) linear equations in the variables \\((\\hat{x},\\hat{z})\\):\n\\[\\left[\\begin{array}{cc}2A^{T}A&C^{T}\\\\ C&0\\end{array}\\right]\\left[\\begin{array}{c}\\hat{x}\\\\ \\hat{z}\\end{array}\\right]=\\left[\\begin{array}{c}2A^{T}b\\\\ d\\end{array}\\right].\\] (16.4)\nThese equations are called the _KKT equations_ for the constrained least squares problem. (KKT are the initials of the last names of William Karush, Harold Kuhn, and Albert Tucker, the three researchers who derived the optimality conditions for a more general form of constrained optimization problem.) The KKT equations (16.4) are an extension of the normal equations (12.4) for a least squares problem with no constraints. So we have reduced the constrained least squares problem to the problem of solving a (square) set of \\(n+p\\) linear equations in \\(n+p\\) variables \\((\\hat{x},\\hat{z})\\).\nInvertibility of KKT matrix.The \\((n+p)\\times(n+p)\\) coefficient matrix in (16.4) is called the _KKT matrix_. It is invertible if and only if\n\nDocument 4:\n----\n\\[\\begin{array}{ll}\\mbox{minimize}&\\|Ax-b\\|^{2}\\\\ \\mbox{subject to}&c_{i}^{T}x=d_{i},\\quad i=1,\\ldots,p,\\end{array}\\]\nwhere \\(c_{i}^{T}\\) are the rows of \\(C\\). We form the _Lagrangian function_\n\\[L(x,z)=\\|Ax-b\\|^{2}+z_{1}(c_{1}^{T}x-d_{1})+\\cdots+z_{p}(c_{p}^{T}x-d_{p}),\\]\nwhere \\(z\\) is the \\(p\\)-vector of _Lagrange multipliers_. The method of Lagrange multipliers tells us that if \\(\\hat{x}\\) is a solution of the constrained least squares problem, then there is a set of Lagrange multipliers \\(\\hat{z}\\) that satisfy\n\\[\\frac{\\partial L}{\\partial x_{i}}(\\hat{x},\\hat{z})=0,\\quad i=1,\\ldots,n,\\qquad \\frac{\\partial L}{\\partial z_{i}}(\\hat{x},\\hat{z})=0,\\quad i=1,\\ldots,p.\\] (16.3)\nThese are the _optimality conditions_ for the constrained least squares problem. Any solution of the constrained least squares problem must satisfy them. We will now see that the optimality conditions can be expressed as a set of linear equations.\nThe second set of equations in the optimality conditions can be written as\n\\[\\frac{\\partial L}{\\partial z_{i}}(\\hat{x},\\hat{z})=c_{i}^{T}\\hat{x}-d_{i}=0, \\quad i=1,\\ldots,p,\\]\nFigure 16.4: The smallest force sequence \\(f^{\\ln}\\) that transfers the mass over a unit distance in 10 steps. _Right:_ The resulting position of the mass \\(p(t)\\).\nwhich states that \\(\\hat{x}\\) satisfies the equality constraints \\(C\\hat{x}=d\\) (which we already knew). The first set of equations, however, is more informative. Expanding the objective \\(\\|Ax-b\\|^{2}\\) as a sum of terms involving the entries of \\(x\\) (as was done on page 229) and taking the partial derivative of \\(L\\) with respect to \\(x_{i}\\) we obtain\n\nDocument 5:\n----\nWe refer to the function \\(\\|Ax-b\\|^{2}\\) as the _objective_ of the problem, and the set of \\(p\\) linear equality constraints \\(Cx=d\\) as the _constraints_ of the problem. They can be written out as \\(p\\) scalar constraints (equations)\n\\[c_{i}^{T}x=d_{i},\\quad i=1,\\ldots,p,\\]\nwhere \\(c_{i}^{T}\\) is the \\(i\\)th row of \\(C\\).\nAn \\(n\\)-vector \\(x\\) is called _feasible_ (for the problem (16.1)) if it satisfies the constraints, _i.e._, \\(Cx=d\\). An \\(n\\)-vector \\(\\hat{x}\\) is called an _optimal point_ or _solution_ of the optimization problem (16.1) if it is feasible, and if \\(\\|A\\hat{x}-b\\|^{2}\\leq\\|Ax-b\\|^{2}\\) holds for any feasible \\(x\\). In other words, \\(\\hat{x}\\) solves the problem (16.1) if it is feasible and has the smallest possible value of the objective function among all feasible vectors.\nThe constrained least squares problem combines the problems of solving a set of linear equations (find \\(x\\) that satisfies \\(Cx=d\\)) with the least squares problem\n(find \\(x\\) that minimizes \\(\\|Ax-b\\|^{2}\\)). Indeed each of these problems can be considered a special case of the constrained least squares problem (16.1).\nThe constrained least squares problem can also be thought of as a limit of a bi-objective least squares problem, with primary objective \\(\\|Ax-b\\|^{2}\\) and secondary objective \\(\\|Cx-d\\|^{2}\\). Roughly speaking, we put infinite weight on the second objective, so that any nonzero value is unacceptable (which forces \\(x\\) to satisfy \\(Cx=d\\)). So we would expect (and it can be verified) that minimizing the weighted objective\n\\[\\|Ax-b\\|^{2}+\\lambda\\|Cx-d\\|^{2},\\]\n\n\n",
        "answer": [
            2,
            0
        ],
        "rationale": {
            "Question": "Solve the following linear programming problems: maximize 3x + y subject to (1) -x + y <= 1, (2) 2x + y <= 4, (3) x>= 0 and y >= 0. What's [x, y] for the optimal solution?",
            "Answer": [
                2,
                0
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "self",
            "id": "jianyu_xu/integer_programming_2.json",
            "explanation": "NONE",
            "theorem": "integer programming",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "wenhuchen/definite_matrix2.json",
        "question": "Consider the matrix of A=[[1, -1], [-1, 4]], is this a positive definite matrix?",
        "prediction": "True",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider the matrix of A=[[1, -1], [-1, 4]], is this a positive definite matrix?\n\nDocument 1:\n----\n\\[x_{t+1}=Ax_{t}+Bu_{t},\\qquad y_{t}=Cx_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(x_{t}\\), input \\(u_{t}\\), and output \\(y_{t}\\). The second is given by\n\\[\\tilde{x}_{t+1}=\\tilde{A}\\tilde{x}_{t}+\\tilde{B}w_{t},\\qquad v_{t}=\\tilde{C} \\tilde{x}_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(\\tilde{x}_{t}\\), input \\(w_{t}\\), and output \\(v_{t}\\). We now connect the output of the first linear dynamical system to the input of the second one, which means we take \\(w_{t}=y_{t}\\). (This is called the _composition_ of the two systems.) Show that this composition can also be expressed as a linear dynamical system with state \\(z_{t}=(x_{t},\\tilde{x}_{t})\\), input \\(u_{t}\\), and output \\(v_{t}\\). (Give the state transition matrix, input matrix, and output matrix.)\n**10.23**: Suppose \\(A\\) is an \\(n\\times n\\) matrix that satisfies \\(A^{2}=0\\). Does this imply that \\(A=0\\)? (This is the case when \\(n=1\\).) If this is (always) true, explain why. If it is not, give a specific counterexample, _i.e._, a matrix \\(A\\) that is nonzero but satisfies \\(A^{2}=0\\).\n**10.24**: _Matrix power identity._ A student says that for any square matrix \\(A\\),\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n\nDocument 2:\n----\nwhere \\(\\lambda^{(k)}\\) is a positive parameter. We add an iteration superscript to the parameter \\(\\lambda\\) since it can take different values in different iterations. For \\(\\lambda^{(k)}\\) small, we primarily minimize the first term, the squared norm of the approximation; for \\(\\lambda^{(k)}\\) large, we choose \\(x^{(k+1)}\\) near \\(x^{(k)}\\). (For \\(\\lambda^{(k)}=0\\), this coincides with the next iterate in the basic Gauss-Newton algorithm.) The second term in (18.11) is sometimes called a _trust penalty_ term, since it penalizes choices of \\(x\\) that are far from \\(x^{(k)}\\), where we cannot trust the affine approximation. The parameter \\(\\lambda^{(k)}\\) is sometimes called the _trust parameter_ (although 'distrust parameter' is perhaps more accurate).\nComputing the minimizer of (18.11) is a multi-objective least squares or regularized least squares problem, and equivalent to minimizing\n\\[\\left\\|\\left[\\begin{array}{c}Df(x^{(k)})\\\\ \\sqrt{\\lambda^{(k)}}I\\end{array}\\right]x-\\left[\\begin{array}{c}Df(x^{(k)})x ^{(k)}-f(x^{(k)})\\\\ \\sqrt{\\lambda^{(k)}}x^{(k)}\\end{array}\\right]\\right\\|^{2}.\\]\nSince \\(\\lambda^{(k)}\\) is positive, the stacked matrix in this least squares problem has linearly independent columns, even when \\(Df(x^{(k)})\\) does not. It follows that the solution of the least squares problem exists and is unique. From the normal equations of the least squares problem we can derive a useful expression for \\(x^{(k+1)}\\):\n\\[\\left(Df(x^{(k)})^{T}Df(x^{(k)})+\\lambda^{(k)}I\\right)x^{(k+1)}\\] \\[= Df(x^{(k)})^{T}\\left(Df(x^{(k)})x^{(k)}-f(x^{(k)})\\right)+\\lambda ^{(k)}x^{(k)}\\] \\[= \\left(Df(x^{(k)})^{T}Df(x^{(k)})+\\lambda^{(k)}I\\right)x^{(k)}-Df( x^{(k)})^{T}f(x^{(k)}),\\]\nand therefore \\[x^{(k+1)}=x^{(k)}-\\left(Df(x^{(k)})^{T}Df(x^{(k)})+\\lambda^{(k)}I\\right)^{-1}Df(x^ {(k)})^{T}f(x^{(k)}).\\] (18.12) The matrix inverse here always exists. From (18.12), we see that \\(x^{(k+1)}=x^{(k)}\\) only if \\(2Df(x^{(k)})^{T}f(x^{(k)})=0\\), _i.e._, only when the optimality condition (18.3) holds for \\(x^{(k)}\\). So like the Gauss-Newton algorithm, the Levenberg-Marquardt algorithm stops (or more accurately, repeats itself with \\(x^{(k+1)}=x^{(k)}\\)) only when the optimality condition (18.3) holds.\n\nDocument 3:\n----\nwhich can be directly verified.\nNegative matrix powers.We can now give a meaning to matrix powers with negative integer exponents. Suppose \\(A\\) is a square invertible matrix and \\(k\\) is a positive integer. Then by repeatedly applying property (11.2), we get\n\\[(A^{k})^{-1}=(A^{-1})^{k}.\\]\nWe denote this matrix as \\(A^{-k}\\). For example, if \\(A\\) is square and invertible, then \\(A^{-2}=A^{-1}A^{-1}=(AA)^{-1}\\). With \\(A^{0}\\) defined as \\(A^{0}=I\\), the identity \\(A^{k+l}=A^{k}A^{l}\\) holds for all integers \\(k\\) and \\(l\\).\nTriangular matrix.A triangular matrix with nonzero diagonal elements is invertible. We first discuss this for a lower triangular matrix. Let \\(L\\) be \\(n\\times n\\) and lower triangular with nonzero diagonal elements. We show that the columns are linearly independent, _i.e._, \\(Lx=0\\) is only possible if \\(x=0\\). Expanding the matrix-vector product, we can write \\(Lx=0\\) as\n\\[L_{11}x_{1} = 0\\] \\[L_{21}x_{1}+L_{22}x_{2} = 0\\] \\[L_{31}x_{1}+L_{32}x_{2}+L_{33}x_{3} = 0\\] \\[\\vdots\\] \\[L_{n1}x_{1}+L_{n2}x_{2}+\\cdots+L_{n,n-1}x_{n-1}+L_{nn}x_{n} = 0.\\]\nSince \\(L_{11}\\neq 0\\), the first equation implies \\(x_{1}=0\\). Using \\(x_{1}=0\\), the second equation reduces to \\(L_{22}x_{2}=0\\). Since \\(L_{22}\\neq 0\\), we conclude that \\(x_{2}=0\\). Using \\(x_{1}=x_{2}=0\\), the third equation now reduces to \\(L_{33}x_{3}=0\\), and since \\(L_{33}\\) is assumed to be nonzero, we have \\(x_{3}=0\\). Continuing this argument, we find that all entries of \\(x\\) are zero, and this shows that the columns of \\(L\\) are linearly independent. It follows that \\(L\\) is invertible.\n\nDocument 4:\n----\n* _Transpose of orthogonal matrix._ Let \\(U\\) be an orthogonal \\(n\\times n\\) matrix. Show that its transpose \\(U^{T}\\) is also orthogonal.\n* _Inverse of a block matrix._ Consider the \\((n+1)\\times(n+1)\\) matrix \\[A=\\left[\\begin{array}{cc}I&a\\\\ a^{T}&0\\end{array}\\right],\\] where \\(a\\) is an \\(n\\)-vector. 1. When is \\(A\\) invertible? Give your answer in terms of \\(a\\). Justify your answer. 2. Assuming the condition you found in part (a) holds, give an expression for the inverse matrix \\(A^{-1}\\).\n* _Inverse of a block upper triangular matrix._ Let \\(B\\) and \\(D\\) be invertible matrices of sizes \\(m\\times m\\) and \\(n\\times n\\), respectively, and let \\(C\\) be any \\(m\\times n\\) matrix. Find the inverse of \\[A=\\left[\\begin{array}{cc}B&C\\\\ 0&D\\end{array}\\right]\\] in terms of \\(B^{-1}\\), \\(C\\), and \\(D^{-1}\\). (The matrix \\(A\\) is called _block upper triangular_.) _Hints._ First get an idea of what the solution should look like by considering the case when \\(B\\), \\(C\\), and \\(D\\) are scalars. For the matrix case, your goal is to find matrices \\(W\\), \\(X\\), \\(Y\\), \\(Z\\) (in terms of \\(B^{-1}\\), \\(C\\), and \\(D^{-1}\\)) that satisfy \\[A\\left[\\begin{array}{cc}W&X\\\\ Y&Z\\end{array}\\right]=I.\\] Use block matrix multiplication to express this as a set of four matrix equations that you can then solve. The method you will find is sometimes called _block back substitution._\n\nDocument 5:\n----\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n**10.38**: _Orthogonal matrix with nonnegative entries._ Suppose the \\(n\\times n\\) matrix \\(A\\) is orthogonal, and all of its entries are nonnegative, _i.e._, \\(A_{ij}\\geq 0\\) for \\(i,j=1,\\ldots,n\\). Show that \\(A\\) must be a permutation matrix, _i.e._, each entry is either \\(0\\) or \\(1\\), each row has exactly one entry with value one, and each column has exactly one entry with value one. (See page 132.)\n**10.39**: _Gram matrix and QR factorization._ Suppose the matrix \\(A\\) has linearly independent columns and QR factorization \\(A=QR\\). What is the relationship between the Gram matrix of \\(A\\) and the Gram matrix of \\(R\\)? What can you say about the angles between the columns of \\(A\\) and the angles between the columns of \\(R\\)?\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Consider the matrix of A=[[1, -1], [-1, 4]], is this a positive definite matrix?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | https://www.math.usm.edu/lambers/mat419/lecture3.pdf",
            "id": "wenhuchen/definite_matrix2.json",
            "explanation": "NONE",
            "theorem": "definite matrix criteria",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "wenhuchen/vieta's_formula.json",
        "question": "Let m and n be the roots of the equation 2x^2 + 15x + 16 = 0. What is the value of 1/m + 1/n?",
        "prediction": "\\( -\\frac{30}{97} \\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet m and n be the roots of the equation 2x^2 + 15x + 16 = 0. What is the value of 1/m + 1/n?\n\nDocument 1:\n----\nThe second step cannot fail, provided the assumption (16.5) holds. Let us analyze the complexity of this algorithm. The first step, forming the Gram matrix, requires \\(mn^{2}\\) flops (see page 16.3). The second step requires the solution of a square system of \\(n+p\\) equations, which costs \\(2(n+p)^{3}\\) flops, so the total is\n\\[mn^{2}+2(n+p)^{3}\\]\nflops. This grows linearly in \\(m\\) and cubicly in \\(n\\) and \\(p\\). The assumption (16.5) implies \\(p\\leq n\\), so in terms of order, \\((n+p)^{3}\\) can be replaced with \\(n^{3}\\).\nSolving constrained least squares problems via QR factorization.We now give a method for solving the constrained least squares problem that generalizes the QR factorization method for least squares problems (algorithm 12.1). We assume that \\(A\\) and \\(C\\) satisfy the conditions (16.5).\nWe start by rewriting the KKT equations (16.4) as\n\\[2(A^{T}A+C^{T}C)\\hat{x}+C^{T}w=2A^{T}b,\\qquad C\\hat{x}=d\\] (16.7)\nwith a new variable \\(w=\\hat{z}-2d\\). To obtain (16.7) we multiplied the equation \\(C\\hat{x}=d\\) on the left by \\(2C^{T}\\), then added the result to the first equation of (16.4), and replaced the variable \\(\\hat{z}\\) with \\(w+2d\\).\nNext we use the QR factorization\n\\[\\left[\\begin{array}{c}A\\\\ C\\end{array}\\right]=QR=\\left[\\begin{array}{c}Q_{1}\\\\ Q_{2}\\end{array}\\right]R\\] (16.8)\nto simplify (16.7). This factorization exists because the stacked matrix has linearly independent columns, by our assumption (16.5). In (16.8) we also partition \\(Q\\) in two blocks \\(Q_{1}\\) and \\(Q_{2}\\), of size \\(m\\times n\\) and \\(p\\times n\\), respectively. If we make the substitutions \\(A=Q_{1}R\\), \\(C=Q_{2}R\\), and \\(A^{T}A+C^{T}C=R^{T}R\\) in (16.7) we obtain\n\\[2R^{T}R\\hat{x}+R^{T}Q_{2}^{T}w=2R^{T}Q_{1}^{T}b,\\qquad Q_{2}R\\hat{x}=d.\\]\nWe multiply the first equation on the left by \\(R^{-T}\\) (which we know exists) to get\n\nDocument 2:\n----\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\nA simple solution of these equations is \\(a=0\\), \\(b=0\\). But we seek a nonzero solution. We can set one of the coefficients, say \\(a_{1}\\), to be one. (This might cause the other quantities to be fractional-valued.) We can add the condition that \\(a_{1}=1\\) to our system of linear equations as\n\\[\\left[\\begin{array}{cc}R&-P\\\\ e_{1}^{T}&0\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=e_{m+1}.\\]\nFinally, we have a set of \\(m+1\\) equations in \\(p+q\\) variables that expresses the requirement that the chemical reaction balances. Finding a solution of this set of equations is called _balancing_ the chemical reaction.\nFor the example of electrolysis of water described above, we have \\(p=1\\) reactant (water) and \\(q=2\\) products (molecular hydrogen and oxygen). The reaction involves \\(m=2\\) atoms, hydrogen and oxygen. The reactant and product matrices are\n\nDocument 3:\n----\n**2.7**: _General formula for affine functions._ Verify that formula (2.4) holds for any affine function \\(f:{\\bf R}^{n}\\to{\\bf R}\\). You can use the fact that \\(f(x)=a^{T}x+b\\) for some \\(n\\)-vector \\(a\\) and scalar \\(b\\).\n**2.8**: _Integral and derivative of polynomial._ Suppose the \\(n\\)-vector \\(c\\) gives the coefficients of a polynomial \\(p(x)=c_{1}+c_{2}x+\\cdots+c_{n}x^{n-1}\\).\n1. Let \\(\\alpha\\) and \\(\\beta\\) be numbers with \\(\\alpha<\\beta\\). Find an \\(n\\)-vector \\(a\\) for which \\[a^{T}c=\\int_{\\alpha}^{\\beta}p(x)\\;dx\\] always holds. This means that the integral of a polynomial over an interval is a linear function of its coefficients.\n2. Let \\(\\alpha\\) be a number. Find an \\(n\\)-vector \\(b\\) for which \\[b^{T}c=p^{\\prime}(\\alpha).\\] This means that the derivative of the polynomial at a given point is a linear function of its coefficients.\n**2.9**: _Taylor approximation._ Consider the function \\(f:{\\bf R}^{2}\\to{\\bf R}\\) given by \\(f(x_{1},x_{2})=x_{1}x_{2}\\). Find the Taylor approximation \\(\\hat{f}\\) at the point \\(z=(1,1)\\). Compare \\(f(x)\\) and \\(\\hat{f}(x)\\) for the following values of \\(x\\): \\[x=(1,1),\\quad x=(1.05,0.95),\\quad x=(0.85,1.25),\\quad x=(-1,2).\\] Make a brief comment about the accuracy of the Taylor approximation in each case.\n**2.10**: _Regression model._ Consider the regression model \\(\\hat{y}=x^{T}\\beta+v\\), where \\(\\hat{y}\\) is the predicted response, \\(x\\) is an 8-vector of features, \\(\\beta\\) is an 8-vector of coefficients, and \\(v\\) is the offset term. Determine whether each of the following statements is true or false.\n\nDocument 4:\n----\nVector-valued functions of vectors.The notation \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}^{m}\\) means that \\(f\\) is a function that maps real \\(n\\)-vectors to real \\(m\\)-vectors. The value of the function \\(f\\), evaluated at an \\(n\\)-vector \\(x\\), is an \\(m\\)-vector \\(f(x)=(f_{1}(x),f_{2}(x),\\ldots,f_{m}(x))\\). Each of the components \\(f_{i}\\) of \\(f\\) is itself a scalar-valued function of \\(x\\). As with scalar-valued functions, we sometimes write \\(f(x)=f(x_{1},x_{2},\\ldots,x_{n})\\) to emphasize that \\(f\\) is a function of \\(n\\) scalar arguments. We use the same notation for each of the components of \\(f\\), writing \\(f_{i}(x)=f_{i}(x_{1},x_{2},\\ldots,x_{n})\\) to emphasize that \\(f_{i}\\) is a function mapping the scalar arguments \\(x_{1},\\ldots,x_{n}\\) into a scalar.\nThe matrix-vector product function.Suppose \\(A\\) is an \\(m\\times n\\) matrix. We can define a function \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}^{m}\\) by \\(f(x)=Ax\\). The inner product function \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}\\), defined as \\(f(x)=a^{T}x\\), discussed in SS2.1, is the special case with \\(m=1\\).\nSuperposition and linearity.The function \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}^{m}\\), defined by \\(f(x)=Ax\\), is _linear_, _i.e._, it satisfies the superposition property:\n\\[f(\\alpha x+\\beta y)=\\alpha f(x)+\\beta f(y)\\] (8.1)\nholds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). It is a good exercise to parse this simple looking equation, since it involves overloading of notation. On the left-hand side, the scalar-vector multiplications \\(\\alpha x\\) and \\(\\beta y\\) involve \\(n\\)-vectors, and the sum \\(\\alpha x+\\beta y\\) is the sum of two \\(n\\)-vectors. The function \\(f\\) maps \\(n\\)-vectors to \\(m\\)-vectors, so \\(f(\\alpha x+\\beta y)\\) is an \\(m\\)-vector. On the right-hand side, the scalar-vector multiplications and the sum are those for \\(m\\)-vectors. Finally, the equality sign is equality between two \\(m\\)-vectors.\nWe can verify that superposition holds for \\(f\\) using properties of matrix-vector and scalar-vector multiplication:\n\\[f(\\alpha x+\\beta y) = A(\\alpha x+\\beta y)\\] \\[= A(\\alpha x)+A(\\beta y)\\] \\[= \\alpha(Ax)+\\beta(Ay)\\] \\[= \\alpha f(x)+\\beta f(y)\\]\nThus we can associate with every matrix \\(A\\) a linear function \\(f(x)=Ax\\).\n\nDocument 5:\n----\n\\[\\beta_{1}e_{1}+\\beta_{2}l_{1}+\\cdots+\\beta_{n}l_{n-1}=0.\\]\nWe can express this as\n\\[\\left[\\begin{array}{c}\\beta_{1}+\\beta_{2}\\\\ \\beta_{3}-(1+r)\\beta_{2}\\\\ \\vdots\\\\ \\beta_{n}-(1+r)\\beta_{n-1}\\\\ -(1+r)\\beta_{n}\\end{array}\\right]=0.\\]\nThe last entry is \\(-(1+r)\\beta_{n}=0\\), which implies that \\(\\beta_{n}=0\\) (since \\(1+r>0\\)). Using \\(\\beta_{n}=0\\), the second to last entry becomes \\(-(1+r)\\beta_{n-1}=0\\), so we conclude that \\(\\beta_{n-1}=0\\). Continuing this way we find that \\(\\beta_{n-2},\\ldots,\\beta_{2}\\) are all zero. The\n## Chapter 5 Linear independence\n### 5.1 Linear independence\nLinear independence is a generalization of the theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of\n\n\n",
        "answer": -0.9375,
        "rationale": {
            "Question": "Let m and n be the roots of the equation 2x^2 + 15x + 16 = 0. What is the value of 1/m + 1/n?",
            "Answer": -0.9375,
            "Answer_type": "float",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float"
    },
    {
        "id": "elainewan/math_algebra_7_4.json",
        "question": "What are the real eigenvalues of the matrix [[3, -2, 5], [1, 0, 7], [0, 0, 2]]?",
        "prediction": "[3, 2, 0]",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat are the real eigenvalues of the matrix [[3, -2, 5], [1, 0, 7], [0, 0, 2]]?\n\nDocument 1:\n----\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\nTwo matrices are equal if they have the same size, and the corresponding entries are all equal. As with vectors, we normally deal with matrices with entries that\nare real numbers, which will be our assumption unless we state otherwise. The set of real \\(m\\times n\\) matrices is denoted \\(\\mathbf{R}^{m\\times n}\\). But matrices with complex entries, for example, do arise in some applications.\nMatrix indexing.As with vectors, standard mathematical notation indexes the rows and columns of a matrix starting from 1. In computer languages, matrices are often (but not always) stored as 2-dimensional arrays, which can be indexed in a variety of ways, depending on the language. Lower level languages typically use indices starting from 0; higher level languages and packages that support matrix operations usually use standard mathematical indexing, starting from 1.\nSquare, tall, and wide matrices.A _square_ matrix has an equal number of rows and columns. A square matrix of size \\(n\\times n\\) is said to be of _order_\\(n\\). A _tall_ matrix has more rows than columns (size \\(m\\times n\\) with \\(m>n\\)). A _wide_ matrix has more columns than rows (size \\(m\\times n\\) with \\(n>m\\)).\n\nDocument 2:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\nDocument 3:\n----\n* \\(A\\), \\(B\\), and \\(C\\) have the same number of rows.\n* \\(B\\) is a tall matrix.\n* _Block matrix notation_. Consider the block matrix \\[A=\\left[\\begin{array}{ccc}I&B&0\\\\ B^{T}&0&0\\\\ 0&0&BB^{T}\\end{array}\\right],\\] where \\(B\\) is \\(10\\times 5\\). What are the dimensions of the four zero matrices and the identity matrix in the definition of \\(A\\)? What are the dimensions of \\(A\\)?\n* _When is the outer product symmetric?_ Let \\(a\\) and \\(b\\) be \\(n\\)-vectors. The inner product is symmetric, _i.e._, we have \\(a^{T}b=b^{T}a\\). The outer product of the two vectors is generally _not_ symmetric; that is, we generally have \\(ab^{T}\\neq ba^{T}\\). What are the conditions on \\(a\\) and \\(b\\) under which \\(ab=ba^{T}\\)? You can assume that all the entries of \\(a\\) and \\(b\\) are nonzero. (The conclusion you come to will hold even when some entries of \\(a\\) or \\(b\\) are zero.) _Hint_. Show that \\(ab^{T}=ba^{T}\\) implies that \\(a_{i}/b_{i}\\) is a constant (_i.e._, independent of \\(i\\)).\n* _Product of rotation matrices_. Let \\(A\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\theta\\) radians, defined in (7.1), and let \\(B\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\omega\\) radians. Show that \\(AB\\) is also a rotation matrix, and give the angle by which it rotates vectors. Verify that \\(AB=BA\\) in this case, and give a simple English explanation.\n\nDocument 4:\n----\nwill show now how this special problem can be solved far more efficiently when \\(m\\) is much smaller than \\(n\\), using something called the _kernel trick_. Recall that the minimizer of \\(J\\) is given by (see (15.3))\n\\[\\hat{x} = (A^{T}A+\\lambda I)^{-1}(A^{T}b+\\lambda x^{\\rm des})\\] \\[= (A^{T}A+\\lambda I)^{-1}(A^{T}b+(\\lambda I+A^{T}A)x^{\\rm des}-(A^{T }A)x^{\\rm des})\\] \\[= (A^{T}A+\\lambda I)^{-1}A^{T}(b-Ax^{\\rm des})+x^{\\rm des}.\\]\nThe matrix inverse here has size \\(n\\times n\\).\nWe will use the identity\n\\[(A^{T}A+\\lambda I)^{-1}A^{T}=A^{T}(AA^{T}+\\lambda I)^{-1}\\] (15.10)\nwhich holds for any matrix \\(A\\) and any \\(\\lambda>0\\). Note that the left-hand side of the identity involves the inverse of an \\(n\\times n\\) matrix, whereas the right-hand side involves the inverse of a (smaller) \\(m\\times m\\) matrix. (This is a variation on the push-through identity from exercise 11.9.)\nTo show the identity (15.10), we first observe that the matrices \\(A^{T}A+\\lambda I\\) and \\(AA^{T}+\\lambda I\\) are invertible. We start with the equation\n\\[A^{T}(AA^{T}+\\lambda I)=(A^{T}A+\\lambda I)A^{T},\\]\nand multiply each side by \\((A^{T}A+\\lambda I)^{-1}\\) on the left and \\((AA^{T}+\\lambda I)^{-1}\\) on the right, which yields the identity above.\nUsing (15.10) we can express the minimizer of \\(J\\) as\n\\[\\hat{x}=A^{T}(AA^{T}+\\lambda I)^{-1}(b-Ax^{\\rm des})+x^{\\rm des}.\\]\nWe can compute the term \\((AA^{T}+\\lambda I)^{-1}(b-Ax^{\\rm des})\\) by computing the QR factorization of the \\((m+n)\\times m\\) matrix\n\\[\\bar{A}=\\left[\\begin{array}{c}A^{T}\\\\ \\sqrt{\\lambda}I\\end{array}\\right],\\]\n\nDocument 5:\n----\nflops. As in the case of linear quadratic control, the matrices \\(\\tilde{A}\\) and \\(\\tilde{C}\\) are very sparse, and by exploiting this sparsity (see page 349), the large constrained least squares problem can be solved in order \\(T(m+p+n)(m+n)^{2}\\) flops, which grows only linearly in \\(T\\).\nThe least squares state estimation problem was formulated in around 1960 by Rudolf Kalman and others (in a statistical framework). He and others developed a particular recursive algorithm for solving the problem, and the whole method has come to be known as _Kalman filtering_. For this work Kalman was awarded the Kyoto Prize in 1985.\n#### Example\nWe consider a system with \\(n=4\\), \\(p=2\\), and \\(m=2\\), and time-invariant matrices\n\\[A=\\left[\\begin{array}{ccccc}1&0&1&0\\\\ 0&1&0&1\\\\ 0&0&1&0\\\\ 0&0&0&1\\end{array}\\right],\\qquad B=\\left[\\begin{array}{ccccc}0&0\\\\ 0&0\\\\ 1&0\\\\ 0&1\\end{array}\\right],\\qquad C=\\left[\\begin{array}{ccccc}1&0&0&0\\\\ 0&1&0&0\\end{array}\\right].\\]\nThis is a very simple model of motion of a mass moving in 2-D. The first two components of \\(x_{t}\\) represent the position coordinates; components 3 and 4 represent the velocity coordinates. The input \\(w_{t}\\) acts like a force on the mass, since it adds to the velocity. We think of the 2-vector \\(Cx_{t}\\) as the exact or true position of the mass at period \\(t\\). The measurement \\(y_{t}=Cx_{t}+v_{t}\\) is a noisy measurement of the mass position. We will estimate the state trajectory over \\(t=1,\\ldots,T\\), with \\(T=100\\).\n\n\n",
        "answer": [
            1,
            2,
            2
        ],
        "rationale": {
            "Question": "What are the real eigenvalues of the matrix [[3, -2, 5], [1, 0, 7], [0, 0, 2]]?",
            "Answer": [
                1,
                2,
                2
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_4.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "wenhuchen/determinant1.json",
        "question": "For matrix A = [[2, 4, 3], [3, 0, 1], [1, 2, 5]], what is its determinant?",
        "prediction": "-7",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[2, 4, 3], [3, 0, 1], [1, 2, 5]], what is its determinant?\n\nDocument 1:\n----\n* _Entries of matrix triple product_. (See page 10.) Suppose \\(A\\) has dimensions \\(m\\times n\\), \\(B\\) has dimensions \\(n\\times p\\), \\(C\\) has dimensions \\(p\\times q\\), and let \\(D=ABC\\). Show that \\[D_{ij}=\\sum_{k=1}^{n}\\sum_{l=1}^{p}A_{ik}B_{kl}C_{lj}.\\] This is the formula analogous to (10.1) for the product of two matrices.\n* _Multiplication by a diagonal matrix_. Suppose that \\(A\\) is an \\(m\\times n\\) matrix, \\(D\\) is a diagonal matrix, and \\(B=DA\\). Describe \\(B\\) in terms of \\(A\\) and the entries of \\(D\\). You can refer to the rows or columns or entries of \\(A\\).\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n\nDocument 2:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\nDocument 3:\n----\n### 11.1 Left and right inverses\nRecall that for a number \\(a\\), its (multiplicative) inverse is the number \\(x\\) for which \\(xa=1\\), which we usually denote as \\(x=1/a\\) or (less frequently) \\(x=a^{-1}\\). The inverse \\(x\\) exists provided \\(a\\) is nonzero. For matrices the concept of inverse is more complicated than for scalars; in the general case, we need to distinguish between left and right inverses. We start with the left inverse.\nLeft inverse.A matrix \\(X\\) that satisfies\n\\[XA=I\\]\nis called a _left inverse_ of \\(A\\). The matrix \\(A\\) is said to be _left-invertible_ if a left inverse exists. Note that if \\(A\\) has size \\(m\\times n\\), a left inverse \\(X\\) will have size \\(n\\times m\\), the same dimensions as \\(A^{T}\\).\nExamples.* If \\(A\\) is a number (_i.e._, a \\(1\\times 1\\) matrix), then a left inverse \\(X\\) is the same as the inverse of the number. In this case, \\(A\\) is left-invertible whenever \\(A\\) is nonzero, and it has only one left inverse.\n* Any nonzero \\(n\\)-vector \\(a\\), considered as an \\(n\\times 1\\) matrix, is left-invertible. For any index \\(i\\) with \\(a_{i}\\neq 0\\), the row \\(n\\)-vector \\(x=(1/a_{i})e_{i}^{T}\\) satisfies \\(xa=1\\).\n* The matrix\nhas two different left inverses:\n\\[B=\\frac{1}{9}\\left[\\begin{array}{rrr}-11&-10&16\\\\ 7&8&-11\\end{array}\\right],\\qquad C=\\frac{1}{2}\\left[\\begin{array}{rrr}0&-1&6\\\\ 0&1&-4\\end{array}\\right].\\]\nThis can be verified by checking that \\(BA=CA=I\\). The example illustrates that a left-invertible matrix can have more than one left inverse. (In fact, if it has more than one left inverse, then it has infinitely many; see exercise 11.1.)\n\nDocument 4:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\nDocument 5:\n----\nDiagonal matrices.A square \\(n\\times n\\) matrix \\(A\\) is _diagonal_ if \\(A_{ij}=0\\) for \\(i\\neq j\\). (The entries of a matrix with \\(i=j\\) are called the _diagonal entries_; those with \\(i\\neq j\\) are its _off-diagonal_ entries.) A diagonal matrix is one for which all off-diagonal entries are zero. Examples of diagonal matrices we have already seen are square zero matrices and identity matrices. Other examples are\n\\[\\left[\\begin{array}{rrr}-3&0\\\\ 0&0\\end{array}\\right],\\qquad\\left[\\begin{array}{rrr}0.2&0&0\\\\ 0&-3&0\\\\ 0&0&1.2\\end{array}\\right].\\]\n(Note that in the first example, one of the diagonal elements is also zero.)\nThe notation \\(\\mathbf{diag}(a_{1},\\ldots,a_{n})\\) is used to compactly describe the \\(n\\times n\\) diagonal matrix \\(A\\) with diagonal entries \\(A_{11}=a_{1}\\), ..., \\(A_{nn}=a_{n}\\). This notation is not yet standard, but is coming into more prevalent use. As examples, the matrices above would be expressed as\n\\[\\mathbf{diag}(-3,0),\\qquad\\mathbf{diag}(0.2,-3,1.2),\\]\nrespectively. We also allow \\(\\mathbf{diag}\\) to take one \\(n\\)-vector argument, as in \\(I=\\mathbf{diag}(\\mathbf{1})\\).\nTriangular matrices.A square \\(n\\times n\\) matrix \\(A\\) is _upper triangular_ if \\(A_{ij}=0\\) for \\(i>j\\), and it is _lower triangular_ if \\(A_{ij}=0\\) for \\(i<j\\). (So a diagonal matrix is one that is both lower and upper triangular.) If a matrix is either lower or upper triangular, it is called _triangular_. For example, the matrices\n\\[\\left[\\begin{array}{rrr}1&-1&0.7\\\\ 0&1.2&-1.1\\\\ 0&0&3.2\\end{array}\\right],\\qquad\\left[\\begin{array}{rrr}-0.6&0\\\\ -0.3&3.5\\end{array}\\right],\\]\nare upper and lower triangular, respectively.\nA triangular \\(n\\times n\\) matrix \\(A\\) has up to \\(n(n+1)/2\\) nonzero entries, _i.e._, around half its entries are zero. Triangular matrices are generally not considered sparse matrices, since their density is around 50%, but their special sparsity pattern will be important in the sequel.\n\n\n",
        "answer": -42,
        "rationale": {
            "Question": "For matrix A = [[2, 4, 3], [3, 0, 1], [1, 2, 5]], what is its determinant?",
            "Answer": -42,
            "Picture": null,
            "Answer_type": "integer",
            "source": "self",
            "id": "wenhuchen/determinant1.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "wenhuchen/vieta's_formula5.json",
        "question": "The product of two of the four roots of the quartic equation x^4 - 18x^3 +kx2 + 200x - 1984 = 0 is -32. Determine the value of k.",
        "prediction": "16",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nThe product of two of the four roots of the quartic equation x^4 - 18x^3 +kx2 + 200x - 1984 = 0 is -32. Determine the value of k.\n\nDocument 1:\n----\n\\[2 = (1)(2)\\] \\[1 = (1)(1)+(0)(2)\\] \\[-3 = (1)(-1)+(0)(1)+(-1)(2)\\] \\[-1 = (0)(-1)+(-1)(1)\\] \\[1 = (-1)(-1).\\]\nPolynomial multiplication.If \\(a\\) and \\(b\\) represent the coefficients of two polynomials\n\\[p(x)=a_{1}+a_{2}x+\\cdots+a_{n}x^{n-1},\\qquad q(x)=b_{1}+b_{2}x+\\cdots+b_{m}x^{ m-1},\\]\nthen the coefficients of the product polynomial \\(p(x)q(x)\\) are represented by \\(c=a*b\\):\n\\[p(x)q(x)=c_{1}+c_{2}x+\\cdots+c_{n+m-1}x^{n+m-2}.\\]\nTo see this we will show that \\(c_{k}\\) is the coefficient of \\(x^{k-1}\\) in \\(p(x)q(x)\\). We expand the product polynomial into \\(mn\\) terms, and collect those terms associated with \\(x^{k-1}\\). These terms have the form \\(a_{i}b_{j}x^{i+j-2}\\), for \\(i\\) and \\(j\\) that satisfy \\(i+j-2=k-1\\), _i.e._, \\(i+j=k-1\\). It follows that \\(c_{k}=\\sum_{i+j=k+1}a_{i}b_{j}\\), which agrees with the convolution formula (7.2).\nProperties of convolution.Convolution is symmetric: We have \\(a*b=b*a\\). It is also associative: We have \\((a*b)*c=a*(b*c)\\), so we can write both as \\(a*b*c\\). Another property is that \\(a*b=0\\) implies that either \\(a=0\\) or \\(b=0\\). These properties follow from the polynomial coefficient property above, and can also be directly shown. As an example, let us show that \\(a*b=b*a\\). Suppose \\(p\\) is the polynomial with coefficients \\(a\\), and \\(q\\) is the polynomial with coefficients \\(b\\). The two polynomials \\(p(x)q(x)\\) and \\(q(x)p(x)\\) are the same (since multiplication of numbers is commutative), so they have the same coefficients. The coefficients of \\(p(x)q(x)\\) are \\(a*b\\) and the coefficients of \\(q(x)p(x)\\) are \\(b*a\\). These must be the same.\n\nDocument 2:\n----\n1. Show that every vector \\(x\\) can be decomposed in a unique way as a sum \\(x=x_{\\rm s}+x_{\\rm a}\\) of a symmetric vector \\(x_{\\rm s}\\) and an anti-symmetric vector \\(x_{\\rm a}\\).\n2. Show that the symmetric and anti-symmetric parts \\(x_{\\rm s}\\) and \\(x_{\\rm a}\\) are linear functions of \\(x\\). Give matrices \\(A_{\\rm s}\\) and \\(A_{\\rm a}\\) such that \\(x_{\\rm s}=A_{\\rm s}x\\) and \\(x_{\\rm a}=A_{\\rm a}x\\) for all \\(x\\).\n**8.6**: _Linear functions._ For each description of \\(y\\) below, express it as \\(y=Ax\\) for some \\(A\\). (You should specify \\(A\\).)\n1. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average of \\(x_{1},\\ldots,x_{i-1}\\). (We take \\(y_{1}=x_{1}\\).)\n2. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average value of all other \\(x_{j}\\)s, _i.e._, the average of \\(x_{1},\\ldots,x_{i-1},x_{i+1},\\ldots,x_{n}\\).\n**8.7**: _Interpolation of polynomial values and derivatives._ The 5-vector \\(c\\) represents the coefficients of a quartic polynomial \\(p(x)=c_{1}+c_{2}x+c_{3}x^{2}+c_{4}x^{3}+c_{5}x^{4}\\). Express the conditions \\[p(0)=0,\\quad p^{\\prime}(0)=0,\\quad p(1)=1,\\quad p^{\\prime}(1)=0,\\] as a set of linear equations of the form \\(Ac=b\\). Is the system of equations underdetermined, over-determined, or square?\n**8.8**: _Interpolation of rational functions._ A _rational function_ of degree two has the form\n\\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}},\\]\nwhere \\(c_{1},c_{2},c_{3},d_{1},d_{2}\\) are coefficients. ('Rational' refers to the fact that \\(f\\) is a ratio of polynomials. Another name for \\(f\\) is _bi-quadratic_.) Consider the interpolation conditions\n\\[f(t_{i})=y_{i},\\quad i=1,\\ldots,K,\\]\n\nDocument 3:\n----\nFrom step 1 of the algorithm, we see that \\(\\tilde{q}_{i}\\) is a linear combination of the vectors \\(a_{1},q_{1},\\ldots,q_{i-1}\\). By the induction hypothesis, each of \\(q_{1},\\ldots,q_{i-1}\\) is a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), so \\(\\tilde{q}_{i}\\) (and therefore also \\(q_{i}\\)) is a linear combination of \\(a_{1},\\ldots,a_{i}\\). Thus assertions 3 and 4 hold.\nGram-Schmidt completion implies linear independence.From the properties 1-4 above, we can argue that the original collection of vectors \\(a_{1},\\ldots,a_{k}\\) is linearly independent. To see this, suppose that\n\\[\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}=0\\] (5.6)\nholds for some \\(\\beta_{1},\\ldots,\\beta_{k}\\). We will show that \\(\\beta_{1}=\\cdots=\\beta_{k}=0\\).\nWe first note that any linear combination of \\(q_{1},\\ldots,q_{k-1}\\) is orthogonal to any multiple of \\(q_{k}\\), since \\(q_{1}^{T}q_{k}=\\cdots=q_{k-1}^{T}q_{k}=0\\) (by definition). But each of \\(a_{1},\\ldots,a_{k-1}\\) is a linear combination of \\(q_{1},\\ldots,q_{k-1}\\), so we have \\(q_{k}^{T}a_{1}=\\cdots=q_{k}^{T}a_{k-1}=0\\). Taking the inner product of \\(q_{k}\\) with the left- and right-hand sides of (5.6) we obtain\n\\[0 = q_{k}^{T}(\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k})\\] \\[= \\beta_{1}q_{k}^{T}a_{1}+\\cdots+\\beta_{k-1}q_{k}^{T}a_{k-1}+\\beta _{k}q_{k}^{T}a_{k}\\] \\[= \\beta_{k}\\|\\tilde{q}_{k}\\|,\\]\nwhere we use \\(q_{k}^{T}a_{k}=\\|\\tilde{q}_{k}\\|\\) in the last line. We conclude that \\(\\beta_{k}=0\\).\nFrom (5.6) and \\(\\beta_{k}=0\\) we have\n\\[\\beta_{1}a_{1}+\\cdots+\\beta_{k-1}a_{k-1}=0.\\]\nWe now repeat the argument above to conclude that \\(\\beta_{k-1}=0\\). Repeating it \\(k\\) times we conclude that all \\(\\beta_{i}\\) are zero.\nEarly termination.Suppose that the Gram-Schmidt algorithm terminates prematurely, in iteration \\(j\\), because \\(\\tilde{q}_{j}=0\\). The conclusions 1-4 above hold for \\(i=1,\\ldots,j-1\\), since in those steps \\(\\tilde{q}_{i}\\) is nonzero. Since \\(\\tilde{q}_{j}=0\\), we have\n\\[a_{j}=(q_{1}^{T}a_{j})q_{1}+\\cdots+(q_{j-1}^{T}a_{j})q_{j-1},\\]\n\nDocument 4:\n----\n2. What is the meaning of the 15th row of \\(G\\)?\n3. Give a simple formula (using matrices, vectors, etc.) for the \\(n\\)-vector \\(M\\), where \\(M_{i}\\) is the total membership (_i.e._, number of students) in group \\(i\\).\n4. Interpret \\((GG^{T})_{ij}\\) in simple English.\n5. Interpret \\((G^{T}G)_{ij}\\) in simple English.\n**10.20**: _Products, materials, and locations._\\(P\\) different products each require some amounts of \\(M\\) different materials, and are manufactured in \\(L\\) different locations, which have different material costs. We let \\(C_{lm}\\) denote the cost of material \\(m\\) in location \\(l\\), for \\(l=1,\\ldots,L\\) and \\(m=1,\\ldots,M\\). We let \\(Q_{mp}\\) denote the amount of material \\(m\\) required to manufacture one unit of product \\(p\\), for \\(m=1,\\ldots,M\\) and \\(p=1,\\ldots,P\\). Let \\(T_{pl}\\) denote the total cost to manufacture product \\(p\\) in location \\(l\\), for \\(p=1,\\ldots,P\\) and \\(l=1,\\ldots,L\\). Give an expression for the matrix \\(T\\).\n**10.21**: _Integral of product of polynomials._ Let \\(p\\) and \\(q\\) be two quadratic polynomials, given by\n\\[p(x)=c_{1}+c_{2}x+c_{3}x^{2},\\qquad q(x)=d_{1}+d_{2}x+d_{3}x^{2}.\\]\nExpress the integral \\(J=\\int_{0}^{1}p(x)q(x)\\,dx\\) in the form \\(J=c^{T}Gd\\), where \\(G\\) is a \\(3\\times 3\\) matrix. Give the entries of \\(G\\) (as numbers).\n**10.22**: _Composition of linear dynamical systems._ We consider two time-invariant linear dynamical systems with outputs. The first one is given by\n\\[x_{t+1}=Ax_{t}+Bu_{t},\\qquad y_{t}=Cx_{t},\\quad t=1,2,\\ldots,\\]\n\nDocument 5:\n----\nthen the coefficients of the product polynomial \\(p(x)q(x)\\) are represented by \\(c=a*b\\):\n\\[p(x)q(x)=c_{1}+c_{2}x+\\cdots+c_{n+m-1}x^{n+m-2}.\\]\nTo see this we will show that \\(c_{k}\\) is the coefficient of \\(x^{k-1}\\) in \\(p(x)q(x)\\). We expand the product polynomial into \\(mn\\) terms, and collect those terms associated with \\(x^{k-1}\\). These terms have the form \\(a_{i}b_{j}x^{i+j-2}\\), for \\(i\\) and \\(j\\) that satisfy \\(i+j-2=k-1\\), _i.e._, \\(i+j=k-1\\). It follows that \\(c_{k}=\\sum_{i+j=k+1}a_{i}b_{j}\\), which agrees with the convolution formula (7.2).\nProperties of convolution.Convolution is symmetric: We have \\(a*b=b*a\\). It is also associative: We have \\((a*b)*c=a*(b*c)\\), so we can write both as \\(a*b*c\\). Another property is that \\(a*b=0\\) implies that either \\(a=0\\) or \\(b=0\\). These properties follow from the polynomial coefficient property above, and can also be directly shown. As an example, let us show that \\(a*b=b*a\\). Suppose \\(p\\) is the polynomial with coefficients \\(a\\), and \\(q\\) is the polynomial with coefficients \\(b\\). The two polynomials \\(p(x)q(x)\\) and \\(q(x)p(x)\\) are the same (since multiplication of numbers is commutative), so they have the same coefficients. The coefficients of \\(p(x)q(x)\\) are \\(a*b\\) and the coefficients of \\(q(x)p(x)\\) are \\(b*a\\). These must be the same.\nA basic property is that for fixed \\(a\\), the convolution \\(a*b\\) is a linear function of \\(b\\); and for fixed \\(b\\), it is a linear function of \\(a\\). This means we can express \\(a*b\\) as a matrix-vector product:\n\n\n",
        "answer": 86,
        "rationale": {
            "Question": "The product of two of the four roots of the quartic equation x^4 - 18x^3 +kx2 + 200x - 1984 = 0 is -32. Determine the value of k.",
            "Answer": 86,
            "Answer_type": "integer",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula5.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "elainewan/math_algebra_4.json",
        "question": "Is the set of 3 * 3 matrices in reduced row-echelon form a subspace of R^{3 * 3}?",
        "prediction": "False",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs the set of 3 * 3 matrices in reduced row-echelon form a subspace of R^{3 * 3}?\n\nDocument 1:\n----\nWe have now reduced the problem of minimizing the weighted sum least squares objective to a standard least squares problem. Provided the columns of \\(\\tilde{A}\\) are linearly independent, the minimizer is unique, and given by\n\\[\\hat{x} = (\\tilde{A}^{T}\\tilde{A})^{-1}\\tilde{A}^{T}\\tilde{b}\\] (15.3) \\[= (\\lambda_{1}A_{1}^{T}A_{1}+\\cdots+\\lambda_{k}A_{k}^{T}A_{k})^{-1 }(\\lambda_{1}A_{1}^{T}b_{1}+\\cdots+\\lambda_{k}A_{k}^{T}b_{k}).\\]\nThis reduces to our standard formula for the solution of a least squares problem when \\(k=1\\) and \\(\\lambda_{1}=1\\). (In fact, when \\(k=1\\), \\(\\lambda_{1}\\) does not matter.) We can compute \\(\\hat{x}\\) via the QR factorization of \\(\\tilde{A}\\).\nIndependent columns of stacked matrix.Our assumption (12.2) that the columns of \\(\\tilde{A}\\) in (15.2) are linearly independent is not the same as assuming that each of \\(A_{1},\\ldots,A_{k}\\) has linearly independent columns. We can state the condition that \\(\\tilde{A}\\) has linearly independent columns as: There is no nonzero vector \\(x\\) that satisfies\n\\(A_{i}x=0\\) for \\(i=1,\\ldots,k\\). This implies that if just _one_ of the matrices \\(A_{1},\\ldots,A_{k}\\) has linearly independent columns, then \\(\\tilde{A}\\) does.\nThe stacked matrix \\(\\tilde{A}\\) can have linearly independent columns even when none of the matrices \\(A_{1},\\ldots,A_{k}\\) do. This can happen when \\(m_{i}<n\\) for all \\(i\\), _i.e._, all \\(A_{i}\\) are wide. However, we must have \\(m_{1}+\\cdots+m_{k}\\geq n\\), since \\(\\tilde{A}\\) must be tall or square for the linearly independent columns assumption to hold.\nOptimal trade-off curve.We start with the special case of two objectives (also called the _bi-criterion problem_), and write the weighted sum objective as\n\\[J=J_{1}+\\lambda J_{2}=\\|A_{1}x-b_{1}\\|^{2}+\\lambda\\|A_{2}x-b_{2}\\|^{2},\\]\n\nDocument 2:\n----\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\nInvertibility conditions.For square matrices, left-invertibility, right-invertibility, and invertibility are equivalent: If a matrix is square and left-invertible, then it is also right-invertible (and therefore invertible) and vice-versa.\nTo see this, suppose \\(A\\) is an \\(n\\times n\\) matrix and left-invertible. This implies that the \\(n\\) columns of \\(A\\) are linearly independent. Therefore they form a basis and so any \\(n\\)-vector can be expressed as a linear combination of the columns of \\(A\\). In particular, each of the \\(n\\) unit vectors \\(e_{i}\\) can be expressed as \\(e_{i}=Ab_{i}\\) for some \\(n\\)-vector \\(b_{i}\\). The matrix \\(B=\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\) satisfies\n\\[AB=\\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right]= \\left[\\begin{array}{cccc}e_{1}&e_{2}&\\cdots&e_{n}\\end{array}\\right]=I.\\]\nSo \\(B\\) is a right inverse of \\(A\\).\nWe have just shown that for a square matrix \\(A\\),\n\nDocument 3:\n----\nTable 6.1: Daily returns of Apple (AAPL), Google (GOOG), 3M (MMM), and Amazon (AMZN), on March 1, 2, and 3, 2016 (based on closing prices).\nof the data matrix \\(X\\) is an \\(N\\)-row-vector whose entries are the values of the \\(i\\)th feature across the examples. We can also directly interpret the entries of the data matrix: \\(X_{ij}\\) (which is a number) is the value of the \\(i\\)th feature for the \\(j\\)th example.\nAs another example, a \\(3\\times M\\) matrix can be used to represent a collection of \\(M\\) locations or positions in 3-D space, with its \\(j\\)th column giving the \\(j\\)th position.\nMatrix representation of a relation or graph.Suppose we have \\(n\\) objects labeled \\(1,\\ldots,n\\). A _relation_\\(\\mathcal{R}\\) on the set of objects \\(\\{1,\\ldots,n\\}\\) is a subset of ordered pairs of objects. As an example, \\(\\mathcal{R}\\) can represent a _preference relation_ among \\(n\\) possible products or choices, with \\((i,j)\\in\\mathcal{R}\\) meaning that choice \\(i\\) is preferred to choice \\(j\\).\nA relation can also be viewed as a _directed graph_, with nodes (or vertices) labeled \\(1,\\ldots,n\\), and a directed edge from \\(j\\) to \\(i\\) for each \\((i,j)\\in\\mathcal{R}\\). This is typically drawn as a graph, with arrows indicating the direction of the edge, as shown in figure 6.1, for the relation on 4 objects\n\nDocument 4:\n----\nNegative matrix powers.We can now give a meaning to matrix powers with negative integer exponents. Suppose \\(A\\) is a square invertible matrix and \\(k\\) is a positive integer. Then by repeatedly applying property (11.2), we get\n\\[(A^{k})^{-1}=(A^{-1})^{k}.\\]\nWe denote this matrix as \\(A^{-k}\\). For example, if \\(A\\) is square and invertible, then \\(A^{-2}=A^{-1}A^{-1}=(AA)^{-1}\\). With \\(A^{0}\\) defined as \\(A^{0}=I\\), the identity \\(A^{k+l}=A^{k}A^{l}\\) holds for all integers \\(k\\) and \\(l\\).\nTriangular matrix.A triangular matrix with nonzero diagonal elements is invertible. We first discuss this for a lower triangular matrix. Let \\(L\\) be \\(n\\times n\\) and lower triangular with nonzero diagonal elements. We show that the columns are linearly independent, _i.e._, \\(Lx=0\\) is only possible if \\(x=0\\). Expanding the matrix-vector product, we can write \\(Lx=0\\) as\n\\[L_{11}x_{1} = 0\\] \\[L_{21}x_{1}+L_{22}x_{2} = 0\\] \\[L_{31}x_{1}+L_{32}x_{2}+L_{33}x_{3} = 0\\] \\[\\vdots\\] \\[L_{n1}x_{1}+L_{n2}x_{2}+\\cdots+L_{n,n-1}x_{n-1}+L_{nn}x_{n} = 0.\\]\nSince \\(L_{11}\\neq 0\\), the first equation implies \\(x_{1}=0\\). Using \\(x_{1}=0\\), the second equation reduces to \\(L_{22}x_{2}=0\\). Since \\(L_{22}\\neq 0\\), we conclude that \\(x_{2}=0\\). Using \\(x_{1}=x_{2}=0\\), the third equation now reduces to \\(L_{33}x_{3}=0\\), and since \\(L_{33}\\) is assumed to be nonzero, we have \\(x_{3}=0\\). Continuing this argument, we find that all entries of \\(x\\) are zero, and this shows that the columns of \\(L\\) are linearly independent. It follows that \\(L\\) is invertible.\n\nDocument 5:\n----\nMultiple sets of linear equations.We can use the column interpretation of matrix multiplication to express a set of \\(k\\) linear equations with the same \\(m\\times n\\) coefficient matrix \\(A\\),\n\\[Ax_{i}=b_{i},\\quad i=1,\\ldots,k,\\]\nin the compact form\n\\[AX=B,\\]\nwhere \\(X=[x_{1}\\;\\cdots\\;x_{k}]\\) and \\(B=[b_{1}\\;\\cdots\\;b_{k}]\\). The matrix equation \\(AX=B\\) is sometimes called a _linear equation with matrix right-hand side_, since it looks like \\(Ax=b\\), but \\(X\\) (the variable) and \\(B\\) (the right-hand side) are now \\(n\\times k\\) matrices, instead of \\(n\\)-vectors (which are \\(n\\times 1\\) matrices).\nRow interpretation of matrix-matrix product.We can give an analogous row interpretation of the product \\(AB\\), by partitioning \\(A\\) and \\(AB\\) as block matrices with row vector blocks. Let \\(a_{1}^{T},\\ldots,a_{m}^{T}\\) be the rows of \\(A\\). Then we have\n\\[AB=\\left[\\begin{array}{c}a_{1}^{T}\\\\ a_{2}^{T}\\\\ \\vdots\\\\ a_{m}^{T}\\end{array}\\right]B=\\left[\\begin{array}{c}a_{1}^{T}B\\\\ a_{2}^{T}B\\\\ \\vdots\\\\ a_{m}^{T}B\\end{array}\\right]=\\left[\\begin{array}{c}(B^{T}a_{1})^{T}\\\\ (B^{T}a_{2})^{T}\\\\ \\vdots\\\\ (B^{T}a_{m})^{T}\\end{array}\\right].\\]\nThis shows that the rows of \\(AB\\) are obtained by applying \\(B^{T}\\) to the transposed row vectors \\(a_{k}\\) of \\(A\\), and transposing the result.\nInner product representation.From the definition of the \\(i,j\\) element of \\(AB\\) in (10.1), we also see that the elements of \\(AB\\) are the inner products of the rows of \\(A\\) with the columns of \\(B\\):\n\\[AB=\\left[\\begin{array}{cccc}a_{1}^{T}b_{1}&a_{1}^{T}b_{2}&\\cdots&a_{1}^{T}b_{n }\\\\ a_{2}^{T}b_{1}&a_{2}^{T}b_{2}&\\cdots&a_{2}^{T}b_{n}\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ a_{m}^{T}b_{1}&a_{m}^{T}b_{2}&\\cdots&a_{m}^{T}b_{n}\\end{array}\\right],\\]\nwhere \\(a_{i}^{T}\\) are the rows of \\(A\\) and \\(b_{j}\\) are the columns of \\(B\\). Thus we can interpret the matrix-matrix product as the \\(mn\\) inner products \\(a_{i}^{T}b_{j}\\) arranged in an \\(m\\times n\\) matrix.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is the set of 3 * 3 matrices in reduced row-echelon form a subspace of R^{3 * 3}?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_4.json",
            "explanation": "NONE",
            "theorem": "linear subspaces",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "elainewan/math_algebra_3_5.json",
        "question": "Consider an m * n matrix A and an n * m matrix B (with n != m) such that AB = I_m. Are the columns of A linearly independent?",
        "prediction": "True",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider an m * n matrix A and an n * m matrix B (with n != m) such that AB = I_m. Are the columns of A linearly independent?\n\nDocument 1:\n----\nto simplify (16.7). This factorization exists because the stacked matrix has linearly independent columns, by our assumption (16.5). In (16.8) we also partition \\(Q\\) in two blocks \\(Q_{1}\\) and \\(Q_{2}\\), of size \\(m\\times n\\) and \\(p\\times n\\), respectively. If we make the substitutions \\(A=Q_{1}R\\), \\(C=Q_{2}R\\), and \\(A^{T}A+C^{T}C=R^{T}R\\) in (16.7) we obtain\n\\[2R^{T}R\\hat{x}+R^{T}Q_{2}^{T}w=2R^{T}Q_{1}^{T}b,\\qquad Q_{2}R\\hat{x}=d.\\]\nWe multiply the first equation on the left by \\(R^{-T}\\) (which we know exists) to get\n\\[R\\hat{x}=Q_{1}^{T}b-(1/2)Q_{2}^{T}w.\\] (16.9)\nSubstituting this expression into \\(Q_{2}R\\hat{x}=d\\) gives an equation in \\(w\\):\n\\[Q_{2}Q_{2}^{T}w=2Q_{2}Q_{1}^{T}b-2d.\\] (16.10)\nWe now use the second part of the assumption (16.5) to show that the matrix \\(Q_{2}^{T}=R^{-T}C^{T}\\) has linearly independent columns. Suppose \\(Q_{2}^{T}z=R^{-T}C^{T}z=0\\). Multiplying with \\(R^{T}\\) gives \\(C^{T}z=0\\). Since \\(C\\) has linearly independent rows, this implies \\(z=0\\), and we conclude that the columns of \\(Q_{2}^{T}\\) are linearly independent.\nThe matrix \\(Q_{2}^{T}\\) therefore has a QR factorization \\(Q_{2}^{T}=\\tilde{Q}\\tilde{R}\\). Substituting this into (16.10) gives\n\\[\\tilde{R}^{T}\\tilde{R}w=2\\tilde{R}^{T}\\tilde{Q}^{T}Q_{1}^{T}b-2d,\\]\nwhich we can write as\n\\[\\tilde{R}w=2\\tilde{Q}^{T}Q_{1}^{T}b-2\\tilde{R}^{-T}d.\\]\nWe can use this to compute \\(w\\), first by computing \\(\\tilde{R}^{-T}d\\) (by forward substitution), then forming the right-hand side, and then solving for \\(w\\) using back substitution. Once we know \\(w\\), we can find \\(\\hat{x}\\) from (16.9). The method is summarized in the following algorithm.\n**Algorithm 16.2** Constrained least squares via QR factorization\n\nDocument 2:\n----\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\nInvertibility conditions.For square matrices, left-invertibility, right-invertibility, and invertibility are equivalent: If a matrix is square and left-invertible, then it is also right-invertible (and therefore invertible) and vice-versa.\nTo see this, suppose \\(A\\) is an \\(n\\times n\\) matrix and left-invertible. This implies that the \\(n\\) columns of \\(A\\) are linearly independent. Therefore they form a basis and so any \\(n\\)-vector can be expressed as a linear combination of the columns of \\(A\\). In particular, each of the \\(n\\) unit vectors \\(e_{i}\\) can be expressed as \\(e_{i}=Ab_{i}\\) for some \\(n\\)-vector \\(b_{i}\\). The matrix \\(B=\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\) satisfies\n\\[AB=\\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right]= \\left[\\begin{array}{cccc}e_{1}&e_{2}&\\cdots&e_{n}\\end{array}\\right]=I.\\]\nSo \\(B\\) is a right inverse of \\(A\\).\nWe have just shown that for a square matrix \\(A\\),\n\nDocument 3:\n----\nThis reduces to our standard formula for the solution of a least squares problem when \\(k=1\\) and \\(\\lambda_{1}=1\\). (In fact, when \\(k=1\\), \\(\\lambda_{1}\\) does not matter.) We can compute \\(\\hat{x}\\) via the QR factorization of \\(\\tilde{A}\\).\nIndependent columns of stacked matrix.Our assumption (12.2) that the columns of \\(\\tilde{A}\\) in (15.2) are linearly independent is not the same as assuming that each of \\(A_{1},\\ldots,A_{k}\\) has linearly independent columns. We can state the condition that \\(\\tilde{A}\\) has linearly independent columns as: There is no nonzero vector \\(x\\) that satisfies\n\\(A_{i}x=0\\) for \\(i=1,\\ldots,k\\). This implies that if just _one_ of the matrices \\(A_{1},\\ldots,A_{k}\\) has linearly independent columns, then \\(\\tilde{A}\\) does.\nThe stacked matrix \\(\\tilde{A}\\) can have linearly independent columns even when none of the matrices \\(A_{1},\\ldots,A_{k}\\) do. This can happen when \\(m_{i}<n\\) for all \\(i\\), _i.e._, all \\(A_{i}\\) are wide. However, we must have \\(m_{1}+\\cdots+m_{k}\\geq n\\), since \\(\\tilde{A}\\) must be tall or square for the linearly independent columns assumption to hold.\nOptimal trade-off curve.We start with the special case of two objectives (also called the _bi-criterion problem_), and write the weighted sum objective as\n\\[J=J_{1}+\\lambda J_{2}=\\|A_{1}x-b_{1}\\|^{2}+\\lambda\\|A_{2}x-b_{2}\\|^{2},\\]\nwhere \\(\\lambda>0\\) is the relative weight put on the second objective, compared to the first. For small \\(\\lambda\\), we care much more about \\(J_{1}\\) being small than \\(J_{2}\\) being small; for large \\(\\lambda\\), we care much less about \\(J_{1}\\) being small than \\(J_{2}\\) being small.\n\nDocument 4:\n----\nWe have now reduced the problem of minimizing the weighted sum least squares objective to a standard least squares problem. Provided the columns of \\(\\tilde{A}\\) are linearly independent, the minimizer is unique, and given by\n\\[\\hat{x} = (\\tilde{A}^{T}\\tilde{A})^{-1}\\tilde{A}^{T}\\tilde{b}\\] (15.3) \\[= (\\lambda_{1}A_{1}^{T}A_{1}+\\cdots+\\lambda_{k}A_{k}^{T}A_{k})^{-1 }(\\lambda_{1}A_{1}^{T}b_{1}+\\cdots+\\lambda_{k}A_{k}^{T}b_{k}).\\]\nThis reduces to our standard formula for the solution of a least squares problem when \\(k=1\\) and \\(\\lambda_{1}=1\\). (In fact, when \\(k=1\\), \\(\\lambda_{1}\\) does not matter.) We can compute \\(\\hat{x}\\) via the QR factorization of \\(\\tilde{A}\\).\nIndependent columns of stacked matrix.Our assumption (12.2) that the columns of \\(\\tilde{A}\\) in (15.2) are linearly independent is not the same as assuming that each of \\(A_{1},\\ldots,A_{k}\\) has linearly independent columns. We can state the condition that \\(\\tilde{A}\\) has linearly independent columns as: There is no nonzero vector \\(x\\) that satisfies\n\\(A_{i}x=0\\) for \\(i=1,\\ldots,k\\). This implies that if just _one_ of the matrices \\(A_{1},\\ldots,A_{k}\\) has linearly independent columns, then \\(\\tilde{A}\\) does.\nThe stacked matrix \\(\\tilde{A}\\) can have linearly independent columns even when none of the matrices \\(A_{1},\\ldots,A_{k}\\) do. This can happen when \\(m_{i}<n\\) for all \\(i\\), _i.e._, all \\(A_{i}\\) are wide. However, we must have \\(m_{1}+\\cdots+m_{k}\\geq n\\), since \\(\\tilde{A}\\) must be tall or square for the linearly independent columns assumption to hold.\nOptimal trade-off curve.We start with the special case of two objectives (also called the _bi-criterion problem_), and write the weighted sum objective as\n\\[J=J_{1}+\\lambda J_{2}=\\|A_{1}x-b_{1}\\|^{2}+\\lambda\\|A_{2}x-b_{2}\\|^{2},\\]\n\nDocument 5:\n----\n3. \\(A=FB\\) for some lower triangular matrix \\(F\\).\n4. \\(A=BJ\\) for some lower triangular matrix \\(J\\).\n**10.34**: Choose one of the responses _always_, _never_, or _sometimes_ for each of the statements below. 'Always' means the statement is always true, 'never' means it is never true, and 'Sometimes' means it can be true or false, depending on the particular values of the matrix or matrices. Give a brief justification of each answer.\n1. An upper triangular matrix has linearly independent columns.\n2. The rows of a tall matrix are linearly dependent.\n3. The columns of \\(A\\) are linearly independent, and \\(AB=0\\) for some nonzero matrix \\(B\\).\n**10.35**: _Orthogonal matrices._ Let \\(U\\) and \\(V\\) be two orthogonal \\(n\\times n\\) matrices. Show that the matrix \\(UV\\) and the \\((2n)\\times(2n)\\) matrix\n\\[\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}U&U\\\\ V&-V\\end{array}\\right]\\]\nare orthogonal.\n**10.36**: _Quadratic form._ Suppose \\(A\\) is an \\(n\\times n\\) matrix and \\(x\\) is an \\(n\\)-vector. The triple product \\(x^{T}Ax\\), a \\(1\\times 1\\) matrix which we consider to be a scalar (_i.e._, number), is called a _quadratic form_ of the vector \\(x\\), with coefficient matrix \\(A\\). A quadratic form is the vector analog of a quadratic function \\(\\alpha u^{2}\\), where \\(\\alpha\\) and \\(u\\) are both numbers. Quadratic forms arise in many fields and applications.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Consider an m * n matrix A and an n * m matrix B (with n != m) such that AB = I_m. Are the columns of A linearly independent?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_5.json",
            "explanation": "NONE",
            "theorem": "linear independence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool"
    },
    {
        "id": "mingyin/linear-dependence1.json",
        "question": "Finding all the real roots of the equation $\\sqrt{x^2+x+1}+\\sqrt{2 x^2+x+5}=\\sqrt{x^2-3 x+13}$. Return the answer as a list with ascending order.",
        "prediction": "[1, 3]",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFinding all the real roots of the equation $\\sqrt{x^2+x+1}+\\sqrt{2 x^2+x+5}=\\sqrt{x^2-3 x+13}$. Return the answer as a list with ascending order.\n\nDocument 1:\n----\n**11.10**: _Reverse-time linear dynamical system._ A linear dynamical system has the form \\[x_{t+1}=Ax_{t},\\] where \\(x_{t}\\) in the (\\(n\\)-vector) state in period \\(t\\), and \\(A\\) is the \\(n\\times n\\) dynamics matrix. This formula gives the state in the next period as a function of the current state. We want to derive a recursion of the form \\[x_{t-1}=A^{\\rm rev}x_{t},\\] which gives the previous state as a function of the current state. We call this the _reverse time linear dynamical system_. 1. When is this possible? When it is possible, what is \\(A^{\\rm rev}\\)? 2. For the specific linear dynamical system with dynamics matrix \\[A=\\left[\\begin{array}{cc}3&2\\\\ -1&4\\end{array}\\right],\\] find \\(A^{\\rm rev}\\), or explain why the reverse time linear dynamical system doesn't exist.\n**11.11**: _Interpolation of rational functions._ (Continuation of exercise 8.8.) Find a rational function \\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}}\\] that satisfies the following interpolation conditions: \\[f(1)=2,\\qquad f(2)=5,\\qquad f(3)=9,\\qquad f(4)=-1,\\qquad f(5)=-4.\\] In exercise 8.8 these conditions were expressed as a set of linear equations in the coefficients \\(c_{1}\\), \\(c_{2}\\), \\(c_{3}\\), \\(d_{1}\\) and \\(d_{2}\\); here we are asking you to form and (numerically) solve the system of equations. Plot the rational function you find over the range \\(x=0\\) to \\(x=6\\). Your plot should include markers at the interpolation points \\((1,2),\\ldots,(5,-4)\\). (Your rational function graph should pass through these points.)\n\nDocument 2:\n----\n**10.36**: _Quadratic form._ Suppose \\(A\\) is an \\(n\\times n\\) matrix and \\(x\\) is an \\(n\\)-vector. The triple product \\(x^{T}Ax\\), a \\(1\\times 1\\) matrix which we consider to be a scalar (_i.e._, number), is called a _quadratic form_ of the vector \\(x\\), with coefficient matrix \\(A\\). A quadratic form is the vector analog of a quadratic function \\(\\alpha u^{2}\\), where \\(\\alpha\\) and \\(u\\) are both numbers. Quadratic forms arise in many fields and applications.\n1. Show that \\(x^{T}Ax=\\sum_{i,j=1}^{n}A_{ij}x_{i}x_{j}\\).\n2. Show that \\(x^{T}(A^{T})x=x^{T}Ax\\). In other words, the quadratic form with the transposed coefficient matrix has the same value for any \\(x\\). _Hint._ Take the transpose of the triple product \\(x^{T}Ax\\).\n3. Show that \\(x^{T}((A+A^{T})/2)x=x^{T}Ax\\). In other words, the quadratic form with coefficient matrix equal to the symmetric part of a matrix (_i.e._, \\((A+A^{T})/2\\)) has the same value as the original quadratic form.\n4. Express \\(2x_{1}^{2}-3x_{1}x_{2}-x_{2}^{2}\\) as a quadratic form, with symmetric coefficient matrix \\(A\\).\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n\nDocument 3:\n----\n1. Show that every vector \\(x\\) can be decomposed in a unique way as a sum \\(x=x_{\\rm s}+x_{\\rm a}\\) of a symmetric vector \\(x_{\\rm s}\\) and an anti-symmetric vector \\(x_{\\rm a}\\).\n2. Show that the symmetric and anti-symmetric parts \\(x_{\\rm s}\\) and \\(x_{\\rm a}\\) are linear functions of \\(x\\). Give matrices \\(A_{\\rm s}\\) and \\(A_{\\rm a}\\) such that \\(x_{\\rm s}=A_{\\rm s}x\\) and \\(x_{\\rm a}=A_{\\rm a}x\\) for all \\(x\\).\n**8.6**: _Linear functions._ For each description of \\(y\\) below, express it as \\(y=Ax\\) for some \\(A\\). (You should specify \\(A\\).)\n1. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average of \\(x_{1},\\ldots,x_{i-1}\\). (We take \\(y_{1}=x_{1}\\).)\n2. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average value of all other \\(x_{j}\\)s, _i.e._, the average of \\(x_{1},\\ldots,x_{i-1},x_{i+1},\\ldots,x_{n}\\).\n**8.7**: _Interpolation of polynomial values and derivatives._ The 5-vector \\(c\\) represents the coefficients of a quartic polynomial \\(p(x)=c_{1}+c_{2}x+c_{3}x^{2}+c_{4}x^{3}+c_{5}x^{4}\\). Express the conditions \\[p(0)=0,\\quad p^{\\prime}(0)=0,\\quad p(1)=1,\\quad p^{\\prime}(1)=0,\\] as a set of linear equations of the form \\(Ac=b\\). Is the system of equations underdetermined, over-determined, or square?\n**8.8**: _Interpolation of rational functions._ A _rational function_ of degree two has the form\n\\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}},\\]\nwhere \\(c_{1},c_{2},c_{3},d_{1},d_{2}\\) are coefficients. ('Rational' refers to the fact that \\(f\\) is a ratio of polynomials. Another name for \\(f\\) is _bi-quadratic_.) Consider the interpolation conditions\n\\[f(t_{i})=y_{i},\\quad i=1,\\ldots,K,\\]\n\nDocument 4:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\nDocument 5:\n----\nIn summary, a right inverse can be used to find \\(a\\) solution of a square or under-determined set of linear equations, for any vector \\(b\\).\nExamples.Consider the matrix appearing in the example above on page 199,\n\\[A=\\left[\\begin{array}{rr}-3&-4\\\\ 4&6\\\\ 1&1\\end{array}\\right]\\]\nand the two left inverses\n\\[B=\\frac{1}{9}\\left[\\begin{array}{rr}-11&-10&16\\\\ 7&8&-11\\end{array}\\right],\\qquad C=\\frac{1}{2}\\left[\\begin{array}{rr}0&-1&6 \\\\ 0&1&-4\\end{array}\\right].\\]\n* The over-determined linear equations \\(Ax=(1,-2,0)\\) have the unique solution \\(x=(1,-1)\\), which can be obtained from _either_ left inverse: \\[x=B(1,-2,0)=C(1,-2,0).\\]\n* The over-determined linear equations \\(Ax=(1,-1,0)\\) do not have a solution, since \\(x=C(1,-1,0)=(1/2,-1/2)\\) does not satisfy \\(Ax=(1,-1,0)\\).\n* The under-determined linear equations \\(A^{T}y=(1,2)\\) has (different) solutions \\[B^{T}(1,2)=(1/3,2/3,38/9),\\qquad C^{T}(1,2)=(0,1/2,-1).\\] (Recall that \\(B^{T}\\) and \\(C^{T}\\) are both right inverses of \\(A^{T}\\).) We can find a solution of \\(A^{T}y=b\\) for any vector \\(b\\).\nLeft and right inverse of matrix product.Suppose \\(A\\) and \\(D\\) are compatible for the matrix product \\(AD\\) (_i.e._, the number of columns in \\(A\\) is equal to the number of rows in \\(D\\).) If \\(A\\) has a right inverse \\(B\\) and \\(D\\) has a right inverse \\(E\\), then \\(EB\\) is a right inverse of \\(AD\\). This follows from \\[(AD)(EB)=A(DE)B=A(IB)=AB=I.\\] If \\(A\\) has a left inverse \\(C\\) and \\(D\\) has a left inverse \\(F\\), then \\(FC\\) is a left inverse of \\(AD\\). This follows from \\[(FC)(AD)=F(CA)D=FD=I.\\]\n\n\n",
        "answer": [
            -1.7807764064,
            0.2807764064
        ],
        "rationale": {
            "Question": "Finding all the real roots of the equation $\\sqrt{x^2+x+1}+\\sqrt{2 x^2+x+5}=\\sqrt{x^2-3 x+13}$. Return the answer as a list with ascending order.",
            "Answer": [
                -1.7807764064,
                0.2807764064
            ],
            "Picture": null,
            "Answer_type": "list of float",
            "source": "linear algebra 2.8 example 2",
            "id": "mingyin/linear-dependence1.json",
            "explanation": "NONE",
            "theorem": "linear dependence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of float"
    },
    {
        "id": "wenhuchen/vieta's_formula4.json",
        "question": "If the quartic x^4 + 3x^3 + 11x^2 + 9x + A has roots k, l, m, and n such that kl = mn, find A.",
        "prediction": "\\( A = klmn \\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIf the quartic x^4 + 3x^3 + 11x^2 + 9x + A has roots k, l, m, and n such that kl = mn, find A.\n\nDocument 1:\n----\nthen the coefficients of the product polynomial \\(p(x)q(x)\\) are represented by \\(c=a*b\\):\n\\[p(x)q(x)=c_{1}+c_{2}x+\\cdots+c_{n+m-1}x^{n+m-2}.\\]\nTo see this we will show that \\(c_{k}\\) is the coefficient of \\(x^{k-1}\\) in \\(p(x)q(x)\\). We expand the product polynomial into \\(mn\\) terms, and collect those terms associated with \\(x^{k-1}\\). These terms have the form \\(a_{i}b_{j}x^{i+j-2}\\), for \\(i\\) and \\(j\\) that satisfy \\(i+j-2=k-1\\), _i.e._, \\(i+j=k-1\\). It follows that \\(c_{k}=\\sum_{i+j=k+1}a_{i}b_{j}\\), which agrees with the convolution formula (7.2).\nProperties of convolution.Convolution is symmetric: We have \\(a*b=b*a\\). It is also associative: We have \\((a*b)*c=a*(b*c)\\), so we can write both as \\(a*b*c\\). Another property is that \\(a*b=0\\) implies that either \\(a=0\\) or \\(b=0\\). These properties follow from the polynomial coefficient property above, and can also be directly shown. As an example, let us show that \\(a*b=b*a\\). Suppose \\(p\\) is the polynomial with coefficients \\(a\\), and \\(q\\) is the polynomial with coefficients \\(b\\). The two polynomials \\(p(x)q(x)\\) and \\(q(x)p(x)\\) are the same (since multiplication of numbers is commutative), so they have the same coefficients. The coefficients of \\(p(x)q(x)\\) are \\(a*b\\) and the coefficients of \\(q(x)p(x)\\) are \\(b*a\\). These must be the same.\nA basic property is that for fixed \\(a\\), the convolution \\(a*b\\) is a linear function of \\(b\\); and for fixed \\(b\\), it is a linear function of \\(a\\). This means we can express \\(a*b\\) as a matrix-vector product:\n\nDocument 2:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\nDocument 3:\n----\n4. Express \\(2x_{1}^{2}-3x_{1}x_{2}-x_{2}^{2}\\) as a quadratic form, with symmetric coefficient matrix \\(A\\).\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n**10.38**: _Orthogonal matrix with nonnegative entries._ Suppose the \\(n\\times n\\) matrix \\(A\\) is orthogonal, and all of its entries are nonnegative, _i.e._, \\(A_{ij}\\geq 0\\) for \\(i,j=1,\\ldots,n\\). Show that \\(A\\) must be a permutation matrix, _i.e._, each entry is either \\(0\\) or \\(1\\), each row has exactly one entry with value one, and each column has exactly one entry with value one. (See page 132.)\n**10.39**: _Gram matrix and QR factorization._ Suppose the matrix \\(A\\) has linearly independent columns and QR factorization \\(A=QR\\). What is the relationship between the Gram matrix of \\(A\\) and the Gram matrix of \\(R\\)? What can you say about the angles between the columns of \\(A\\) and the angles between the columns of \\(R\\)?\n\nDocument 4:\n----\n1. Show that every vector \\(x\\) can be decomposed in a unique way as a sum \\(x=x_{\\rm s}+x_{\\rm a}\\) of a symmetric vector \\(x_{\\rm s}\\) and an anti-symmetric vector \\(x_{\\rm a}\\).\n2. Show that the symmetric and anti-symmetric parts \\(x_{\\rm s}\\) and \\(x_{\\rm a}\\) are linear functions of \\(x\\). Give matrices \\(A_{\\rm s}\\) and \\(A_{\\rm a}\\) such that \\(x_{\\rm s}=A_{\\rm s}x\\) and \\(x_{\\rm a}=A_{\\rm a}x\\) for all \\(x\\).\n**8.6**: _Linear functions._ For each description of \\(y\\) below, express it as \\(y=Ax\\) for some \\(A\\). (You should specify \\(A\\).)\n1. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average of \\(x_{1},\\ldots,x_{i-1}\\). (We take \\(y_{1}=x_{1}\\).)\n2. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average value of all other \\(x_{j}\\)s, _i.e._, the average of \\(x_{1},\\ldots,x_{i-1},x_{i+1},\\ldots,x_{n}\\).\n**8.7**: _Interpolation of polynomial values and derivatives._ The 5-vector \\(c\\) represents the coefficients of a quartic polynomial \\(p(x)=c_{1}+c_{2}x+c_{3}x^{2}+c_{4}x^{3}+c_{5}x^{4}\\). Express the conditions \\[p(0)=0,\\quad p^{\\prime}(0)=0,\\quad p(1)=1,\\quad p^{\\prime}(1)=0,\\] as a set of linear equations of the form \\(Ac=b\\). Is the system of equations underdetermined, over-determined, or square?\n**8.8**: _Interpolation of rational functions._ A _rational function_ of degree two has the form\n\\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}},\\]\nwhere \\(c_{1},c_{2},c_{3},d_{1},d_{2}\\) are coefficients. ('Rational' refers to the fact that \\(f\\) is a ratio of polynomials. Another name for \\(f\\) is _bi-quadratic_.) Consider the interpolation conditions\n\\[f(t_{i})=y_{i},\\quad i=1,\\ldots,K,\\]\n\nDocument 5:\n----\n### 10.3 Matrix power\nIt makes sense to multiply a square matrix \\(A\\) by itself to form \\(AA\\). We refer to this matrix as \\(A^{2}\\). Similarly, if \\(k\\) is a positive integer, then \\(k\\) copies of \\(A\\) multiplied together is denoted \\(A^{k}\\). If \\(k\\) and \\(l\\) are positive integers, and \\(A\\) is square, then \\(A^{k}A^{l}=A^{k+l}\\) and \\((A^{k})^{l}=A^{kl}\\). By convention we take \\(A^{0}=I\\), which makes the formulas above hold for all nonnegative integer values of \\(k\\) and \\(l\\).\nWe should mention one ambiguity in matrix power notation that occasionally arises. When \\(A\\) is a square matrix and \\(T\\) is a nonnegative integer, \\(A^{T}\\) can mean either the transpose of the matrix \\(A\\) or its \\(T\\)th power. Usually which is meant is clear from the context, or the author explicitly states which meaning is intended. To avoid this ambiguity, some authors use a different symbol for the transpose, such as \\(A^{\\mathrm{T}}\\) (with the superscript in roman font) or \\(A^{\\prime}\\), or avoid referring to the \\(T\\)th power of a matrix. When \\(A\\) is not square there is no ambiguity, since \\(A^{T}\\) can only be the transpose in this case.\nOther matrix powers.Matrix powers \\(A^{k}\\) with \\(k\\) a negative integer will be discussed in SS11.2. Non-integer powers, such as \\(A^{1/2}\\) (the matrix squareroot), need not make sense, or can be ambiguous, unless certain conditions on \\(A\\) hold. This is an advanced topic in linear algebra that we will not pursue in this book.\n\n\n",
        "answer": 9,
        "rationale": {
            "Question": "If the quartic x^4 + 3x^3 + 11x^2 + 9x + A has roots k, l, m, and n such that kl = mn, find A.",
            "Answer": 9,
            "Answer_type": "integer",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula4.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "elainewan/math_algebra_1_2.json",
        "question": "Suppose the graph of a polynomial f(t) = a + bt + ct^2 passes through points (1, -1), (2, 3), and (3, 13). What is f(-1)?",
        "prediction": "the value of \\( f(-1) \\) obtained by solving the system of equations to find the coefficients of the polynomial",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nSuppose the graph of a polynomial f(t) = a + bt + ct^2 passes through points (1, -1), (2, 3), and (3, 13). What is f(-1)?\n\nDocument 1:\n----\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\nDocument 2:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\nDocument 3:\n----\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\nBalancing chemical reactions.A chemical reaction involves \\(p\\) reactants (molecules) and \\(q\\) products, and can be written as\n\\[a_{1}R_{1}+\\cdots+a_{p}R_{p}\\longrightarrow b_{1}P_{1}+\\cdots+b_{q}P_{q}.\\]\nHere \\(R_{1},\\ldots,R_{p}\\) are the reactants, \\(P_{1},\\ldots,P_{q}\\) are the products, and the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\) are positive numbers that tell us how many of each of these molecules is involved in the reaction. They are typically integers, but can be scaled arbitrarily; we could double all of these numbers, for example, and we still have the same reaction. As a simple example, we have the electrolysis of water,\n\nDocument 4:\n----\n* Show that for a skew-symmetric matrix \\(A\\), and any \\(n\\)-vector \\(x\\), \\((Ax)\\perp x\\). This means that \\(Ax\\) and \\(x\\) are orthogonal. _Hint_. First show that for any \\(n\\times n\\) matrix \\(A\\) and \\(n\\)-vector \\(x\\), \\(x^{T}(Ax)=\\sum_{i,j=1}^{n}A_{ij}x_{i}x_{j}\\).\n* Now suppose \\(A\\) is any matrix for which \\((Ax)\\perp x\\) for any \\(n\\)-vector \\(x\\). Show that \\(A\\) must be skew-symmetric. _Hint_. You might find the formula \\[(e_{i}+e_{j})^{T}(A(e_{i}+e_{j}))=A_{ii}+A_{jj}+A_{ij}+A_{ji},\\] valid for any \\(n\\times n\\) matrix \\(A\\), useful. For \\(i=j\\), this reduces to \\(e_{i}^{T}(Ae_{i})=A_{ii}\\).\n* _Polynomial differentiation_. Suppose \\(p\\) is a polynomial of degree \\(n-1\\) or less, given by \\(p(t)=c_{1}+c_{2}t+\\cdots+c_{n}t^{n-1}\\). Its derivative (with respect to \\(t\\)) \\(p^{\\prime}(t)\\) is a polynomial of degree \\(n-2\\) or less, given by \\(p^{\\prime}(t)=d_{1}+d_{2}t+\\cdots+d_{n-1}t^{n-2}\\). Find a matrix \\(D\\) for which \\(d=Dc\\). (Give the entries of \\(D\\), and be sure to specify its dimensions.)\n* _Norm of matrix-vector product_. Suppose \\(A\\) is an \\(m\\times n\\) matrix and \\(x\\) is an \\(n\\)-vector. A famous inequality relates \\(\\|x\\|\\), \\(\\|A\\|\\), and \\(\\|Ax\\|\\): \\[\\|Ax\\|\\leq\\|A\\|\\|x\\|.\\] The left-hand side is the (vector) norm of the matrix-vector product; the right-hand side is the (scalar) product of the matrix and vector norms. Show this inequality. _Hints_. Let \\(a_{i}^{T}\\) be the \\(i\\)th row of \\(A\\). Use the Cauchy-Schwarz inequality to get \\((a_{i}^{T}x)^{2}\\leq\\|a_{i}\\|^{2}\\|x\\|^{2}\\). Then add the resulting \\(m\\) inequalities.\n\nDocument 5:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\nBalancing chemical reactions.(See page 154 for background.) We consider the problem of balancing the chemical reaction\n\\[a_{1}\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+a_{2}\\mathrm{Fe}^{2+}+a_{3}\\mathrm{H} ^{+}\\longrightarrow b_{1}\\mathrm{Cr}^{3+}+b_{2}\\mathrm{Fe}^{3+}+b_{3} \\mathrm{H}_{2}\\mathrm{O},\\]\n\n\n",
        "answer": 9,
        "rationale": {
            "Question": "Suppose the graph of a polynomial f(t) = a + bt + ct^2 passes through points (1, -1), (2, 3), and (3, 13). What is f(-1)?",
            "Answer": 9,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_1_2.json",
            "explanation": "solutions/math_algebra_1_2.txt",
            "theorem": "linear systems",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "mingyin/minimal-polynomial1.json",
        "question": "matrix $A=(\\begin{array}{rrrr} -2 & -1 & -1 & -1 \\ 2 & 1 & 3 & 2 \\ 1 & 1 & 0 & 1 \\ -1 & -1 & -2 & -2 \\end{array})$. Suppose f is the minimal polynomial of A. What is f(99)? Return the numeric without explanation.",
        "prediction": "-1",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nmatrix $A=(\\begin{array}{rrrr} -2 & -1 & -1 & -1 \\ 2 & 1 & 3 & 2 \\ 1 & 1 & 0 & 1 \\ -1 & -1 & -2 & -2 \\end{array})$. Suppose f is the minimal polynomial of A. What is f(99)? Return the numeric without explanation.\n\nDocument 1:\n----\n(Note that here, \\(x^{i}\\) means the generic scalar value \\(x\\) raised to the \\(i\\)th power; \\(x^{(i)}\\) means the \\(i\\)th observed scalar data value.) In this case the matrix \\(A\\) in (13.1) has the form\n\\[A=\\left[\\begin{array}{ccccc}1&x^{(1)}&\\cdots&(x^{(1)})^{p-1}\\\\ 1&x^{(2)}&\\cdots&(x^{(2)})^{p-1}\\\\ \\vdots&\\vdots&&\\vdots\\\\ 1&x^{(N)}&\\cdots&(x^{(N)})^{p-1}\\end{array}\\right],\\]\n_i.e._, it is a Vandermonde matrix (see (6.7)). Its columns are linearly independent provided the numbers \\(x^{(1)},\\ldots,x^{(N)}\\) include at least \\(p\\) different values. Figure 13.6 shows an example of the least squares fit of polynomials of degree 2, 6, 10, and 15 to a set of 100 data points. Since any polynomial of degree less than \\(r\\) is also a polynomial of degree less than \\(s\\), for \\(r\\leq s\\), it follows that the RMS fit attained by a polynomial with a larger degree is smaller (or at least, no larger) than that obtained by a fit with a smaller degree polynomial. This suggests that we should use the largest degree polynomial that we can, since this results in the smallest residual and the best RMS fit. But we will see in SS13.2 that this is not true, and explore rational methods for choosing a model from among several candidates.\nPiecewise-linear fit.A _piecewise-linear_ function, with _knot points_ or _kink points_\\(a_{1}<a_{2}<\\cdots<a_{k}\\), is a continuous function that is affine in between the knot points. (Such functions should be called piecewise-affine.) We can describe any\n\nDocument 2:\n----\nThe complexity of this algorithm is dominated by the cost of the QR factorization in step 1, _i.e._, \\(2np^{2}\\) flops.\n## 16 Constrained least squares\n### 16.1 Smallest right inverse\nSuppose the \\(m\\times n\\) matrix \\(A\\) is wide, with linearly independent rows. Its pseudo-inverse \\(A^{\\dagger}\\) is a right inverse of \\(A\\). In fact, there are many right inverses of \\(A\\) and it turns out that \\(A^{\\dagger}\\) is the smallest one among them, as measured by the matrix norm. In other words, if \\(X\\) satisfies \\(AX=I\\), then \\(\\|X\\|\\geq\\|A^{\\dagger}\\|\\). You will show this in this problem.\n1. Suppose \\(AX=I\\), and let \\(x_{1},\\ldots,x_{m}\\) denote the columns of \\(X\\). Let \\(b_{j}\\) denote the \\(j\\)th column of \\(A^{\\dagger}\\). Explain why \\(\\|x_{j}\\|^{2}\\geq\\|b_{j}\\|^{2}\\). _Hint_. Show that \\(z=b_{j}\\) is the vector of smallest norm that satisfies \\(Az=e_{j}\\), for \\(j=1,\\ldots,m\\).\n2. Use the inequalities from part (a) to establish \\(\\|X\\|\\geq\\|A^{\\dagger}\\|\\).\n3. _Matrix least norm problem_. The matrix least norm problem is minimize \\[\\|X\\|^{2}\\] subject to \\[CX=D,\\] where the variable to be chosen is the \\(n\\times k\\) matrix \\(X\\); the \\(p\\times n\\) matrix \\(C\\) and the \\(p\\times k\\) matrix \\(D\\) are given. Show that the solution of this problem is \\(\\hat{X}=C^{\\dagger}D\\), assuming the rows of \\(C\\) are linearly independent. _Hint_. Show that we can find the columns of \\(X\\) independently, by solving a least norm problem for each one.\n\nDocument 3:\n----\nOur assumption (12.2) that the columns of \\(A\\) are linearly independent implies that the Gram matrix \\(A^{T}A\\) is invertible (SS11.5, page 214). This implies that\n\\[\\hat{x}=(A^{T}A)^{-1}A^{T}b\\] (12.5)\nis the only solution of the normal equations (12.4). So this must be the unique solution of the least squares problem (12.1).\nWe have already encountered the matrix \\((A^{T}A)^{-1}A^{T}\\) that appears in (12.5): It is the pseudo-inverse of the matrix \\(A\\), given in (11.5). So we can write the solution of the least squares problem in the simple form\n\\[\\hat{x}=A^{\\dagger}b.\\] (12.6)\nWe observed in SS11.5 that \\(A^{\\dagger}\\) is a left inverse of \\(A\\), which means that \\(\\hat{x}=A^{\\dagger}b\\) solves \\(Ax=b\\) if this set of over-determined equations has a solution. But now we see that \\(\\hat{x}=A^{\\dagger}b\\) is the least squares approximate solution, _i.e._, it minimizes \\(\\|Ax-b\\|^{2}\\). (And if there is a solution of \\(Ax=b\\), then \\(\\hat{x}=A^{\\dagger}b\\) is it.)\nThe equation (12.6) looks very much like the formula for solution of the linear equations \\(Ax=b\\), when \\(A\\) is square and invertible, _i.e._, \\(x=A^{-1}b\\). It is very important to understand the difference between the formula (12.6) for the least squares approximate solution, and the formula for the solution of a square set of linear equations, \\(x=A^{-1}b\\). In the case of linear equations and the inverse, \\(x=A^{-1}b\\) actually satisfies \\(Ax=b\\). In the case of the least squares approximate solution, \\(\\hat{x}=A^{\\dagger}b\\) generally _does not_ satisfy \\(A\\hat{x}=b\\).\n\nDocument 4:\n----\nPseudo-inverse of a square or wide matrix.Transposing all the equations, we can show that a (square or wide) matrix \\(A\\) has a right inverse if and only if its rows are linearly independent. Indeed, one right inverse is given by\n\\[A^{T}(AA^{T})^{-1}.\\] (11.6)\n(The matrix \\(AA^{T}\\) is invertible if and only if the rows of \\(A\\) are linearly independent.)\nThe matrix in (11.6) is also referred to as the pseudo-inverse of \\(A\\), and denoted \\(A^{\\dagger}\\). The only possible confusion in defining the pseudo-inverse using the two different formulas (11.5) and (11.6) occurs when the matrix \\(A\\) is square. In this case, however, they both reduce to the ordinary inverse:\n\\[A^{T}(AA^{T})^{-1}=A^{T}A^{-T}A^{-1}=A^{-1}.\\]\nPseudo-inverse in other cases.The pseudo-inverse \\(A^{\\dagger}\\) is defined for any matrix, including the case when \\(A\\) is tall but its columns are linearly dependent, the case when \\(A\\) is wide but its rows are linearly dependent, and the case when \\(A\\) is square but not invertible. In these cases, however, it is not a left inverse, right inverse, or inverse, respectively. We mention it here since the reader may encounter it. (We will see what \\(A^{\\dagger}\\) means in these cases in exercise 11.)\nPseudo-inverse via QR factorization.The QR factorization gives a simple formula for the pseudo-inverse. If \\(A\\) is left-invertible, its columns are linearly independent and the QR factorization \\(A=QR\\) exists. We have\n\nDocument 5:\n----\nWe can solve this using the methods for general constrained least squares, or derive the solution directly, which we do now. The first block row of this equation is \\(2\\hat{x}+C^{T}\\hat{z}=0\\), so\n\\[\\hat{x}=-(1/2)C^{T}\\hat{z}.\\]\nWe substitute this into the second block equation, \\(C\\hat{x}=d\\), to obtain\n\\[-(1/2)CC^{T}\\hat{z}=d.\\]\nSince the rows of \\(C\\) are linearly independent, \\(CC^{T}\\) is invertible, so we have\n\\[\\hat{z}=-2(CC^{T})^{-1}d.\\]\nSubstituting this expression for \\(\\hat{z}\\) into the formula for \\(\\hat{x}\\) above gives\n\\[\\hat{x}=C^{T}(CC^{T})^{-1}d.\\] (16.12)\nWe have seen the matrix in this formula before: It is the pseudo-inverse of a wide matrix with linearly independent rows. So we can express the solution of the least norm problem (16.2) in the very compact form\n\\[\\hat{x}=C^{\\dagger}d.\\]\nIn SS11.5, we saw that \\(C^{\\dagger}\\) is a right inverse of \\(C\\); here we see that not only does \\(\\hat{x}=C^{\\dagger}d\\) satisfy \\(Cx=d\\), but it gives the vector of least norm that satisfies \\(Cx=d\\).\nIn SS11.5, we also saw that the pseudo-inverse of \\(C\\) can be expressed as \\(C^{\\dagger}=QR^{-T}\\), where \\(C^{T}=QR\\) is the QR factorization of \\(C^{T}\\). The solution of the least norm problem can therefore be expressed as\n\\[\\hat{x}=QR^{-T}d\\]\nand this leads to an algorithm for solving the least norm problem via the QR factorization.\n``` given a \\(p\\times n\\) matrix \\(C\\) with linearly independent rows and a \\(p\\)-vector \\(d\\).\n1. QR factorization. Compute the QR factorization \\(C^{T}=QR\\).\n\n\n",
        "answer": 990000.0,
        "rationale": {
            "Question": "matrix $A=(\\begin{array}{rrrr} -2 & -1 & -1 & -1 \\ 2 & 1 & 3 & 2 \\ 1 & 1 & 0 & 1 \\ -1 & -1 & -2 & -2 \\end{array})$. Suppose f is the minimal polynomial of A. What is f(99)? Return the numeric without explanation.",
            "Answer": 990000.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 7.1 example 1",
            "id": "mingyin/minimal-polynomial1.json",
            "explanation": "NONE",
            "theorem": "minimal polynomial",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float"
    },
    {
        "id": "elainewan/math_algebra_6_2.json",
        "question": "Let V be the space spanned by functions cos(2x) and sin(2x). Find the determinant of the linear transformation D(f) = f' from V to V.",
        "prediction": "\\(-4\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet V be the space spanned by functions cos(2x) and sin(2x). Find the determinant of the linear transformation D(f) = f' from V to V.\n\nDocument 1:\n----\nholds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). It is a good exercise to parse this simple looking equation, since it involves overloading of notation. On the left-hand side, the scalar-vector multiplications \\(\\alpha x\\) and \\(\\beta y\\) involve \\(n\\)-vectors, and the sum \\(\\alpha x+\\beta y\\) is the sum of two \\(n\\)-vectors. The function \\(f\\) maps \\(n\\)-vectors to \\(m\\)-vectors, so \\(f(\\alpha x+\\beta y)\\) is an \\(m\\)-vector. On the right-hand side, the scalar-vector multiplications and the sum are those for \\(m\\)-vectors. Finally, the equality sign is equality between two \\(m\\)-vectors.\nWe can verify that superposition holds for \\(f\\) using properties of matrix-vector and scalar-vector multiplication:\n\\[f(\\alpha x+\\beta y) = A(\\alpha x+\\beta y)\\] \\[= A(\\alpha x)+A(\\beta y)\\] \\[= \\alpha(Ax)+\\beta(Ay)\\] \\[= \\alpha f(x)+\\beta f(y)\\]\nThus we can associate with every matrix \\(A\\) a linear function \\(f(x)=Ax\\).\nThe converse is also true. Suppose \\(f\\) is a function that maps \\(n\\)-vectors to \\(m\\)-vectors, and is linear, _i.e._, (8.1) holds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). Then there exists an \\(m\\times n\\) matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\). This can be shown in the same way as for scalar-valued functions in SS2.1, by showing that if \\(f\\) is linear, then\n\\[f(x)=x_{1}f(e_{1})+x_{2}f(e_{2})+\\cdots+x_{n}f(e_{n}),\\] (8.2)\nwhere \\(e_{k}\\) is the \\(k\\)th unit vector of size \\(n\\). The right-hand side can also be written as a matrix-vector product \\(Ax\\), with\n\\[A=\\left[\\begin{array}{cccc}f(e_{1})&f(e_{2})&\\cdots&f(e_{n})\\end{array} \\right].\\]\n\nDocument 2:\n----\nNote that in each of the three equations above, the vectors appearing in the left- and right-hand sides have different dimensions, \\(m\\) on the left and \\(n\\) on the right.\nWe can verify these properties using simple matrix properties. We start with the second statement, that multiplication by \\(A\\) preserves the inner product. We have\n\\[(Ax)^{T}(Ay) = (x^{T}A^{T})(Ay)\\] \\[= x^{T}(A^{T}A)y\\] \\[= x^{T}Iy\\] \\[= x^{T}y.\\]\nIn the first line, we use the transpose-of-product rule; in the second, we re-associate a product of 4 matrices (considering the row vector \\(x^{T}\\) and column vector \\(x\\) as matrices); in the third line, we use \\(A^{T}A=I\\); and in the fourth line, we use \\(Iy=y\\).\nFrom the second property we can derive the first one: By taking \\(y=x\\) we get \\((Ax)^{T}(Ax)=x^{T}x\\); taking the squareroot of each side gives \\(\\|Ax\\|=\\|x\\|\\). The third property, angle preservation, follows from the first two, since\n\\[\\angle(Ax,Ay)=\\arccos\\left(\\frac{(Ax)^{T}(Ay)}{\\|Ax\\|\\|Ay\\|}\\right)=\\arccos \\left(\\frac{x^{T}y}{\\|x\\|\\|y\\|}\\right)=\\angle(x,y).\\]\nQR factorization.We can express the result of the Gram-Schmidt algorithm described in SS5.4 in a compact form using matrices. Let \\(A\\) be an \\(n\\times k\\) matrix with linearly independent columns \\(a_{1},\\ldots,a_{k}\\). By the independence-dimension inequality, \\(A\\) is tall or square. Let \\(Q\\) be the \\(n\\times k\\) matrix with columns \\(q_{1},\\ldots,q_{k}\\), the orthonormal vectors produced by the Gram-Schmidt algorithm applied to the \\(n\\)-vectors \\(a_{1},\\ldots,a_{k}\\). Orthonormality of \\(q_{1},\\ldots,q_{k}\\) is expressed in matrix form as \\(Q^{T}Q=I\\). We express the equation relating \\(a_{i}\\) and \\(q_{i}\\),\n\\[a_{i}=(q_{1}^{T}a_{i})q_{1}+\\cdots+(q_{i-1}^{T}a_{i})q_{i-1}+\\|\\tilde{q}_{i}\\| q_{i},\\]\n\nDocument 3:\n----\n### 11.5 Pseudo-inverse\nLinearly independent columns and Gram invertibility.We first show that an \\(m\\times n\\) matrix \\(A\\) has linearly independent columns if and only if its \\(n\\times n\\) Gram matrix \\(A^{T}A\\) is invertible.\nFirst suppose that the columns of \\(A\\) are linearly independent. Let \\(x\\) be an \\(n\\)-vector which satisfies \\((A^{T}A)x=0\\). Multiplying on the left by \\(x^{T}\\) we get\n\\[0=x^{T}0=x^{T}(A^{T}Ax)=x^{T}A^{T}Ax=\\|Ax\\|^{2},\\]\nwhich implies that \\(Ax=0\\). Since the columns of \\(A\\) are linearly independent, we conclude that \\(x=0\\). Since the only solution of \\((A^{T}A)x=0\\) is \\(x=0\\), we conclude that \\(A^{T}A\\) is invertible.\nNow let's show the converse. Suppose the columns of \\(A\\) are linearly dependent, which means there is a nonzero \\(n\\)-vector \\(x\\) which satisfies \\(Ax=0\\). Multiply on the left by \\(A^{T}\\) to get \\((A^{T}A)x=0\\). This shows that the Gram matrix \\(A^{T}A\\) is singular.\nPseudo-inverse of square or tall matrix.We show here that if \\(A\\) has linearly independent columns (and therefore, is square or tall) then it has a left inverse. (We already have observed the converse, that a matrix with a left inverse has linearly independent columns.) Assuming \\(A\\) has linearly independent columns, we know that \\(A^{T}A\\) is invertible. We now observe that the matrix \\((A^{T}A)^{-1}A^{T}\\) is a left inverse of \\(A\\):\n\nDocument 4:\n----\n**2.7**: _General formula for affine functions._ Verify that formula (2.4) holds for any affine function \\(f:{\\bf R}^{n}\\to{\\bf R}\\). You can use the fact that \\(f(x)=a^{T}x+b\\) for some \\(n\\)-vector \\(a\\) and scalar \\(b\\).\n**2.8**: _Integral and derivative of polynomial._ Suppose the \\(n\\)-vector \\(c\\) gives the coefficients of a polynomial \\(p(x)=c_{1}+c_{2}x+\\cdots+c_{n}x^{n-1}\\).\n1. Let \\(\\alpha\\) and \\(\\beta\\) be numbers with \\(\\alpha<\\beta\\). Find an \\(n\\)-vector \\(a\\) for which \\[a^{T}c=\\int_{\\alpha}^{\\beta}p(x)\\;dx\\] always holds. This means that the integral of a polynomial over an interval is a linear function of its coefficients.\n2. Let \\(\\alpha\\) be a number. Find an \\(n\\)-vector \\(b\\) for which \\[b^{T}c=p^{\\prime}(\\alpha).\\] This means that the derivative of the polynomial at a given point is a linear function of its coefficients.\n**2.9**: _Taylor approximation._ Consider the function \\(f:{\\bf R}^{2}\\to{\\bf R}\\) given by \\(f(x_{1},x_{2})=x_{1}x_{2}\\). Find the Taylor approximation \\(\\hat{f}\\) at the point \\(z=(1,1)\\). Compare \\(f(x)\\) and \\(\\hat{f}(x)\\) for the following values of \\(x\\): \\[x=(1,1),\\quad x=(1.05,0.95),\\quad x=(0.85,1.25),\\quad x=(-1,2).\\] Make a brief comment about the accuracy of the Taylor approximation in each case.\n**2.10**: _Regression model._ Consider the regression model \\(\\hat{y}=x^{T}\\beta+v\\), where \\(\\hat{y}\\) is the predicted response, \\(x\\) is an 8-vector of features, \\(\\beta\\) is an 8-vector of coefficients, and \\(v\\) is the offset term. Determine whether each of the following statements is true or false.\n\nDocument 5:\n----\nWhen a matrix \\(A\\) has both a left inverse \\(Y\\) and a right inverse \\(X\\), we call the matrix \\(X=Y\\) simply the _inverse_ of \\(A\\), and denote it as \\(A^{-1}\\). We say that \\(A\\) is _invertible_ or _nonsingular_. A square matrix that is not invertible is called _singular_.\nDimensions of invertible matrices.Invertible matrices must be square, since tall matrices are not right-invertible, while wide matrices are not left-invertible. A matrix \\(A\\) and its inverse (if it exists) satisfy\n\\[AA^{-1}=A^{-1}A=I.\\]\nIf \\(A\\) has inverse \\(A^{-1}\\), then the inverse of \\(A^{-1}\\) is \\(A\\); in other words, we have \\((A^{-1})^{-1}=A\\). For this reason we say that \\(A\\) and \\(A^{-1}\\) are inverses (of each other).\nSolving linear equations with the inverse.Consider the square system of \\(n\\) linear equations with \\(n\\) variables, \\(Ax=b\\). If \\(A\\) is invertible, then for any \\(n\\)-vector \\(b\\),\n\\[x=A^{-1}b\\] (11.1)\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\n\n\n",
        "answer": 4,
        "rationale": {
            "Question": "Let V be the space spanned by functions cos(2x) and sin(2x). Find the determinant of the linear transformation D(f) = f' from V to V.",
            "Answer": 4,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_2.json",
            "explanation": "NONE",
            "theorem": "basis",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "jianyu_xu/geometric_counting_1.json",
        "question": "How many triangles are there whose sides are all integers and whose maximum side length equals 11?",
        "prediction": "2",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nHow many triangles are there whose sides are all integers and whose maximum side length equals 11?\n\nDocument 1:\n----\nAs an example, consider the 4-vectors\n\\[u=\\left[\\begin{array}{c}1.8\\\\ 2.0\\\\ -3.7\\\\ 4.7\\end{array}\\right],\\qquad v=\\left[\\begin{array}{c}0.6\\\\ 2.1\\\\ 1.9\\\\ -1.4\\end{array}\\right],\\qquad w=\\left[\\begin{array}{c}2.0\\\\ 1.9\\\\ -4.0\\\\ 4.6\\end{array}\\right].\\]\nThe distances between pairs of them are\n\\[\\|u-v\\|=8.368,\\qquad\\|u-w\\|=0.387,\\qquad\\|v-w\\|=8.533,\\]\nso we can say that \\(u\\) is much nearer (or closer) to \\(w\\) than it is to \\(v\\). We can also say that \\(w\\) is much nearer to \\(u\\) than it is to \\(v\\).\nTriangle inequality.We can now explain where the triangle inequality gets its name. Consider a triangle in two or three dimensions, whose vertices have coordinates \\(a\\), \\(b\\), and \\(c\\). The lengths of the sides are the distances between the vertices,\n\\[\\mathbf{dist}(a,b)=\\|a-b\\|,\\qquad\\mathbf{dist}(b,c)=\\|b-c\\|,\\qquad\\mathbf{dist }(a,c)=\\|a-c\\|.\\]\nGeometric intuition tells us that the length of any side of a triangle cannot exceed the sum of the lengths of the other two sides. For example, we have\n\\[\\|a-c\\|\\leq\\|a-b\\|+\\|b-c\\|.\\] (3.3)\nThis follows from the triangle inequality, since\n\\[\\|a-c\\|=\\|(a-b)+(b-c)\\|\\leq\\|a-b\\|+\\|b-c\\|.\\]\nThis is illustrated in figure 3.2.\nFigure 3.2: Triangle inequality.\n#### Examples.\n* _Feature distance._ If \\(x\\) and \\(y\\) represent vectors of \\(n\\) features of two objects, the quantity \\(\\|x-y\\|\\) is called the _feature distance_, and gives a measure of how different the objects are (in terms of their feature values). Suppose for example the feature vectors are associated with patients in a hospital, with entries such as weight, age, presence of chest pain, difficulty breathing, and the results of tests. We can use feature vector distance to say that one patient case is near another one (at least in terms of their feature vectors).\n\nDocument 2:\n----\nWhen the distance between two \\(n\\)-vectors \\(x\\) and \\(y\\) is small, we say they are 'close' or 'nearby', and when the distance \\(\\|x-y\\|\\) is large, we say they are 'far'. The particular numerical values of \\(\\|x-y\\|\\) that correspond to 'close' or 'far' depend on\nFigure 3.1: The norm of the displacement \\(b-a\\) is the distance between the points with coordinates \\(a\\) and \\(b\\).\nAs an example, consider the 4-vectors\n\\[u=\\left[\\begin{array}{c}1.8\\\\ 2.0\\\\ -3.7\\\\ 4.7\\end{array}\\right],\\qquad v=\\left[\\begin{array}{c}0.6\\\\ 2.1\\\\ 1.9\\\\ -1.4\\end{array}\\right],\\qquad w=\\left[\\begin{array}{c}2.0\\\\ 1.9\\\\ -4.0\\\\ 4.6\\end{array}\\right].\\]\nThe distances between pairs of them are\n\\[\\|u-v\\|=8.368,\\qquad\\|u-w\\|=0.387,\\qquad\\|v-w\\|=8.533,\\]\nso we can say that \\(u\\) is much nearer (or closer) to \\(w\\) than it is to \\(v\\). We can also say that \\(w\\) is much nearer to \\(u\\) than it is to \\(v\\).\nTriangle inequality.We can now explain where the triangle inequality gets its name. Consider a triangle in two or three dimensions, whose vertices have coordinates \\(a\\), \\(b\\), and \\(c\\). The lengths of the sides are the distances between the vertices,\n\\[\\mathbf{dist}(a,b)=\\|a-b\\|,\\qquad\\mathbf{dist}(b,c)=\\|b-c\\|,\\qquad\\mathbf{dist }(a,c)=\\|a-c\\|.\\]\nGeometric intuition tells us that the length of any side of a triangle cannot exceed the sum of the lengths of the other two sides. For example, we have\n\\[\\|a-c\\|\\leq\\|a-b\\|+\\|b-c\\|.\\] (3.3)\nThis follows from the triangle inequality, since\n\\[\\|a-c\\|=\\|(a-b)+(b-c)\\|\\leq\\|a-b\\|+\\|b-c\\|.\\]\nThis is illustrated in figure 3.2.\n\nDocument 3:\n----\nThe numbers or values of the elements in a vector are called _scalars_. We will focus on the case that arises in most applications, where the scalars are real numbers. In this case we refer to vectors as _real vectors_. (Occasionally other types of scalars arise, for example, complex numbers, in which case we refer to the vector as a _complex vector_.) The set of all real numbers is written as \\(\\mathbf{R}\\), and the set of all real \\(n\\)-vectors is denoted \\(\\mathbf{R}^{n}\\), so \\(a\\in\\mathbf{R}^{n}\\) is another way to say that \\(a\\) is an \\(n\\)-vector with real entries. Here we use set notation: \\(a\\in\\mathbf{R}^{n}\\) means that \\(a\\) is an element of the set \\(\\mathbf{R}^{n}\\); see appendix A.\nBlock or stacked vectors.It is sometimes useful to define vectors by _concatenating_ or _stacking_ two or more vectors, as in\n\\[a=\\left[\\begin{array}{c}b\\\\ c\\\\ d\\end{array}\\right],\\]\nwhere \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are vectors. If \\(b\\) is an \\(m\\)-vector, \\(c\\) is an \\(n\\)-vector, and \\(d\\) is a \\(p\\)-vector, this defines the \\((m+n+p)\\)-vector\n\\[a=(b_{1},b_{2},\\dots,b_{m},c_{1},c_{2},\\dots,c_{n},d_{1},d_{2},\\dots,d_{p}).\\]\nThe stacked vector \\(a\\) is also written as \\(a=(b,c,d)\\).\nStacked vectors can include scalars (numbers). For example if \\(a\\) is a 3-vector, \\((1,a)\\) is the 4-vector \\((1,a_{1},a_{2},a_{3})\\).\nSubvectors.In the equation above, we say that \\(b\\), \\(c\\), and \\(d\\) are _subvectors_ or _slices_ of \\(a\\), with sizes \\(m\\), \\(n\\), and \\(p\\), respectively. _Colon notation_ is used to denote subvectors. If \\(a\\) is a vector, then \\(a_{r:s}\\) is the vector of size \\(s-r+1\\), with entries \\(a_{r},\\dots,a_{s}\\):\n\\[a_{r:s}=(a_{r},\\dots,a_{s}).\\]\nThe subscript \\(r\\!:\\!s\\) is called the _index range_. Thus, in our example above, we have\n\\[b=a_{1:m},\\qquad c=a_{(m+1):(m+n)},\\qquad d=a_{(m+n+1):(m+n+p)}.\\]\nAs a more concrete example, if \\(z\\) is the 4-vector \\((1,-1,2,0)\\), the slice \\(z_{2:3}\\) is \\(z_{2:3}=(-1,2)\\). Colon notation is not completely standard, but it is growing in popularity.\n\nDocument 4:\n----\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n1. Find all diagonal squareroots of \\(I_{n}\\). How many are there? (For \\(n=1\\), you should get \\(2\\).)\n* Find a nondiagonal \\(2\\times 2\\) matrix \\(A\\) that satisfies \\(A^{2}=I\\). This means that in general there are even more squareroots of \\(I_{n}\\) than you found in part (a).\n* _Circular shift matrices._ Let \\(A\\) be the \\(5\\times 5\\) matrix \\[A=\\left[\\begin{array}{cccc}0&0&0&0&1\\\\ 1&0&0&0&0\\\\ 0&1&0&0&0\\\\ 0&0&1&0&0\\\\ 0&0&0&1&0\\end{array}\\right].\\] 1. How is \\(Ax\\) related to \\(x\\)? Your answer should be in English. _Hint._ See exercise title. 2. What is \\(A^{5}\\)? _Hint._ The answer should make sense, given your answer to part (a).\n* _Dynamics of an economy._ Let \\(x_{1},x_{2},\\ldots\\) be \\(n\\)-vectors that give the level of economic activity of a country in years \\(1,2,\\ldots\\), in \\(n\\) different sectors (like energy, defense, manufacturing). Specifically, \\((x_{t})_{i}\\) is the level of economic activity in economic sector \\(i\\) (say, in billions of dollars) in year \\(t\\). A common model that connects these economic activity vectors is \\(x_{t+1}=Bx_{t}\\), where \\(B\\) is an \\(n\\times n\\) matrix. (See exercise 9.2.) Five a matrix expression for the total economic activity across all sectors in year \\(t=6\\), in terms of the matrix \\(B\\) and the vector of initial activity levels \\(x_{1}\\). Suppose you can increase economic activity in year \\(t=1\\) by some fixed amount (say, one billion dollars) in _one_ sector, by government spending. How should you choose which sector to stimulate so as to maximize the total economic output in year \\(t=6\\)?\n\nDocument 5:\n----\n\\[A=\\left[\\begin{array}{ccccc}0&1&0&0&1\\\\ 1&0&1&0&0\\\\ 0&0&1&1&1\\\\ 1&0&0&0&0\\\\ 0&0&0&1&0\\end{array}\\right],\\qquad A^{2}=\\left[\\begin{array}{ccccc}1&0&1&1& 0\\\\ 0&1&1&1&2\\\\ 1&0&1&2&1\\\\ 0&1&0&0&1\\\\ 1&0&0&0&0\\end{array}\\right].\\]\nWe can verify there is exactly one path of length two from vertex 1 to itself, _i.e._, the path \\((1,2,1))\\), and one path of length two from vertex 3 to vertex 1, _i.e._, the path \\((3,2,1)\\). There are two paths of length two from vertex 4 to vertex 3, \\((4,3,3)\\) and \\((4,5,3)\\), so \\((A^{2})_{34}=2\\).\nThe property extends to higher powers of \\(A\\). If \\(\\ell\\) is a positive integer, then the \\(i,j\\) element of \\(A^{\\ell}\\) is the number of paths of length \\(\\ell\\) from vertex \\(j\\) to vertex \\(i\\). This can be proved by induction on \\(\\ell\\). We have already shown the result for \\(\\ell=2\\). Assume that it is true that the elements of \\(A^{\\ell}\\) give the paths of length \\(\\ell\\) between the different vertices. Consider the expression for the \\(i,j\\) element of \\(A^{\\ell+1}\\):\n\\[(A^{\\ell+1})_{ij}=\\sum_{k=1}^{n}A_{ik}(A^{\\ell})_{kj}.\\]\nThe \\(k\\)th term in the sum is equal to the number of paths of length \\(\\ell\\) from \\(j\\) to \\(k\\) if there is an edge from \\(k\\) to \\(i\\), and is equal to zero otherwise. Therefore it is equal to the number of paths of length \\(\\ell+1\\) from \\(j\\) to \\(i\\) that end with the edge \\((k,i)\\), _i.e._, of the form \\((j,\\ldots,k,i)\\). By summing over all \\(k\\) we obtain the total number of paths of length \\(\\ell+1\\) from vertex \\(j\\) to \\(i\\).\n\n\n",
        "answer": 36,
        "rationale": {
            "Question": "How many triangles are there whose sides are all integers and whose maximum side length equals 11?",
            "Answer": 36,
            "Answer_type": "integer",
            "Picture": null,
            "source": "self",
            "id": "jianyu_xu/geometric_counting_1.json",
            "explanation": "NONE",
            "theorem": "integer programming",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer"
    },
    {
        "id": "mingyin/gaussian-elimination3.json",
        "question": "Suppose V is a finite-dimensional vector space on F. $M1={a_1,a_2,a_3}$ is a basis of V, $M2={b_1,b_2,b_3}$ is another basis of V. Suppose the coordinates of b_1,b_2,b_3 under M1 are $c_1=(1,1,-1),c_2=(1,-1,1),c_3=(-1,1,1)$. Suppose the coordinate of $d\\in V$ under M1 is (1,3,5). What is the coordinate of d under M2? Return the three coordinate values as a list.",
        "prediction": "\\([-1, 3, 7]\\)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nSuppose V is a finite-dimensional vector space on F. $M1={a_1,a_2,a_3}$ is a basis of V, $M2={b_1,b_2,b_3}$ is another basis of V. Suppose the coordinates of b_1,b_2,b_3 under M1 are $c_1=(1,1,-1),c_2=(1,-1,1),c_3=(-1,1,1)$. Suppose the coordinate of $d\\in V$ under M1 is (1,3,5). What is the coordinate of d under M2? Return the three coordinate values as a list.\n\nDocument 1:\n----\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\\[l_{i}=\\left[\\begin{array}{c}0_{i-1}\\\\ 1\\\\ -(1+r)\\\\ 0_{n-i-1}\\end{array}\\right],\\quad i=1,\\ldots,n-1,\\]\nwhere \\(r\\geq 0\\) is the per-period interest rate. The cash flow \\(l_{i}\\) represents a loan of $1 in period \\(i\\), which is paid back in period \\(i+1\\) with interest \\(r\\). (The subscripts on the zero vectors above give their dimensions.) Scaling \\(l_{i}\\) changes the loan amount; scaling \\(l_{i}\\) by a negative coefficient converts it into a loan _to_ another entity (which is paid back in period \\(i+1\\) with interest).\n\nDocument 2:\n----\nthen (evidently) \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); conversely, if \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\), the identity (5.4) holds for \\(y\\).\nOrthonormal basis.If the \\(n\\)-vectors \\(a_{1},\\ldots,a_{n}\\) are orthonormal, they are linearly independent, and therefore also a basis. In this case they are called an _orthonormal basis_. The three examples above (on page 5.2) are orthonormal bases.\nIf \\(a_{1},\\ldots,a_{n}\\) is an orthonormal basis, then we have, for any \\(n\\)-vector \\(x\\), the identity\n\\[x=(a_{1}^{T}x)a_{1}+\\cdots+(a_{n}^{T}x)a_{n}.\\] (5.5)\nTo see this, we note that since \\(a_{1},\\ldots,a_{n}\\) are a basis, \\(x\\) can be expressed as a linear combination of them; hence the identity (5.4) above holds. The equation above is sometimes called the _orthonormal expansion formula_; the right-hand side is called the _expansion of \\(x\\) in the basis \\(a_{1},\\ldots,a_{n}\\)_. It shows that any \\(n\\)-vector can be expressed as a linear combination of the basis elements, with the coefficients given by taking the inner product of \\(x\\) with the elements of the basis.\nAs an example, we express the 3-vector \\(x=(1,2,3)\\) as a linear combination of the orthonormal basis given in (5.3). The inner products of \\(x\\) with these vectors\nare\n\\[\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right]^{T}x=-3,\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ 1\\\\ 0\\end{array}\\right]^{T}x=\\frac{3}{\\sqrt{2}},\\qquad\\frac{1}{\\sqrt{2}}\\left[ \\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right]^{T}x=\\frac{-1}{\\sqrt{2}}.\\]\nIt can be verified that the expansion of \\(x\\) in this basis is\n\\[x=(-3)\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right]+\\frac{3}{\\sqrt{2}}\\left(\\frac{1}{\\sqrt{2}}\\left[\\begin{array} []{c}1\\\\ 1\\\\ 0\\end{array}\\right]\\right)+\\frac{-1}{\\sqrt{2}}\\left(\\frac{1}{\\sqrt{2}}\\left[ \\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right]\\right).\\]\n\nDocument 3:\n----\nSince the order of the transpose and inverse operations does not matter, this matrix is sometimes written as \\(A^{-T}\\).\nInverse of matrix product.If \\(A\\) and \\(B\\) are invertible (hence, square) and of the same size, then \\(AB\\) is invertible, and\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\nNow suppose that \\(x\\) is any \\(n\\)-vector. It can be expressed as a linear combination of the basis vectors \\(a_{1},\\ldots,a_{n}\\):\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{n}a_{n}.\\]\nThe dual basis gives us a simple way to find the coefficients \\(\\beta_{1},\\ldots,\\beta_{n}\\).\nWe start with \\(AB=I\\), and multiply by \\(x\\) to get\n\\[x=ABx=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right]\\left[ \\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right]x=(b_{1}^{T}x)a_{1}+\\cdots+(b_{n}^{T}x)a_{n}.\\]\nThis means (since the vectors \\(a_{1},\\ldots,a_{n}\\) are linearly independent) that \\(\\beta_{i}=b_{i}^{T}x\\). In words: The coefficients in the expansion of a vector in a basis are given by the inner products with the dual basis vectors. Using matrix notation, we can say that \\(\\beta=B^{T}x=(A^{-1})^{T}x\\) is the vector of coefficients of \\(x\\) in the basis given by the columns of \\(A\\).\n\nDocument 4:\n----\n### 7.1 Geometric transformations\nSuppose the 2-vector (or 3-vector) \\(x\\) represents a position in 2-D (or 3-D) space. Several important geometric transformations or mappings from points to points can be expressed as matrix-vector products \\(y=Ax\\), with \\(A\\) a \\(2\\times 2\\) (or \\(3\\times 3\\)) matrix. In the examples below, we consider the mapping from \\(x\\) to \\(y\\), and focus on the 2-D case (for which some of the matrices are simpler to describe).\nScaling.Scaling is the mapping \\(y=ax\\), where \\(a\\) is a scalar. This can be expressed as \\(y=Ax\\) with \\(A=aI\\). This mapping stretches a vector by the factor \\(|a|\\) (or shrinks it when \\(|a|<1\\)), and it flips the vector (reverses its direction) if \\(a<0\\).\nDilation.Dilation is the mapping \\(y=Dx\\), where \\(D\\) is a diagonal matrix, \\(D=\\mathbf{diag}(d_{1},d_{2})\\). This mapping stretches the vector \\(x\\) by different factors along the two different axes. (Or shrinks, if \\(|d_{i}|<1\\), and flips, if \\(d_{i}<0\\).)\nRotation.Suppose that \\(y\\) is the vector obtained by rotating \\(x\\) by \\(\\theta\\) radians counterclockwise. Then we have\n\\[y=\\left[\\begin{array}{cc}\\cos\\theta&-\\sin\\theta\\\\ \\sin\\theta&\\cos\\theta\\end{array}\\right]x.\\] (7.1)\nThis matrix is called (for obvious reasons) a _rotation matrix_.\nReflection.Suppose that \\(y\\) is the vector obtained by reflecting \\(x\\) through the line that passes through the origin, inclined \\(\\theta\\) radians with respect to horizontal. Then we have\n\nDocument 5:\n----\n### 7.1 Projection on a line\nLet \\(P(x)\\) denote the projection of the 2-D point (2-vector) \\(x\\) onto the line that passes through \\((0,0)\\) and \\((1,3)\\). (This means that \\(P(x)\\) is the point on the line that is closest to \\(x\\); see exercise 3.12.) Show that \\(P\\) is a linear function, and give the matrix \\(A\\) for which \\(P(x)=Ax\\) for any \\(x\\).\n_3-D rotation._ Let \\(x\\) and \\(y\\) be 3-vectors representing positions in 3-D. Suppose that the vector \\(y\\) is obtained by rotating the vector \\(x\\) about the vertical axis (_i.e._, \\(e_{3}\\)) by \\(45^{\\circ}\\) (counterclockwise, _i.e._, from \\(e_{1}\\) toward \\(e_{2}\\)). Find the \\(3\\times 3\\) matrix \\(A\\) for which \\(y=Ax\\). _Hint._ Determine the three columns of \\(A\\) by finding the result of the transformation on the unit vectors \\(e_{1},e_{2},e_{3}\\).\n_3-Trimming a vector._ Find a matrix \\(A\\) for which \\(Ax=(x_{2},\\ldots,x_{n-1})\\), where \\(x\\) is an \\(n\\)-vector. (Be sure to specify the size of \\(A\\), and describe all its entries.)\n_3-Down-sampling and up-conversion._ We consider \\(n\\)-vectors \\(x\\) that represent signals, with \\(x_{k}\\) the value of the signal at time \\(k\\) for \\(k=1,\\ldots,n\\). Below we describe two functions of \\(x\\) that produce new signals \\(f(x)\\). For each function, give a matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\).\n\n\n",
        "answer": [
            2,
            3,
            4
        ],
        "rationale": {
            "Question": "Suppose V is a finite-dimensional vector space on F. $M1={a_1,a_2,a_3}$ is a basis of V, $M2={b_1,b_2,b_3}$ is another basis of V. Suppose the coordinates of b_1,b_2,b_3 under M1 are $c_1=(1,1,-1),c_2=(1,-1,1),c_3=(-1,1,1)$. Suppose the coordinate of $d\\in V$ under M1 is (1,3,5). What is the coordinate of d under M2? Return the three coordinate values as a list.",
            "Answer": [
                2,
                3,
                4
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "linear algebra 2.6 example 2",
            "id": "mingyin/gaussian-elimination3.json",
            "explanation": "NONE",
            "theorem": "gaussian elimination",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer"
    },
    {
        "id": "xinyi/linear_projection.json",
        "question": "Which of the following matrices takes any vector $v$ and projects it onto the space spanned by the columns of $\\Phi$? (a) $(\\Phi^T\\Phi)^{-1}$. (b) $\\Phi(\\Phi^T\\Phi)^{-1}$. (c) $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$. (d) $\\Phi^T(\\Phi^T\\Phi)^{-1}\\Phi^T$.",
        "prediction": "(c) $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhich of the following matrices takes any vector $v$ and projects it onto the space spanned by the columns of $\\Phi$? (a) $(\\Phi^T\\Phi)^{-1}$. (b) $\\Phi(\\Phi^T\\Phi)^{-1}$. (c) $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$. (d) $\\Phi^T(\\Phi^T\\Phi)^{-1}\\Phi^T$.\n\nDocument 1:\n----\nare _compatible_.) Then \\(C=AB\\) can be expressed as the \\(m\\times n\\) block matrix with entries \\(C_{ij}\\), given by the formula (10.1). For example, we have\n\\[\\left[\\begin{array}{cc}A&B\\\\ C&D\\end{array}\\right]\\left[\\begin{array}{cc}E&F\\\\ G&H\\end{array}\\right]=\\left[\\begin{array}{cc}AE+BG&AF+BH\\\\ CE+DG&CF+DH\\end{array}\\right],\\]\nfor any matrices \\(A,B,\\ldots,H\\) for which the matrix products above make sense. This formula is the same as the formula for multiplying two \\(2\\times 2\\) matrices (_i.e._, with scalar entries); but when the entries of the matrix are themselves matrices (as in the block matrix above), we must be careful to preserve the multiplication order.\nColumn interpretation of matrix-matrix product.We can derive some additional insight into matrix multiplication by interpreting the operation in terms of the columns of the second matrix. Consider the matrix product of an \\(m\\times p\\) matrix \\(A\\) and a \\(p\\times n\\) matrix \\(B\\), and denote the columns of \\(B\\) by \\(b_{k}\\). Using block-matrix notation, we can write the product \\(AB\\) as\n\\[AB=A\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\,= \\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right].\\]\nThus, the columns of \\(AB\\) are the matrix-vector products of \\(A\\) and the columns of \\(B\\). The product \\(AB\\) can be interpreted as the matrix obtained by 'applying' \\(A\\) to each of the columns of \\(B\\).\nMultiple sets of linear equations.We can use the column interpretation of matrix multiplication to express a set of \\(k\\) linear equations with the same \\(m\\times n\\) coefficient matrix \\(A\\),\n\\[Ax_{i}=b_{i},\\quad i=1,\\ldots,k,\\]\nin the compact form\n\\[AX=B,\\]\n\nDocument 2:\n----\n\\[A_{p:q,r:s}=\\left[\\begin{array}{cccc}A_{pr}&A_{p,r+1}&\\cdots&A_{ps}\\\\ A_{p+1,r}&A_{p+1,r+1}&\\cdots&A_{p+1,s}\\\\ \\vdots&\\vdots&&\\vdots\\\\ A_{qr}&A_{q,r+1}&\\cdots&A_{qs}\\end{array}\\right].\\]\nThis submatrix has size \\((q-p+1)\\times(s-r+1)\\) and is obtained by extracting from \\(A\\) the elements in rows \\(p\\) through \\(q\\) and columns \\(r\\) through \\(s\\).\nFor the specific matrix \\(A\\) in (6.1), we have\n\\[A_{2:3,3:4}=\\left[\\begin{array}{cc}1&4\\\\ 5&4\\end{array}\\right].\\]\nColumn and row representation of a matrix.Using block matrix notation we can write an \\(m\\times n\\) matrix \\(A\\) as a block matrix with one block row and \\(n\\) block columns,\n\\[A=\\left[\\begin{array}{cccc}a_{1}&a_{2}&\\cdots&a_{n}\\end{array}\\right],\\]\nwhere \\(a_{j}\\), which is an \\(m\\)-vector, is the \\(j\\)th column of \\(A\\). Thus, an \\(m\\times n\\) matrix can be viewed as its \\(n\\) columns, concatenated.\nSimilarly, an \\(m\\times n\\) matrix \\(A\\) can be written as a block matrix with one block column and \\(m\\) block rows:\n\\[A=\\left[\\begin{array}{c}b_{1}\\\\ b_{2}\\\\ \\vdots\\\\ b_{m}\\end{array}\\right],\\]\nwhere \\(b_{i}\\), which is a row \\(n\\)-vector, is the \\(i\\)th row of \\(A\\). In this notation, the matrix \\(A\\) is interpreted as its \\(m\\) rows, stacked.\n## Examples\nTable interpretation.The most direct interpretation of a matrix is as a table of numbers that depend on two indices, \\(i\\) and \\(j\\). (A vector is a list of numbers that depend on only one index.) In this case the rows and columns of the matrix usually have some simple interpretation. Some examples are given below.\n\nDocument 3:\n----\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\nInvertibility conditions.For square matrices, left-invertibility, right-invertibility, and invertibility are equivalent: If a matrix is square and left-invertible, then it is also right-invertible (and therefore invertible) and vice-versa.\nTo see this, suppose \\(A\\) is an \\(n\\times n\\) matrix and left-invertible. This implies that the \\(n\\) columns of \\(A\\) are linearly independent. Therefore they form a basis and so any \\(n\\)-vector can be expressed as a linear combination of the columns of \\(A\\). In particular, each of the \\(n\\) unit vectors \\(e_{i}\\) can be expressed as \\(e_{i}=Ab_{i}\\) for some \\(n\\)-vector \\(b_{i}\\). The matrix \\(B=\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\) satisfies\n\\[AB=\\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right]= \\left[\\begin{array}{cccc}e_{1}&e_{2}&\\cdots&e_{n}\\end{array}\\right]=I.\\]\nSo \\(B\\) is a right inverse of \\(A\\).\nWe have just shown that for a square matrix \\(A\\),\n\nDocument 4:\n----\nLeast squares model fitting.A very common method for choosing the model parameters \\(\\theta_{1},\\ldots,\\theta_{p}\\) is to minimize the RMS prediction error on the given data set, which is the same as minimizing the sum of squares of the prediction errors, \\(\\|r^{\\mathrm{d}}\\|^{2}\\). We now show that this is a least squares problem.\nExpressing \\(\\hat{y}^{(i)}=\\hat{f}(x^{(i)})\\) in terms of the model parameters, we have\n\\[\\hat{y}^{(i)}=A_{i1}\\theta_{1}+\\cdots+A_{ip}\\theta_{p},\\quad i=1,\\ldots,N,\\]\nwhere we define the \\(N\\times p\\) matrix \\(A\\) as\n\\[A_{ij}=\\hat{f}_{j}(x^{(i)}),\\quad i=1,\\ldots,N,\\quad j=1,\\ldots,p,\\] (13.1)\nand the \\(p\\)-vector \\(\\theta\\) as \\(\\theta=(\\theta_{1},\\ldots,\\theta_{p})\\). The \\(j\\)th column of \\(A\\) is the \\(j\\)th basis function, evaluated at each of the data points \\(x^{(1)},\\ldots,x^{(N)}\\). Its \\(i\\)th row gives the values of the \\(p\\) basis functions on the \\(i\\)th data point \\(x^{(i)}\\). In matrix-vector notation we have\n\\[\\hat{y}^{\\mathrm{d}}=A\\theta.\\]\nThis simple equation shows how our choice of model parameters maps into the vector of predicted values of the outcomes in the \\(N\\) different experiments. We know the matrix \\(A\\) from the given data points, and choice of basis functions; our goal is to choose the \\(p\\)-vector of model coefficients \\(\\theta\\).\nThe sum of squares of the residuals is then\n\\[\\|r^{\\mathrm{d}}\\|^{2}=\\|y^{\\mathrm{d}}-\\hat{y}^{\\mathrm{d}}\\|^{2}=\\|y^{ \\mathrm{d}}-A\\theta\\|^{2}=\\|A\\theta-y^{\\mathrm{d}}\\|^{2}.\\]\n(In the last step we use the fact that the norm of a vector is the same as the norm of its negative.) Choosing \\(\\theta\\) to minimize this is evidently a least squares problem, of the same form as (12.1). Provided the columns of \\(A\\) are linearly independent, we can solve this least squares problem to find \\(\\hat{\\theta}\\), the model parameter values that minimize the norm of the prediction error on our data set, as\n\nDocument 5:\n----\n## 16 Constrained least squares\n### 16.1 Smallest right inverse\nSuppose the \\(m\\times n\\) matrix \\(A\\) is wide, with linearly independent rows. Its pseudo-inverse \\(A^{\\dagger}\\) is a right inverse of \\(A\\). In fact, there are many right inverses of \\(A\\) and it turns out that \\(A^{\\dagger}\\) is the smallest one among them, as measured by the matrix norm. In other words, if \\(X\\) satisfies \\(AX=I\\), then \\(\\|X\\|\\geq\\|A^{\\dagger}\\|\\). You will show this in this problem.\n1. Suppose \\(AX=I\\), and let \\(x_{1},\\ldots,x_{m}\\) denote the columns of \\(X\\). Let \\(b_{j}\\) denote the \\(j\\)th column of \\(A^{\\dagger}\\). Explain why \\(\\|x_{j}\\|^{2}\\geq\\|b_{j}\\|^{2}\\). _Hint_. Show that \\(z=b_{j}\\) is the vector of smallest norm that satisfies \\(Az=e_{j}\\), for \\(j=1,\\ldots,m\\).\n2. Use the inequalities from part (a) to establish \\(\\|X\\|\\geq\\|A^{\\dagger}\\|\\).\n3. _Matrix least norm problem_. The matrix least norm problem is minimize \\[\\|X\\|^{2}\\] subject to \\[CX=D,\\] where the variable to be chosen is the \\(n\\times k\\) matrix \\(X\\); the \\(p\\times n\\) matrix \\(C\\) and the \\(p\\times k\\) matrix \\(D\\) are given. Show that the solution of this problem is \\(\\hat{X}=C^{\\dagger}D\\), assuming the rows of \\(C\\) are linearly independent. _Hint_. Show that we can find the columns of \\(X\\) independently, by solving a least norm problem for each one.\n4. _Closest solution to a given point_. Suppose the wide matrix \\(A\\) has linearly independent rows. Find an expression for the point \\(x\\) that is closest to a given vector \\(y\\) (_i.e._, minimizes \\(\\|x-y\\|^{2}\\)) among all vectors that satisfy \\(Ax=b\\). _Remark_. This problem comes up when \\(x\\) is some set of inputs to be found, \\(Ax=b\\) represents some set of requirements, and \\(y\\) is some nominal value of the inputs. For example, when the inputs represent actions that are re-calculated each day (say, because \\(b\\) changes every day), \\(y\\) might be yesterday's action, and the today's action \\(x\\) found as above gives the least change from yesterday's action, subject to meeting today's requirements.\n\n\n",
        "answer": "(c)",
        "rationale": {
            "Question": "Which of the following matrices takes any vector $v$ and projects it onto the space spanned by the columns of $\\Phi$? (a) $(\\Phi^T\\Phi)^{-1}$. (b) $\\Phi(\\Phi^T\\Phi)^{-1}$. (c) $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$. (d) $\\Phi^T(\\Phi^T\\Phi)^{-1}\\Phi^T$.",
            "Answer": "(c)",
            "Answer_type": "option",
            "Picture": null,
            "source": "textbook 3.2",
            "id": "xinyi/linear_projection.json",
            "explanation": "NONE",
            "theorem": "projection theory",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "option"
    }
]