[
    {
        "id": "elainewan/math_algebra_1.json",
        "question": "Find the x value of the solutions to the linear system: 7x - y = 15x, -6x + 8y = 15y.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFind the x value of the solutions to the linear system: 7x - y = 15x, -6x + 8y = 15y.\n\nDocument 1:\n----\nOver-determined and under-determined systems of linear equations.The set of linear equations is called _over-determined_ if \\(m>n\\), _under-determined_ if \\(m<n\\), and _square_ if \\(m=n\\); these correspond to the coefficient matrix being tall, wide, and square, respectively. When the system of linear equations is over-determined, there are more equations than variables or unknowns. When the system of linear equations is under-determined, there are more unknowns than equations. When the system of linear equations is square, the numbers of unknowns and equations is the same. A set of equations with zero right-hand side, \\(Ax=0\\), is called a _homogeneous_ set of equations. Any homogeneous set of equations has \\(x=0\\) as a solution.\nIn chapter 11 we will address the question of how to determine if a system of linear equations has a solution, and how to find one when it does. For now, we give a few interesting examples.\n#### Examples\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\n\nDocument 2:\n----\nIn summary, a right inverse can be used to find \\(a\\) solution of a square or under-determined set of linear equations, for any vector \\(b\\).\nExamples.Consider the matrix appearing in the example above on page 199,\n\\[A=\\left[\\begin{array}{rr}-3&-4\\\\ 4&6\\\\ 1&1\\end{array}\\right]\\]\nand the two left inverses\n\\[B=\\frac{1}{9}\\left[\\begin{array}{rr}-11&-10&16\\\\ 7&8&-11\\end{array}\\right],\\qquad C=\\frac{1}{2}\\left[\\begin{array}{rr}0&-1&6 \\\\ 0&1&-4\\end{array}\\right].\\]\n* The over-determined linear equations \\(Ax=(1,-2,0)\\) have the unique solution \\(x=(1,-1)\\), which can be obtained from _either_ left inverse: \\[x=B(1,-2,0)=C(1,-2,0).\\]\n* The over-determined linear equations \\(Ax=(1,-1,0)\\) do not have a solution, since \\(x=C(1,-1,0)=(1/2,-1/2)\\) does not satisfy \\(Ax=(1,-1,0)\\).\n* The under-determined linear equations \\(A^{T}y=(1,2)\\) has (different) solutions \\[B^{T}(1,2)=(1/3,2/3,38/9),\\qquad C^{T}(1,2)=(0,1/2,-1).\\] (Recall that \\(B^{T}\\) and \\(C^{T}\\) are both right inverses of \\(A^{T}\\).) We can find a solution of \\(A^{T}y=b\\) for any vector \\(b\\).\nLeft and right inverse of matrix product.Suppose \\(A\\) and \\(D\\) are compatible for the matrix product \\(AD\\) (_i.e._, the number of columns in \\(A\\) is equal to the number of rows in \\(D\\).) If \\(A\\) has a right inverse \\(B\\) and \\(D\\) has a right inverse \\(E\\), then \\(EB\\) is a right inverse of \\(AD\\). This follows from \\[(AD)(EB)=A(DE)B=A(IB)=AB=I.\\] If \\(A\\) has a left inverse \\(C\\) and \\(D\\) has a left inverse \\(F\\), then \\(FC\\) is a left inverse of \\(AD\\). This follows from \\[(FC)(AD)=F(CA)D=FD=I.\\]\n\n\n",
        "answer": 0,
        "rationale": {
            "Question": "Find the x value of the solutions to the linear system: 7x - y = 15x, -6x + 8y = 15y.",
            "Answer": 0,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_1.json",
            "explanation": "solutions/math_algebra_1.txt",
            "theorem": "linear systems",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_3.json",
        "question": "Is W = {[x, y] in R^2: x >= 0 and y >= 0} a subspace of R^2?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs W = {[x, y] in R^2: x >= 0 and y >= 0} a subspace of R^2?\n\nDocument 1:\n----\n## 17 Constrained least squares applications\n### 17.1 A variation on the portfolio optimization formulation\nConsider the following variation on the linearly constrained least squares problem (17.2):\n\\[\\begin{array}{ll}\\mbox{minimize}&\\|Rw\\|^{2}\\\\ \\mbox{subject to}&\\left[\\begin{array}{c}\\mathbf{1}^{T}\\\\ \\mu^{T}\\end{array}\\right]w=\\left[\\begin{array}{c}1\\\\ \\rho\\end{array}\\right],\\end{array}\\] (17.12)\nwith variable \\(w\\). (The difference is that here we drop the term \\(\\rho\\mathbf{1}\\) that appears inside the norm square objective in (17.2).) Show that this problem is equivalent to (17.2). This means \\(w\\) is a solution of (17.12) if and only if it is a solution of (17.2).\n_Hint._ You can argue directly by expanding the objective in (17.2) or via the KKT systems of the two problems.\n### 17.2 A more conventional formulation of the portfolio optimization problem\nIn this problem we derive an equivalent formulation of the portfolio optimization problem (17.2) that appears more frequently in the literature than our version. (Equivalent means that the two problems always have the same solution.) This formulation is based on the _return covariance matrix_, which we define below. (See also exercise 10.16.)\nThe means of the columns of the asset return matrix \\(R\\) are the entries of the vector \\(\\mu\\). The _de-meaned returns matrix_ is given by \\(\\tilde{R}=R-\\mathbf{1}\\mu^{T}\\). (The columns of the matrix \\(\\tilde{R}=R-\\mathbf{1}\\mu^{T}\\) are the de-meaned return time series for the assets.) The return covariance matrix, traditionally denoted \\(\\Sigma\\), is its Gram matrix \\(\\Sigma=(1/T)\\tilde{R}^{T}\\tilde{R}\\).\n\nDocument 2:\n----\n\\[\\tilde{R}^{T}\\tilde{R}w=2\\tilde{R}^{T}\\tilde{Q}^{T}Q_{1}^{T}b-2d,\\]\nwhich we can write as\n\\[\\tilde{R}w=2\\tilde{Q}^{T}Q_{1}^{T}b-2\\tilde{R}^{-T}d.\\]\nWe can use this to compute \\(w\\), first by computing \\(\\tilde{R}^{-T}d\\) (by forward substitution), then forming the right-hand side, and then solving for \\(w\\) using back substitution. Once we know \\(w\\), we can find \\(\\hat{x}\\) from (16.9). The method is summarized in the following algorithm.\n**Algorithm 16.2** Constrained least squares via QR factorization\nIn the unconstrained case (when \\(p=0\\)), step 1 reduces to computing the QR factorization of \\(A\\), steps 2 and 3 are not needed, and step 4 reduces to solving \\(R\\hat{x}=Q_{1}^{T}b\\). This is the same as algorithm 12.1 for solving (unconstrained) least squares problems.\nWe now give a complexity analysis. Step 1 involves the QR factorizations of an \\((m+p)\\times n\\) and an \\(n\\times p\\) matrix, which costs \\(2(m+p)n^{2}+2np^{2}\\) flops. Step 2 requires \\(p^{2}\\) flops. In step 3, we first evaluate \\(Q_{1}^{T}b\\) (\\(2mn\\) flops), multiply the result by \\(\\tilde{Q}^{T}\\) (\\(2pn\\) flops), and then solve for \\(w\\) using forward substitution (\\(p^{2}\\) flops). Step 4 requires \\(2mn+2pn\\) flops to form the right-hand side, and \\(n^{2}\\) flops to compute \\(\\hat{x}\\) via back substitution. The costs of steps 2, 3, and 4 are quadratic in the dimensions, and so are negligible compared to the cost of step 1, so our final complexity is\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is W = {[x, y] in R^2: x >= 0 and y >= 0} a subspace of R^2?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3.json",
            "explanation": "solutions/math_algebra_3.png",
            "theorem": "linear subspaces",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_3_6.json",
        "question": "Consider the basis B of R^2 consisting of vectors v_1 = [3,1] and v_2 = [-1, 3]. If x = [10, 10], find the B-coordinate vector of x",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider the basis B of R^2 consisting of vectors v_1 = [3,1] and v_2 = [-1, 3]. If x = [10, 10], find the B-coordinate vector of x\n\nDocument 1:\n----\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\\[l_{i}=\\left[\\begin{array}{c}0_{i-1}\\\\ 1\\\\ -(1+r)\\\\ 0_{n-i-1}\\end{array}\\right],\\quad i=1,\\ldots,n-1,\\]\nwhere \\(r\\geq 0\\) is the per-period interest rate. The cash flow \\(l_{i}\\) represents a loan of $1 in period \\(i\\), which is paid back in period \\(i+1\\) with interest \\(r\\). (The subscripts on the zero vectors above give their dimensions.) Scaling \\(l_{i}\\) changes the loan amount; scaling \\(l_{i}\\) by a negative coefficient converts it into a loan _to_ another entity (which is paid back in period \\(i+1\\) with interest).\n\nDocument 2:\n----\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\nNow suppose that \\(x\\) is any \\(n\\)-vector. It can be expressed as a linear combination of the basis vectors \\(a_{1},\\ldots,a_{n}\\):\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{n}a_{n}.\\]\nThe dual basis gives us a simple way to find the coefficients \\(\\beta_{1},\\ldots,\\beta_{n}\\).\nWe start with \\(AB=I\\), and multiply by \\(x\\) to get\n\\[x=ABx=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right]\\left[ \\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right]x=(b_{1}^{T}x)a_{1}+\\cdots+(b_{n}^{T}x)a_{n}.\\]\nThis means (since the vectors \\(a_{1},\\ldots,a_{n}\\) are linearly independent) that \\(\\beta_{i}=b_{i}^{T}x\\). In words: The coefficients in the expansion of a vector in a basis are given by the inner products with the dual basis vectors. Using matrix notation, we can say that \\(\\beta=B^{T}x=(A^{-1})^{T}x\\) is the vector of coefficients of \\(x\\) in the basis given by the columns of \\(A\\).\n\n\n",
        "answer": [
            4,
            2
        ],
        "rationale": {
            "Question": "Consider the basis B of R^2 consisting of vectors v_1 = [3,1] and v_2 = [-1, 3]. If x = [10, 10], find the B-coordinate vector of x",
            "Answer": [
                4,
                2
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_6.json",
            "explanation": "solutions/math_algebra_3_6.txt",
            "theorem": "basis",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_4_2.json",
        "question": "Let V be the space of all infinite sequences of real numbers. Consider the transformation T(x_0, x_1, x_2, ...) = (x_1, x_2, x_3, ...) from V to V. Is the sequence (1,2,3,...) in the image of T?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet V be the space of all infinite sequences of real numbers. Consider the transformation T(x_0, x_1, x_2, ...) = (x_1, x_2, x_3, ...) from V to V. Is the sequence (1,2,3,...) in the image of T?\n\nDocument 1:\n----\nIn this problem we consider several linear functions of a monochrome image with \\(N\\times N\\) pixels. To keep the matrices small enough to work out by hand, we will consider the case with \\(N=3\\) (which would hardly qualify as an image). We represent a \\(3\\times 3\\) image as a 9-vector using the ordering of pixels shown below.\n\\begin{tabular}{|c|c|c|} \\hline\n1 & 4 & 7 \\\\ \\hline\n2 & 5 & 8 \\\\ \\hline\n3 & 6 & 9 \\\\ \\hline \\end{tabular} (This ordering is called _column-major_.) Each of the operations or transformations below defines a function \\(y=f(x)\\), where the 9-vector \\(x\\) represents the original image, and the 9-vector \\(y\\) represents the resulting or transformed image. For each of these operations, give the \\(9\\times 9\\) matrix \\(A\\) for which \\(y=Ax\\).\n* Turn the original image \\(x\\) upside-down.\n* Rotate the original image \\(x\\) clockwise \\(90^{\\circ}\\).\n* Translate the image up by 1 pixel and to the right by 1 pixel. In the translated image, assign the value \\(y_{i}=0\\) to the pixels in the first column and the last row.\n* Set each pixel value \\(y_{i}\\) to be the average of the neighbors of pixel \\(i\\) in the original image. By neighbors, we mean the pixels immediately above and below, and immediately to the left and right. The center pixel has 4 neighbors; corner pixels have 2 neighbors, and the remaining pixels have 3 neighbors.\n\nDocument 2:\n----\nTriangular matrix.A triangular matrix with nonzero diagonal elements is invertible. We first discuss this for a lower triangular matrix. Let \\(L\\) be \\(n\\times n\\) and lower triangular with nonzero diagonal elements. We show that the columns are linearly independent, _i.e._, \\(Lx=0\\) is only possible if \\(x=0\\). Expanding the matrix-vector product, we can write \\(Lx=0\\) as\n\\[L_{11}x_{1} = 0\\] \\[L_{21}x_{1}+L_{22}x_{2} = 0\\] \\[L_{31}x_{1}+L_{32}x_{2}+L_{33}x_{3} = 0\\] \\[\\vdots\\] \\[L_{n1}x_{1}+L_{n2}x_{2}+\\cdots+L_{n,n-1}x_{n-1}+L_{nn}x_{n} = 0.\\]\nSince \\(L_{11}\\neq 0\\), the first equation implies \\(x_{1}=0\\). Using \\(x_{1}=0\\), the second equation reduces to \\(L_{22}x_{2}=0\\). Since \\(L_{22}\\neq 0\\), we conclude that \\(x_{2}=0\\). Using \\(x_{1}=x_{2}=0\\), the third equation now reduces to \\(L_{33}x_{3}=0\\), and since \\(L_{33}\\) is assumed to be nonzero, we have \\(x_{3}=0\\). Continuing this argument, we find that all entries of \\(x\\) are zero, and this shows that the columns of \\(L\\) are linearly independent. It follows that \\(L\\) is invertible.\nA similar argument can be followed to show that an upper triangular matrix with nonzero diagonal elements is invertible. One can also simply note that if \\(R\\) is upper triangular, then \\(L=R^{T}\\) is lower triangular with the same diagonal, and use the formula \\((L^{T})^{-1}=(L^{-1})^{T}\\) for the inverse of the transpose.\nInverse via QR factorization.The QR factorization gives a simple expression for the inverse of an invertible matrix. If \\(A\\) is square and invertible, its columns are linearly independent, so it has a QR factorization \\(A=QR\\). The matrix \\(Q\\) is orthogonal and \\(R\\) is upper triangular with positive diagonal entries. Hence \\(Q\\) and \\(R\\) are invertible, and the formula for the inverse product gives\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Let V be the space of all infinite sequences of real numbers. Consider the transformation T(x_0, x_1, x_2, ...) = (x_1, x_2, x_3, ...) from V to V. Is the sequence (1,2,3,...) in the image of T?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_4_2.json",
            "explanation": "solutions/math_algebra_4_2.png",
            "theorem": "image of linear transformations",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "mingyin/linear-dependence3.json",
        "question": "suppose the 10-by-10 matrix A has the form: if i \\neq j, A_{i,j}=a_i*b_j; if i=j,  A_{i,j}=1+a_i*b_j for all 1<=i,j<=10. Here a_i = 1/i, b_i=1/(i+1). Find the determinant of A. return the numeric.",
        "prediction": 1.2,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nsuppose the 10-by-10 matrix A has the form: if i \\neq j, A_{i,j}=a_i*b_j; if i=j,  A_{i,j}=1+a_i*b_j for all 1<=i,j<=10. Here a_i = 1/i, b_i=1/(i+1). Find the determinant of A. return the numeric.\n\nDocument 1:\n----\n### 15.5 Complexity\nFigure 15.12: RMS training and test errors as a function of the regularization parameter \\(\\lambda\\). _Bottom_. The regularization path. The dashed horizontal lines show the values of the coefficients used to generate the data.\n#### Gram caching\nWe start from the formula (15.3) for the minimizer of the weighted sum objective,\n\\[\\hat{x}=(\\lambda_{1}A_{1}^{T}A_{1}+\\cdots+\\lambda_{k}A_{k}^{T}A_{k})^{-1}( \\lambda_{1}A_{1}^{T}b_{1}+\\cdots+\\lambda_{k}A_{k}^{T}b_{k}).\\]\nThe matrix appearing in the inverse is a weighted sum of the Gram matrices \\(G_{i}=A_{i}^{T}A_{i}\\) associated with the matrices \\(A_{i}\\). We can compute \\(\\hat{x}\\) by forming these Gram matrices \\(G_{i}\\), along with the vectors \\(h_{i}=A_{i}^{T}b_{i}\\), then forming the weighted sums\n\\[G=\\lambda_{1}G_{1}+\\cdots+\\lambda_{k}G_{k},\\qquad h=\\lambda_{1}h_{1}+\\cdots+ \\lambda_{k}h_{k},\\]\nand finally, solving the \\(n\\times n\\) set of equations \\(G\\hat{x}=h\\). Forming \\(G_{i}\\) and \\(h_{i}\\) costs \\(m_{i}n^{2}\\) and \\(2m_{i}n\\) flops, respectively. (We save a factor of two in forming the Gram matrix; see page 182.) Ignoring the second term and adding over \\(i=1,\\ldots,k\\) we get a total of \\(mn^{2}\\) flops. Forming the weighted sums \\(G\\) and \\(h\\) costs \\(2kn^{2}\\) flops. Solving \\(G\\hat{x}=h\\) costs order \\(2n^{3}\\) flops.\n_Gram caching_ is the simple trick of computing \\(G_{i}\\) (and \\(h_{i}\\)) just once, and re-using these matrices and vectors for the \\(L\\) different choices of weights. This leads to a complexity of\n\\[mn^{2}+L(k+2n)n^{2}\\]\nflops. When \\(m\\) is much larger than \\(k+n\\), which is a common occurrence, this cost is smaller than \\(Lmn^{2}\\), the cost for the simple method.\n\nDocument 2:\n----\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n**10.11**: _Trace of matrix-matrix product_. The sum of the diagonal entries of a square matrix is called the _trace_ of the matrix, denoted \\(\\mathbf{tr}(A)\\).\n1. Suppose \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices. Show that \\[\\mathbf{tr}(A^{T}B)=\\sum_{i=1}^{m}\\sum_{j=1}^{n}A_{ij}B_{ij}.\\] What is the complexity of calculating \\(\\mathbf{tr}(A^{T}B)\\)?\n2. The number \\(\\mathbf{tr}(A^{T}B)\\) is sometimes referred to as the inner product of the matrices \\(A\\) and \\(B\\). (This allows us to extend concepts like angle to matrices.) Show that \\(\\mathbf{tr}(A^{T}B)=\\mathbf{tr}(B^{T}A)\\).\n3. Show that \\(\\mathbf{tr}(A^{T}A)=\\left\\|A\\right\\|^{2}\\). In other words, the square of the norm of a matrix is the trace of its Gram matrix.\n\n\n",
        "answer": 1.9,
        "rationale": {
            "Question": "suppose the 10-by-10 matrix A has the form: if i \\neq j, A_{i,j}=a_i*b_j; if i=j,  A_{i,j}=1+a_i*b_j for all 1<=i,j<=10. Here a_i = 1/i, b_i=1/(i+1). Find the determinant of A. return the numeric.",
            "Answer": 1.9,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 4.5 example 4",
            "id": "mingyin/linear-dependence3.json",
            "explanation": "NONE",
            "theorem": "linear dependence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_7_3.json",
        "question": "Does the following transformation have an eigenvector: Counterclockwise rotation through an angle of 45 degrees followed by a scaling by 2 in R^2.",
        "prediction": true,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nDoes the following transformation have an eigenvector: Counterclockwise rotation through an angle of 45 degrees followed by a scaling by 2 in R^2.\n\nDocument 1:\n----\nSuppose the 2-vector (or 3-vector) \\(x\\) represents a position in 2-D (or 3-D) space. Several important geometric transformations or mappings from points to points can be expressed as matrix-vector products \\(y=Ax\\), with \\(A\\) a \\(2\\times 2\\) (or \\(3\\times 3\\)) matrix. In the examples below, we consider the mapping from \\(x\\) to \\(y\\), and focus on the 2-D case (for which some of the matrices are simpler to describe).\nScaling.Scaling is the mapping \\(y=ax\\), where \\(a\\) is a scalar. This can be expressed as \\(y=Ax\\) with \\(A=aI\\). This mapping stretches a vector by the factor \\(|a|\\) (or shrinks it when \\(|a|<1\\)), and it flips the vector (reverses its direction) if \\(a<0\\).\nDilation.Dilation is the mapping \\(y=Dx\\), where \\(D\\) is a diagonal matrix, \\(D=\\mathbf{diag}(d_{1},d_{2})\\). This mapping stretches the vector \\(x\\) by different factors along the two different axes. (Or shrinks, if \\(|d_{i}|<1\\), and flips, if \\(d_{i}<0\\).)\nRotation.Suppose that \\(y\\) is the vector obtained by rotating \\(x\\) by \\(\\theta\\) radians counterclockwise. Then we have\n\\[y=\\left[\\begin{array}{cc}\\cos\\theta&-\\sin\\theta\\\\ \\sin\\theta&\\cos\\theta\\end{array}\\right]x.\\] (7.1)\nThis matrix is called (for obvious reasons) a _rotation matrix_.\nReflection.Suppose that \\(y\\) is the vector obtained by reflecting \\(x\\) through the line that passes through the origin, inclined \\(\\theta\\) radians with respect to horizontal. Then we have\n\nDocument 2:\n----\n\\[y=\\left[\\begin{array}{cc}(1/2)(1+\\cos(2\\theta))&(1/2)\\sin(2\\theta)\\\\ (1/2)\\sin(2\\theta)&(1/2)(1-\\cos(2\\theta))\\end{array}\\right]x.\\]\nSome of these geometric transformations are illustrated in figure 7.1.\nFinding the matrix.When a geometric transformation is represented by matrix-vector multiplication (as in the examples above), a simple method to find the matrix is to find its columns. The \\(i\\)th column is the vector obtained by applying the transformation to \\(e_{i}\\). As a simple example consider clockwise rotation by \\(90^{\\circ}\\) in 2-D. Rotating the vector \\(e_{1}=(1,0)\\) by \\(90^{\\circ}\\) gives \\((0,-1)\\); rotating \\(e_{2}=(0,1)\\) by \\(90^{\\circ}\\) gives \\((1,0)\\). So rotation by \\(90^{\\circ}\\) is given by\n\\[y=\\left[\\begin{array}{cc}0&1\\\\ -1&0\\end{array}\\right]x.\\]\nChange of coordinates.In many applications multiple coordinate systems are used to describe locations or positions in 2-D or 3-D. For example in aerospace engineering we can describe a position using _earth-fixed_ coordinates or _body-fixed_ coordinates, where the body refers to an aircraft. Earth-fixed coordinates are with respect to a specific origin, with the three axes pointing East, North, and straight up, respectively. The origin of the body-fixed coordinates is a specific location on the aircraft (typically the center of gravity), and the three axes point forward (along the aircraft body), left (with respect to the aircraft body), and up (with respect to the aircraft body). Suppose the 3-vector \\(x^{\\text{body}}\\) describes a location using the body coordinates, and \\(x^{\\text{earth}}\\) describes the same location in earth-fixed coordinates. These are related by\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Does the following transformation have an eigenvector: Counterclockwise rotation through an angle of 45 degrees followed by a scaling by 2 in R^2.",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_3.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "wenhuchen/factor's_theory.json",
        "question": "Is x-1 a factor of 2*x^4+3*x^2-5x+7?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs x-1 a factor of 2*x^4+3*x^2-5x+7?\n\nDocument 1:\n----\nOver-determined and under-determined systems of linear equations.The set of linear equations is called _over-determined_ if \\(m>n\\), _under-determined_ if \\(m<n\\), and _square_ if \\(m=n\\); these correspond to the coefficient matrix being tall, wide, and square, respectively. When the system of linear equations is over-determined, there are more equations than variables or unknowns. When the system of linear equations is under-determined, there are more unknowns than equations. When the system of linear equations is square, the numbers of unknowns and equations is the same. A set of equations with zero right-hand side, \\(Ax=0\\), is called a _homogeneous_ set of equations. Any homogeneous set of equations has \\(x=0\\) as a solution.\nIn chapter 11 we will address the question of how to determine if a system of linear equations has a solution, and how to find one when it does. For now, we give a few interesting examples.\n#### Examples\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\n\nDocument 2:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\nBalancing chemical reactions.(See page 154 for background.) We consider the problem of balancing the chemical reaction\n\\[a_{1}\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+a_{2}\\mathrm{Fe}^{2+}+a_{3}\\mathrm{H} ^{+}\\longrightarrow b_{1}\\mathrm{Cr}^{3+}+b_{2}\\mathrm{Fe}^{3+}+b_{3} \\mathrm{H}_{2}\\mathrm{O},\\]\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is x-1 a factor of 2*x^4+3*x^2-5x+7?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "self",
            "id": "wenhuchen/factor's_theory.json",
            "explanation": "NONE",
            "theorem": "factor's theorem",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "wenhuchen/cramer's_rule1.json",
        "question": "For the two linear equations $2 * x + 3 * y = 10$ and $4 * x + 4 * y = 12$ iwth variables x and y. Use cramer's rule to solve these two variables.",
        "prediction": 4,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor the two linear equations $2 * x + 3 * y = 10$ and $4 * x + 4 * y = 12$ iwth variables x and y. Use cramer's rule to solve these two variables.\n\nDocument 1:\n----\nIn summary, a right inverse can be used to find \\(a\\) solution of a square or under-determined set of linear equations, for any vector \\(b\\).\nExamples.Consider the matrix appearing in the example above on page 199,\n\\[A=\\left[\\begin{array}{rr}-3&-4\\\\ 4&6\\\\ 1&1\\end{array}\\right]\\]\nand the two left inverses\n\\[B=\\frac{1}{9}\\left[\\begin{array}{rr}-11&-10&16\\\\ 7&8&-11\\end{array}\\right],\\qquad C=\\frac{1}{2}\\left[\\begin{array}{rr}0&-1&6 \\\\ 0&1&-4\\end{array}\\right].\\]\n* The over-determined linear equations \\(Ax=(1,-2,0)\\) have the unique solution \\(x=(1,-1)\\), which can be obtained from _either_ left inverse: \\[x=B(1,-2,0)=C(1,-2,0).\\]\n* The over-determined linear equations \\(Ax=(1,-1,0)\\) do not have a solution, since \\(x=C(1,-1,0)=(1/2,-1/2)\\) does not satisfy \\(Ax=(1,-1,0)\\).\n* The under-determined linear equations \\(A^{T}y=(1,2)\\) has (different) solutions \\[B^{T}(1,2)=(1/3,2/3,38/9),\\qquad C^{T}(1,2)=(0,1/2,-1).\\] (Recall that \\(B^{T}\\) and \\(C^{T}\\) are both right inverses of \\(A^{T}\\).) We can find a solution of \\(A^{T}y=b\\) for any vector \\(b\\).\nLeft and right inverse of matrix product.Suppose \\(A\\) and \\(D\\) are compatible for the matrix product \\(AD\\) (_i.e._, the number of columns in \\(A\\) is equal to the number of rows in \\(D\\).) If \\(A\\) has a right inverse \\(B\\) and \\(D\\) has a right inverse \\(E\\), then \\(EB\\) is a right inverse of \\(AD\\). This follows from \\[(AD)(EB)=A(DE)B=A(IB)=AB=I.\\] If \\(A\\) has a left inverse \\(C\\) and \\(D\\) has a left inverse \\(F\\), then \\(FC\\) is a left inverse of \\(AD\\). This follows from \\[(FC)(AD)=F(CA)D=FD=I.\\]\n\nDocument 2:\n----\n* A list consisting of a single vector is linearly dependent only if the vector is zero. It is linearly independent only if the vector is nonzero.\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\n\n\n",
        "answer": [
            -1,
            4
        ],
        "rationale": {
            "Question": "For the two linear equations $2 * x + 3 * y = 10$ and $4 * x + 4 * y = 12$ iwth variables x and y. Use cramer's rule to solve these two variables.",
            "Answer": [
                -1,
                4
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/cramer's_rule1.json",
            "explanation": "NONE",
            "theorem": "cramer's rule",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_6_3.json",
        "question": "What is the determinant of matrix [[0, 1, 2], [7, 8, 3], [6, 5, 4]]?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the determinant of matrix [[0, 1, 2], [7, 8, 3], [6, 5, 4]]?\n\nDocument 1:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\nDocument 2:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\n\n",
        "answer": -36,
        "rationale": {
            "Question": "What is the determinant of matrix [[0, 1, 2], [7, 8, 3], [6, 5, 4]]?",
            "Answer": -36,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_3.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "wenhuchen/eigen_value1.json",
        "question": "For matrix A = [[5, 4], [1, 2]], what are its eigen values?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[5, 4], [1, 2]], what are its eigen values?\n\nDocument 1:\n----\n* \\(A\\), \\(B\\), and \\(C\\) have the same number of rows.\n* \\(B\\) is a tall matrix.\n* _Block matrix notation_. Consider the block matrix \\[A=\\left[\\begin{array}{ccc}I&B&0\\\\ B^{T}&0&0\\\\ 0&0&BB^{T}\\end{array}\\right],\\] where \\(B\\) is \\(10\\times 5\\). What are the dimensions of the four zero matrices and the identity matrix in the definition of \\(A\\)? What are the dimensions of \\(A\\)?\n* _When is the outer product symmetric?_ Let \\(a\\) and \\(b\\) be \\(n\\)-vectors. The inner product is symmetric, _i.e._, we have \\(a^{T}b=b^{T}a\\). The outer product of the two vectors is generally _not_ symmetric; that is, we generally have \\(ab^{T}\\neq ba^{T}\\). What are the conditions on \\(a\\) and \\(b\\) under which \\(ab=ba^{T}\\)? You can assume that all the entries of \\(a\\) and \\(b\\) are nonzero. (The conclusion you come to will hold even when some entries of \\(a\\) or \\(b\\) are zero.) _Hint_. Show that \\(ab^{T}=ba^{T}\\) implies that \\(a_{i}/b_{i}\\) is a constant (_i.e._, independent of \\(i\\)).\n* _Product of rotation matrices_. Let \\(A\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\theta\\) radians, defined in (7.1), and let \\(B\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\omega\\) radians. Show that \\(AB\\) is also a rotation matrix, and give the angle by which it rotates vectors. Verify that \\(AB=BA\\) in this case, and give a simple English explanation.\n\nDocument 2:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\n\n",
        "answer": [
            1,
            6
        ],
        "rationale": {
            "Question": "For matrix A = [[5, 4], [1, 2]], what are its eigen values?",
            "Answer": [
                1,
                6
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/eigen_value1.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_5.json",
        "question": "Find the orthogonal projection of 9e_1 onto the subspace of R^4 spanned by [2, 2, 1, 0] and [-2, 2, 0, 1].",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFind the orthogonal projection of 9e_1 onto the subspace of R^4 spanned by [2, 2, 1, 0] and [-2, 2, 0, 1].\n\nDocument 1:\n----\n4. Express \\(2x_{1}^{2}-3x_{1}x_{2}-x_{2}^{2}\\) as a quadratic form, with symmetric coefficient matrix \\(A\\).\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n**10.38**: _Orthogonal matrix with nonnegative entries._ Suppose the \\(n\\times n\\) matrix \\(A\\) is orthogonal, and all of its entries are nonnegative, _i.e._, \\(A_{ij}\\geq 0\\) for \\(i,j=1,\\ldots,n\\). Show that \\(A\\) must be a permutation matrix, _i.e._, each entry is either \\(0\\) or \\(1\\), each row has exactly one entry with value one, and each column has exactly one entry with value one. (See page 132.)\n**10.39**: _Gram matrix and QR factorization._ Suppose the matrix \\(A\\) has linearly independent columns and QR factorization \\(A=QR\\). What is the relationship between the Gram matrix of \\(A\\) and the Gram matrix of \\(R\\)? What can you say about the angles between the columns of \\(A\\) and the angles between the columns of \\(R\\)?\n\nDocument 2:\n----\nExample.We define three vectors\n\\[a_{1}=(-1,1,-1,1),\\qquad a_{2}=(-1,3,-1,3),\\qquad a_{3}=(1,3,5,7).\\]\nApplying the Gram-Schmidt algorithm gives the following results.\n* \\(i=1\\). We have \\(\\|\\tilde{q}_{1}\\|=2\\), so \\[q_{1}=\\frac{1}{\\|\\tilde{q}_{1}\\|}\\tilde{q}_{1}=(-1/2,1/2,-1/2,1/2),\\] which is simply \\(a_{1}\\) normalized.\n* \\(i=2\\). We have \\(q_{1}^{T}a_{2}=4\\), so \\[\\tilde{q}_{2}=a_{2}-(q_{1}^{T}a_{2})q_{1}=\\left[\\begin{array}{c}-1\\\\ 3\\\\ -1\\\\ 3\\end{array}\\right]-4\\left[\\begin{array}{c}-1/2\\\\ 1/2\\\\ -1/2\\\\ 1/2\\end{array}\\right]=\\left[\\begin{array}{c}1\\\\ 1\\\\ 1\\\\ 1\\end{array}\\right],\\]\nwhich is indeed orthogonal to \\(q_{1}\\) (and \\(a_{1}\\)). It has norm \\(\\|\\tilde{q}_{2}\\|=2\\); normalizing it gives \\[q_{2}=\\frac{1}{\\|\\tilde{q}_{2}\\|}\\tilde{q}_{2}=(1/2,1/2,1/2,1/2).\\]\n* \\(i=3\\). We have \\(q_{1}^{T}a_{3}=2\\) and \\(q_{2}^{T}a_{3}=8\\), so \\[\\tilde{q}_{3} = a_{3}-(q_{1}^{T}a_{3})q_{1}-(q_{2}^{T}a_{3})q_{2}\\] \\[= \\left[\\begin{array}{c}1\\\\ 3\\\\ 5\\\\ 7\\end{array}\\right]-2\\left[\\begin{array}{c}-1/2\\\\ 1/2\\\\ -1/2\\\\ 1/2\\end{array}\\right]-8\\left[\\begin{array}{c}1/2\\\\ 1/2\\\\ 1/2\\\\ 1/2\\end{array}\\right]\\] \\[= \\left[\\begin{array}{c}-2\\\\ -2\\\\ 2\\\\ 2\\end{array}\\right],\\] which is orthogonal to \\(q_{1}\\) and \\(q_{2}\\) (and \\(a_{1}\\) and \\(a_{2}\\)). We have \\(\\|\\tilde{q}_{3}\\|=4\\), so the normalized vector is \\[q_{3}=\\frac{1}{\\|\\tilde{q}_{3}\\|}\\tilde{q}_{3}=(-1/2,-1/2,1/2,1/2).\\]\nCompletion of the Gram-Schmidt algorithm without early termination tells us that the vectors \\(a_{1}\\), \\(a_{2}\\), \\(a_{3}\\) are linearly independent.\nDetermining if a vector is a linear combination of linearly independent vectors.Suppose the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, and we wish to determine if another vector \\(b\\) is a linear combination of them. (We have already noted on page 91 that if it is a linear combination of them, the coefficients are unique.) The Gram-Schmidt algorithm provides an explicit way to do this. We apply the Gram-Schmidt algorithm to the list of \\(k+1\\) vectors\n\\[a_{1},\\ldots,a_{k},b.\\]\nThese vectors are linearly dependent if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); they are linearly independent if \\(b\\) is not a linear combination of \\(a_{1},\\ldots,a_{k}\\). The Gram-Schmidt algorithm will determine which of these two cases holds. It cannot terminate in the first \\(k\\) steps, since we assume that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. It will terminate in the \\((k+1)\\)st step with \\(\\tilde{q}_{k+1}=0\\) if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\). It will not terminate in the \\((k+1)\\)st step (_i.e._, \\(\\tilde{q}_{k+1}\\neq 0\\)), otherwise.\n\n\n",
        "answer": [
            8,
            0,
            2,
            -2
        ],
        "rationale": {
            "Question": "Find the orthogonal projection of 9e_1 onto the subspace of R^4 spanned by [2, 2, 1, 0] and [-2, 2, 0, 1].",
            "Answer": [
                8,
                0,
                2,
                -2
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_5.json",
            "explanation": "NONE",
            "theorem": "projection theory",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_6.json",
        "question": "For a matrix A, is the function F(A) = det A from the linear space R^{3*3} to R a linear transformation?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor a matrix A, is the function F(A) = det A from the linear space R^{3*3} to R a linear transformation?\n\nDocument 1:\n----\nholds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). It is a good exercise to parse this simple looking equation, since it involves overloading of notation. On the left-hand side, the scalar-vector multiplications \\(\\alpha x\\) and \\(\\beta y\\) involve \\(n\\)-vectors, and the sum \\(\\alpha x+\\beta y\\) is the sum of two \\(n\\)-vectors. The function \\(f\\) maps \\(n\\)-vectors to \\(m\\)-vectors, so \\(f(\\alpha x+\\beta y)\\) is an \\(m\\)-vector. On the right-hand side, the scalar-vector multiplications and the sum are those for \\(m\\)-vectors. Finally, the equality sign is equality between two \\(m\\)-vectors.\nWe can verify that superposition holds for \\(f\\) using properties of matrix-vector and scalar-vector multiplication:\n\\[f(\\alpha x+\\beta y) = A(\\alpha x+\\beta y)\\] \\[= A(\\alpha x)+A(\\beta y)\\] \\[= \\alpha(Ax)+\\beta(Ay)\\] \\[= \\alpha f(x)+\\beta f(y)\\]\nThus we can associate with every matrix \\(A\\) a linear function \\(f(x)=Ax\\).\nThe converse is also true. Suppose \\(f\\) is a function that maps \\(n\\)-vectors to \\(m\\)-vectors, and is linear, _i.e._, (8.1) holds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). Then there exists an \\(m\\times n\\) matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\). This can be shown in the same way as for scalar-valued functions in SS2.1, by showing that if \\(f\\) is linear, then\n\\[f(x)=x_{1}f(e_{1})+x_{2}f(e_{2})+\\cdots+x_{n}f(e_{n}),\\] (8.2)\nwhere \\(e_{k}\\) is the \\(k\\)th unit vector of size \\(n\\). The right-hand side can also be written as a matrix-vector product \\(Ax\\), with\n\\[A=\\left[\\begin{array}{cccc}f(e_{1})&f(e_{2})&\\cdots&f(e_{n})\\end{array} \\right].\\]\n\nDocument 2:\n----\nwhere \\(e_{k}\\) is the \\(k\\)th unit vector of size \\(n\\). The right-hand side can also be written as a matrix-vector product \\(Ax\\), with\n\\[A=\\left[\\begin{array}{cccc}f(e_{1})&f(e_{2})&\\cdots&f(e_{n})\\end{array} \\right].\\]\nThe expression (8.2) is the same as (2.3), but here \\(f(x)\\) and \\(f(e_{k})\\) are vectors. The implications are exactly the same: A linear vector-valued function \\(f\\) is completely characterized by evaluating \\(f\\) at the \\(n\\) unit vectors \\(e_{1},\\ldots,e_{n}\\).\nAs in SS2.1 it is easily shown that the matrix-vector representation of a linear function is unique. If \\(f:{\\bf R}^{n}\\to{\\bf R}^{m}\\) is a linear function, then there exists exactly one matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\).\nExamples of linear functions.In the examples below we define functions \\(f\\) that map \\(n\\)-vectors \\(x\\) to \\(n\\)-vectors \\(f(x)\\). Each function is described in words, in terms of its effect on an arbitrary \\(x\\). In each case we give the associated matrix multiplication representation.\n* _Negation._\\(f\\) changes the sign of \\(x\\): \\(f(x)=-x\\). Negation can be expressed as \\(f(x)=Ax\\) with \\(A=-I\\).\n* _Reversal._\\(f\\) reverses the order of the elements of \\(x\\): \\(f(x)=(x_{n},x_{n-1},\\ldots,x_{1})\\). The reversal function can be expressed as \\(f(x)=Ax\\) with \\[A=\\left[\\begin{array}{cccc}0&\\cdots&0&1\\\\ 0&\\cdots&1&0\\\\ \\vdots&\\cdot\\cdot\\cdot&\\vdots&\\vdots\\\\ 1&\\cdots&0&0\\end{array}\\right].\\] (This is the \\(n\\times n\\) identity matrix with the order of its columns reversed. It is the _reverser matrix_ introduced in SS7.2.)\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "For a matrix A, is the function F(A) = det A from the linear space R^{3*3} to R a linear transformation?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_3_4.json",
        "question": "Are the vectors [1, 2], [2, 3], and [3, 4] linearly independent?",
        "prediction": true,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nAre the vectors [1, 2], [2, 3], and [3, 4] linearly independent?\n\nDocument 1:\n----\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\\[x=\\gamma_{1}a_{1}+\\cdots+\\gamma_{k}a_{k},\\]\nthen \\(\\beta_{i}=\\gamma_{i}\\) for \\(i=1,\\ldots,k\\). This tells us that, in principle at least, we can find the coefficients that form a vector \\(x\\) as a linear combination of linearly independent vectors.\nTo see this, we subtract the two equations above to get\n\\[0=(\\beta_{1}-\\gamma_{1})a_{1}+\\cdots+(\\beta_{k}-\\gamma_{k})a_{k}.\\]\nSince \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we conclude that \\(\\beta_{i}-\\gamma_{i}\\) are all zero.\nThe converse is also true: If each linear combination of a list of vectors can only be expressed as a linear combination with one set of coefficients, then the list of vectors is linearly independent. This gives a nice interpretation of linear independence: A list of vectors is linearly independent if and only if for any linear combination of them, we can infer or deduce the associated coefficients. (We will see later how to do this.)\nSupersets and subsets.If a collection of vectors is linearly dependent, then any superset of it is linearly dependent. In other words: If we add vectors to a linearly dependent collection of vectors, the new collection is also linearly dependent. Any nonempty subset of a linearly independent collection of vectors is linearly independent. In other words: Removing vectors from a collection of vectors preserves linear independence.\n\nDocument 2:\n----\n* \\(i=3\\). We have \\(q_{1}^{T}a_{3}=2\\) and \\(q_{2}^{T}a_{3}=8\\), so \\[\\tilde{q}_{3} = a_{3}-(q_{1}^{T}a_{3})q_{1}-(q_{2}^{T}a_{3})q_{2}\\] \\[= \\left[\\begin{array}{c}1\\\\ 3\\\\ 5\\\\ 7\\end{array}\\right]-2\\left[\\begin{array}{c}-1/2\\\\ 1/2\\\\ -1/2\\\\ 1/2\\end{array}\\right]-8\\left[\\begin{array}{c}1/2\\\\ 1/2\\\\ 1/2\\\\ 1/2\\end{array}\\right]\\] \\[= \\left[\\begin{array}{c}-2\\\\ -2\\\\ 2\\\\ 2\\end{array}\\right],\\] which is orthogonal to \\(q_{1}\\) and \\(q_{2}\\) (and \\(a_{1}\\) and \\(a_{2}\\)). We have \\(\\|\\tilde{q}_{3}\\|=4\\), so the normalized vector is \\[q_{3}=\\frac{1}{\\|\\tilde{q}_{3}\\|}\\tilde{q}_{3}=(-1/2,-1/2,1/2,1/2).\\]\nCompletion of the Gram-Schmidt algorithm without early termination tells us that the vectors \\(a_{1}\\), \\(a_{2}\\), \\(a_{3}\\) are linearly independent.\nDetermining if a vector is a linear combination of linearly independent vectors.Suppose the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, and we wish to determine if another vector \\(b\\) is a linear combination of them. (We have already noted on page 91 that if it is a linear combination of them, the coefficients are unique.) The Gram-Schmidt algorithm provides an explicit way to do this. We apply the Gram-Schmidt algorithm to the list of \\(k+1\\) vectors\n\\[a_{1},\\ldots,a_{k},b.\\]\nThese vectors are linearly dependent if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); they are linearly independent if \\(b\\) is not a linear combination of \\(a_{1},\\ldots,a_{k}\\). The Gram-Schmidt algorithm will determine which of these two cases holds. It cannot terminate in the first \\(k\\) steps, since we assume that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. It will terminate in the \\((k+1)\\)st step with \\(\\tilde{q}_{k+1}=0\\) if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\). It will not terminate in the \\((k+1)\\)st step (_i.e._, \\(\\tilde{q}_{k+1}\\neq 0\\)), otherwise.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Are the vectors [1, 2], [2, 3], and [3, 4] linearly independent?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_4.json",
            "explanation": "solutions/math_algebra_3_4.txt",
            "theorem": "linear independence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_3_2.json",
        "question": "Are the vectors v_1 = [1,2,3], v_2 = [4,5,6], v_3 = [7,8,9] linearly independent?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nAre the vectors v_1 = [1,2,3], v_2 = [4,5,6], v_3 = [7,8,9] linearly independent?\n\nDocument 1:\n----\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\\[x=\\gamma_{1}a_{1}+\\cdots+\\gamma_{k}a_{k},\\]\nthen \\(\\beta_{i}=\\gamma_{i}\\) for \\(i=1,\\ldots,k\\). This tells us that, in principle at least, we can find the coefficients that form a vector \\(x\\) as a linear combination of linearly independent vectors.\nTo see this, we subtract the two equations above to get\n\\[0=(\\beta_{1}-\\gamma_{1})a_{1}+\\cdots+(\\beta_{k}-\\gamma_{k})a_{k}.\\]\nSince \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we conclude that \\(\\beta_{i}-\\gamma_{i}\\) are all zero.\nThe converse is also true: If each linear combination of a list of vectors can only be expressed as a linear combination with one set of coefficients, then the list of vectors is linearly independent. This gives a nice interpretation of linear independence: A list of vectors is linearly independent if and only if for any linear combination of them, we can infer or deduce the associated coefficients. (We will see later how to do this.)\nSupersets and subsets.If a collection of vectors is linearly dependent, then any superset of it is linearly dependent. In other words: If we add vectors to a linearly dependent collection of vectors, the new collection is also linearly dependent. Any nonempty subset of a linearly independent collection of vectors is linearly independent. In other words: Removing vectors from a collection of vectors preserves linear independence.\n\nDocument 2:\n----\n* \\(i=3\\). We have \\(q_{1}^{T}a_{3}=2\\) and \\(q_{2}^{T}a_{3}=8\\), so \\[\\tilde{q}_{3} = a_{3}-(q_{1}^{T}a_{3})q_{1}-(q_{2}^{T}a_{3})q_{2}\\] \\[= \\left[\\begin{array}{c}1\\\\ 3\\\\ 5\\\\ 7\\end{array}\\right]-2\\left[\\begin{array}{c}-1/2\\\\ 1/2\\\\ -1/2\\\\ 1/2\\end{array}\\right]-8\\left[\\begin{array}{c}1/2\\\\ 1/2\\\\ 1/2\\\\ 1/2\\end{array}\\right]\\] \\[= \\left[\\begin{array}{c}-2\\\\ -2\\\\ 2\\\\ 2\\end{array}\\right],\\] which is orthogonal to \\(q_{1}\\) and \\(q_{2}\\) (and \\(a_{1}\\) and \\(a_{2}\\)). We have \\(\\|\\tilde{q}_{3}\\|=4\\), so the normalized vector is \\[q_{3}=\\frac{1}{\\|\\tilde{q}_{3}\\|}\\tilde{q}_{3}=(-1/2,-1/2,1/2,1/2).\\]\nCompletion of the Gram-Schmidt algorithm without early termination tells us that the vectors \\(a_{1}\\), \\(a_{2}\\), \\(a_{3}\\) are linearly independent.\nDetermining if a vector is a linear combination of linearly independent vectors.Suppose the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, and we wish to determine if another vector \\(b\\) is a linear combination of them. (We have already noted on page 91 that if it is a linear combination of them, the coefficients are unique.) The Gram-Schmidt algorithm provides an explicit way to do this. We apply the Gram-Schmidt algorithm to the list of \\(k+1\\) vectors\n\\[a_{1},\\ldots,a_{k},b.\\]\nThese vectors are linearly dependent if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); they are linearly independent if \\(b\\) is not a linear combination of \\(a_{1},\\ldots,a_{k}\\). The Gram-Schmidt algorithm will determine which of these two cases holds. It cannot terminate in the first \\(k\\) steps, since we assume that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. It will terminate in the \\((k+1)\\)st step with \\(\\tilde{q}_{k+1}=0\\) if \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\). It will not terminate in the \\((k+1)\\)st step (_i.e._, \\(\\tilde{q}_{k+1}\\neq 0\\)), otherwise.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Are the vectors v_1 = [1,2,3], v_2 = [4,5,6], v_3 = [7,8,9] linearly independent?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_2.json",
            "explanation": "solutions/math_algebra_3_2.png",
            "theorem": "linear independence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_additional_1.json",
        "question": "Let f_1, ..., f_n be polynomials. Do they span the space P of all polynomials?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet f_1, ..., f_n be polynomials. Do they span the space P of all polynomials?\n\nDocument 1:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\nBalancing chemical reactions.(See page 154 for background.) We consider the problem of balancing the chemical reaction\n\\[a_{1}\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+a_{2}\\mathrm{Fe}^{2+}+a_{3}\\mathrm{H} ^{+}\\longrightarrow b_{1}\\mathrm{Cr}^{3+}+b_{2}\\mathrm{Fe}^{3+}+b_{3} \\mathrm{H}_{2}\\mathrm{O},\\]\n\nDocument 2:\n----\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Let f_1, ..., f_n be polynomials. Do they span the space P of all polynomials?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "self",
            "id": "elainewan/math_algebra_additional_1.json",
            "explanation": "NONE",
            "theorem": "linear span",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_6_5.json",
        "question": "What is the determinant of the matrix A = [[1, 0, 0, 0, 0, 0], [2, 7, 0, 0, 0, 0], [3, 8, 6, 0, 0, 0], [4, 9, 5, 2, 1, 4], [5, 8, 4, 0, 2, 5], [6, 7, 3, 0, 3, 6]]?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the determinant of the matrix A = [[1, 0, 0, 0, 0, 0], [2, 7, 0, 0, 0, 0], [3, 8, 6, 0, 0, 0], [4, 9, 5, 2, 1, 4], [5, 8, 4, 0, 2, 5], [6, 7, 3, 0, 3, 6]]?\n\nDocument 1:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\nDocument 2:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\n\n",
        "answer": -252,
        "rationale": {
            "Question": "What is the determinant of the matrix A = [[1, 0, 0, 0, 0, 0], [2, 7, 0, 0, 0, 0], [3, 8, 6, 0, 0, 0], [4, 9, 5, 2, 1, 4], [5, 8, 4, 0, 2, 5], [6, 7, 3, 0, 3, 6]]?",
            "Answer": -252,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_5.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "wenhuchen/cramer's_rule2.json",
        "question": "For the two linear equations $2 * x + 3 * y + z = 8$ and $4 * x + 4 * y + 4z = 12$ and $x + y + 8z = 10$ with variables x, y and z. Use cramer's rule to solve these three variables.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor the two linear equations $2 * x + 3 * y + z = 8$ and $4 * x + 4 * y + 4z = 12$ and $x + y + 8z = 10$ with variables x, y and z. Use cramer's rule to solve these three variables.\n\nDocument 1:\n----\nIn summary, a right inverse can be used to find \\(a\\) solution of a square or under-determined set of linear equations, for any vector \\(b\\).\nExamples.Consider the matrix appearing in the example above on page 199,\n\\[A=\\left[\\begin{array}{rr}-3&-4\\\\ 4&6\\\\ 1&1\\end{array}\\right]\\]\nand the two left inverses\n\\[B=\\frac{1}{9}\\left[\\begin{array}{rr}-11&-10&16\\\\ 7&8&-11\\end{array}\\right],\\qquad C=\\frac{1}{2}\\left[\\begin{array}{rr}0&-1&6 \\\\ 0&1&-4\\end{array}\\right].\\]\n* The over-determined linear equations \\(Ax=(1,-2,0)\\) have the unique solution \\(x=(1,-1)\\), which can be obtained from _either_ left inverse: \\[x=B(1,-2,0)=C(1,-2,0).\\]\n* The over-determined linear equations \\(Ax=(1,-1,0)\\) do not have a solution, since \\(x=C(1,-1,0)=(1/2,-1/2)\\) does not satisfy \\(Ax=(1,-1,0)\\).\n* The under-determined linear equations \\(A^{T}y=(1,2)\\) has (different) solutions \\[B^{T}(1,2)=(1/3,2/3,38/9),\\qquad C^{T}(1,2)=(0,1/2,-1).\\] (Recall that \\(B^{T}\\) and \\(C^{T}\\) are both right inverses of \\(A^{T}\\).) We can find a solution of \\(A^{T}y=b\\) for any vector \\(b\\).\nLeft and right inverse of matrix product.Suppose \\(A\\) and \\(D\\) are compatible for the matrix product \\(AD\\) (_i.e._, the number of columns in \\(A\\) is equal to the number of rows in \\(D\\).) If \\(A\\) has a right inverse \\(B\\) and \\(D\\) has a right inverse \\(E\\), then \\(EB\\) is a right inverse of \\(AD\\). This follows from \\[(AD)(EB)=A(DE)B=A(IB)=AB=I.\\] If \\(A\\) has a left inverse \\(C\\) and \\(D\\) has a left inverse \\(F\\), then \\(FC\\) is a left inverse of \\(AD\\). This follows from \\[(FC)(AD)=F(CA)D=FD=I.\\]\n\nDocument 2:\n----\n**8.13**: _Portfolio sector exposures._ (See exercise 1.14.) The \\(n\\)-vector \\(h\\) denotes a portfolio of investments in \\(n\\) assets, with \\(h_{i}\\) the dollar value invested in asset \\(i\\). We consider a set of \\(m\\) industry sectors, such as pharmaceuticals or consumer electronics. Each asset is assigned to one of these sectors. (More complex models allow for an asset to be assigned to more than one sector.) The _exposure_ of the portfolio to sector \\(i\\) is defined as the sum of investments in the assets in that sector. We denote the sector exposures using the \\(m\\)-vector \\(s\\), where \\(s_{i}\\) is the portfolio exposure to sector \\(i\\). (When \\(s_{i}=0\\), the portfolio is said to be _neutral_ to sector \\(i\\).) An investment advisor specifies a set of desired sector exposures, given as the \\(m\\)-vector \\(s^{\\rm des}\\). Express the requirement \\(s=s^{\\rm des}\\) as a set of linear equations of the form \\(Ah=b\\). (You must describe the matrix \\(A\\) and the vector \\(b\\).) _Remark._ A typical practical case involves \\(n=1000\\) assets and \\(m=50\\) sectors. An advisor might specify \\(s_{i}^{\\rm des}=0\\) if she does not have an opinion as how companies in that sector will do in the future; she might specify a positive value for \\(s_{i}^{\\rm des}\\) if she thinks the companies in that sector will do well (_i.e._, generate positive returns) in the future, and a negative value if she thinks they will do poorly.\n**8.14**: _Affine combinations of solutions of linear equations._ Consider the set of \\(m\\) linear equations in \\(n\\) variables \\(Ax=b\\), where \\(A\\) is an \\(m\\times n\\) matrix, \\(b\\) is an \\(m\\)-vector, and \\(x\\) is the \\(n\\)-vector of variables. Suppose that the \\(n\\)-vectors \\(z_{1},\\ldots,z_{k}\\) are solutions of this set of equations, _i.e._, satisfy \\(Az_{i}=b\\). Show that if the coefficients \\(\\alpha_{1},\\ldots,\\alpha_{k}\\) satisfy \\(\\alpha_{1}+\\cdots+\\alpha_{k}=1\\), then the affine combination\n\n\n",
        "answer": [
            -1,
            3,
            1
        ],
        "rationale": {
            "Question": "For the two linear equations $2 * x + 3 * y + z = 8$ and $4 * x + 4 * y + 4z = 12$ and $x + y + 8z = 10$ with variables x, y and z. Use cramer's rule to solve these three variables.",
            "Answer": [
                -1,
                3,
                1
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/cramer's_rule2.json",
            "explanation": "NONE",
            "theorem": "cramer's rule",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "jianyu_xu/integer_programming_1.json",
        "question": "30 students from 5 classes solved 40 math problems. Each student must answer at least one question. Every two students in the same class solved the same number of questions. The number of questions answered by any two students in different classes is also different. Question: What's maximum possible number of students who only answered one question?",
        "prediction": 5,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\n30 students from 5 classes solved 40 math problems. Each student must answer at least one question. Every two students in the same class solved the same number of questions. The number of questions answered by any two students in different classes is also different. Question: What's maximum possible number of students who only answered one question?\n\nDocument 1:\n----\n3. \\(S^{T}S\\).\n4. \\(SS^{T}\\).\n**10.18**: _Students, classes, and majors._ We consider \\(m\\) students, \\(n\\) classes, and \\(p\\) majors. Each student can be in any number of the classes (although we'd expect the number to range from 3 to 6), and can have any number of the majors (although the common values would be 0, 1, or 2). The data about the students' classes and majors are given by an \\(m\\times n\\) matrix \\(C\\) and an \\(m\\times p\\) matrix \\(M\\), where \\[C_{ij}=\\left\\{\\begin{array}{ll}1&\\text{student $i$ is in class $j$}\\\\ 0&\\text{student $i$ is not in class $j$},\\end{array}\\right.\\] and \\[M_{ij}=\\left\\{\\begin{array}{ll}1&\\text{student $i$ is in major $j$}\\\\ 0&\\text{student $i$ is not in major $j$}.\\end{array}\\right.\\] 1. Let \\(E\\) be the \\(n\\)-vector with \\(E_{i}\\) being the enrollment in class \\(i\\). Express \\(E\\) using matrix notation, in terms of the matrices \\(C\\) and \\(M\\). 2. Define the \\(n\\times p\\) matrix \\(S\\) where \\(S_{ij}\\) is the total number of students in class \\(i\\) with major \\(j\\). Express \\(S\\) using matrix notation, in terms of the matrices \\(C\\) and \\(M\\).\n**10.19**: _Student group membership._ Let \\(G\\in{\\bf R}^{m\\times n}\\) represent a contingency matrix of \\(m\\) students who are members of \\(n\\) groups:\n\\[G_{ij}=\\left\\{\\begin{array}{ll}1&\\mbox{student $i$ is in group $j$}\\\\ 0&\\mbox{student $i$ is not in group $j$.}\\end{array}\\right.\\]\n\nDocument 2:\n----\n1. What can you say about \\(\\psi(1,-1)\\)? Either give the value of \\(\\psi(1,-1)\\), or state that it cannot be determined.\n2. What can you say about \\(\\psi(2,-2)\\)? Either give the value of \\(\\psi(2,-2)\\), or state that it cannot be determined. Justify your answers.\n**2.6**: _Questionnaire scoring._ A questionnaire in a magazine has 30 questions, broken into two sets of 15 questions. Someone taking the questionnaire answers each question with 'Rarely', 'Sometimes', or 'Often'. The answers are recorded as a 30-vector \\(a\\), with \\(a_{i}=1,2,3\\) if question \\(i\\) is answered Rarely, Sometimes, or Often, respectively. The total score on a completed questionnaire is found by adding up 1 point for every question answered Sometimes and 2 points for every question answered Often on questions 1-15, and by adding 2 points and 4 points for those responses on questions 16-30. (Nothing is added to the score for Rarely responses.) Express the total score \\(s\\) in the form of an affine function \\(s=w^{T}a+v\\), where \\(w\\) is a 30-vector and \\(v\\) is a scalar (number).\n**2.7**: _General formula for affine functions._ Verify that formula (2.4) holds for any affine function \\(f:{\\bf R}^{n}\\to{\\bf R}\\). You can use the fact that \\(f(x)=a^{T}x+b\\) for some \\(n\\)-vector \\(a\\) and scalar \\(b\\).\n\n\n",
        "answer": 26,
        "rationale": {
            "Question": "30 students from 5 classes solved 40 math problems. Each student must answer at least one question. Every two students in the same class solved the same number of questions. The number of questions answered by any two students in different classes is also different. Question: What's maximum possible number of students who only answered one question?",
            "Answer": 26,
            "Answer_type": "integer",
            "Picture": null,
            "source": "self",
            "id": "jianyu_xu/integer_programming_1.json",
            "explanation": "NONE",
            "theorem": "integer programming",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "mingyin/gaussian-elimination1.json",
        "question": "for the matrix $A=(\\begin{array}{rrrrr} 1 & 2 & 3 & 4 & -3 \\1 & 2 & 0 & -5 & 1 \\2 & 4 & -3 & -19 & 6 \\3 & 6 & -3 & -24 & 7\\end{array})$, what is its row rank and column rank? return the two numbers as a list.",
        "prediction": [
            3,
            3
        ],
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nfor the matrix $A=(\\begin{array}{rrrrr} 1 & 2 & 3 & 4 & -3 \\1 & 2 & 0 & -5 & 1 \\2 & 4 & -3 & -19 & 6 \\3 & 6 & -3 & -24 & 7\\end{array})$, what is its row rank and column rank? return the two numbers as a list.\n\nDocument 1:\n----\nAn important attribute of a matrix is its _size_ or _dimensions_, _i.e._, the numbers of rows and columns. The matrix above has 3 rows and 4 columns, so its size is \\(3\\times 4\\). A matrix of size \\(m\\times n\\) is called an \\(m\\times n\\) matrix.\nThe _elements_ (or _entries_ or _coefficients_) of a matrix are the values in the array. The \\(i,j\\) element is the value in the \\(i\\)th row and \\(j\\)th column, denoted by double subscripts: the \\(i,j\\) element of a matrix \\(A\\) is denoted \\(A_{ij}\\) (or \\(A_{i,j}\\), when \\(i\\) or \\(j\\) is more than one digit or character). The positive integers \\(i\\) and \\(j\\) are called the (row and column) _indices_. If \\(A\\) is an \\(m\\times n\\) matrix, then the row index \\(i\\) runs from 1 to \\(m\\) and the column index \\(j\\) runs from 1 to \\(n\\). Row indices go from top to bottom, so row 1 is the top row and row \\(m\\) is the bottom row. Column indices go from left to right, so column 1 is the left column and column \\(n\\) is the right column.\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\n\nDocument 2:\n----\nAn important attribute of a matrix is its _size_ or _dimensions_, _i.e._, the numbers of rows and columns. The matrix above has 3 rows and 4 columns, so its size is \\(3\\times 4\\). A matrix of size \\(m\\times n\\) is called an \\(m\\times n\\) matrix.\nThe _elements_ (or _entries_ or _coefficients_) of a matrix are the values in the array. The \\(i,j\\) element is the value in the \\(i\\)th row and \\(j\\)th column, denoted by double subscripts: the \\(i,j\\) element of a matrix \\(A\\) is denoted \\(A_{ij}\\) (or \\(A_{i,j}\\), when \\(i\\) or \\(j\\) is more than one digit or character). The positive integers \\(i\\) and \\(j\\) are called the (row and column) _indices_. If \\(A\\) is an \\(m\\times n\\) matrix, then the row index \\(i\\) runs from 1 to \\(m\\) and the column index \\(j\\) runs from 1 to \\(n\\). Row indices go from top to bottom, so row 1 is the top row and row \\(m\\) is the bottom row. Column indices go from left to right, so column 1 is the left column and column \\(n\\) is the right column.\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\nTwo matrices are equal if they have the same size, and the corresponding entries are all equal. As with vectors, we normally deal with matrices with entries that\n\n\n",
        "answer": [
            2,
            2
        ],
        "rationale": {
            "Question": "for the matrix $A=(\\begin{array}{rrrrr} 1 & 2 & 3 & 4 & -3 \\1 & 2 & 0 & -5 & 1 \\2 & 4 & -3 & -19 & 6 \\3 & 6 & -3 & -24 & 7\\end{array})$, what is its row rank and column rank? return the two numbers as a list.",
            "Answer": [
                2,
                2
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "linear algebra 2.2 example 3",
            "id": "mingyin/gaussian-elimination1.json",
            "explanation": "NONE",
            "theorem": "gaussian elimination",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "wenhuchen/gauss_lemma.json",
        "question": "Is 7 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
        "prediction": true,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs 7 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.\n\nDocument 1:\n----\nThe constrained least squares problem can also be thought of as a limit of a bi-objective least squares problem, with primary objective \\(\\|Ax-b\\|^{2}\\) and secondary objective \\(\\|Cx-d\\|^{2}\\). Roughly speaking, we put infinite weight on the second objective, so that any nonzero value is unacceptable (which forces \\(x\\) to satisfy \\(Cx=d\\)). So we would expect (and it can be verified) that minimizing the weighted objective\n\\[\\|Ax-b\\|^{2}+\\lambda\\|Cx-d\\|^{2},\\]\nfor a very large value of \\(\\lambda\\) yields a vector close to a solution of the constrained least squares problem (16.1). We will encounter this idea again in chapter 19, when we consider the nonlinear constrained least squares problem.\nExample.In figure 16.1 we fit a _piecewise-polynomial_ function \\(\\hat{f}(x)\\) to a set of \\(N=140\\) points \\((x_{i},y_{i})\\) in the plane. The function \\(\\hat{f}(x)\\) is defined as\n\\[\\hat{f}(x)=\\left\\{\\begin{array}{ll}p(x)&x\\leq a\\\\ q(x)&x>a,\\end{array}\\right.\\]\nwith \\(a\\) given, and \\(p(x)\\) and \\(q(x)\\) polynomials of degree three or less,\n\\[p(x)=\\theta_{1}+\\theta_{2}x+\\theta_{3}x^{2}+\\theta_{4}x^{3},\\qquad q(x)=\\theta _{5}+\\theta_{6}x+\\theta_{7}x^{2}+\\theta_{8}x^{3}.\\]\nWe also impose the condition that \\(p(a)=q(a)\\) and \\(p^{\\prime}(a)=q^{\\prime}(a)\\), so that \\(\\hat{f}(x)\\) is continuous and has a continuous first derivative at \\(x=a\\). Suppose the \\(N\\) data\nFigure 16.1: Least squares fit of two cubic polynomials to 140 points, with continuity constraints \\(p(a)=q(a)\\) and \\(p^{\\prime}(a)=q^{\\prime}(a)\\).\npoints \\((x_{i},y_{i})\\) are numbered so that \\(x_{1},\\ldots,x_{M}\\leq a\\) and \\(x_{M+1},\\ldots,x_{N}>a\\). The sum of squares of the prediction errors is\n\\[\\sum_{i=1}^{M}(\\theta_{1}+\\theta_{2}x_{i}+\\theta_{3}x_{i}^{2}+\\theta_{4}x_{i}^{ 3}-y_{i})^{2}+\\sum_{i=M+1}^{N}(\\theta_{5}+\\theta_{6}x_{i}+\\theta_{7}x_{i}^{2}+ \\theta_{8}x_{i}^{3}-y_{i})^{2}.\\]\nThe conditions \\(p(a)-q(a)=0\\) and \\(p^{\\prime}(a)-q^{\\prime}(a)=0\\) are two linear equations\n\\[\\theta_{1}+\\theta_{2}a+\\theta_{3}a^{2}+\\theta_{4}a^{3}-\\theta_{5} -\\theta_{6}a-\\theta_{7}a^{2}-\\theta_{8}a^{3} = 0\\] \\[\\theta_{2}+2\\theta_{3}a+3\\theta_{4}a^{2}-\\theta_{6}-2\\theta_{7}a- 3\\theta_{8}a^{2} = 0.\\]\nWe can determine the coefficients \\(\\hat{\\theta}=(\\hat{\\theta}_{1},\\ldots,\\hat{\\theta}_{8})\\) that minimize the sum of squares of the prediction errors, subject to the continuity constraints, by solving a constrained least squares problem\n\nDocument 2:\n----\nwhere \\(\\lambda_{i}>0\\) are weights, and \\(J_{2},\\ldots,J_{p}\\) are the regularization terms. This is called _regularized inversion_ or _regularized estimation_. We may repeat this for several choices of the weights, and choose the best estimate for the particular application.\nTikhonov regularized inversion.Choosing \\(\\hat{x}\\) to minimize\n\\[\\|Ax-y\\|^{2}+\\lambda\\|x\\|^{2}\\]\nfor some choice of \\(\\lambda>0\\) is called _Tikhonov regularized inversion_, after the mathematician Andrey Tikhonov. Here we seek a guess \\(\\hat{x}\\) that is consistent with the measurements (_i.e._, \\(\\|A\\hat{x}-y\\|^{2}\\) is small), but not too big.\nThe stacked matrix in this case,\n\\[\\tilde{A}=\\left[\\begin{array}{c}A\\\\ \\sqrt{\\lambda}I\\end{array}\\right],\\]\nalways has linearly independent columns, without any assumption about \\(A\\), which can have any dimensions, and need not have linearly independent columns. To see this we note that \\(\\tilde{A}x=(Ax,\\sqrt{\\lambda}x)=0\\) implies that \\(\\sqrt{\\lambda}x=0\\), which implies \\(x=0\\). The Gram matrix associated with \\(\\tilde{A}\\),\n\\[\\tilde{A}^{T}\\tilde{A}=A^{T}A+\\lambda I,\\]\nis therefore always invertible (provided \\(\\lambda>0\\)). The Tikhonov regularized approximate solution is then\n\\[\\hat{x}=(A^{T}A+\\lambda I)^{-1}A^{T}b.\\]\nEqualization.The vector \\(x\\) represents a transmitted signal or message, consisting of \\(n\\) real values. The matrix \\(A\\) represents the mapping from the transmitted signal to what is received (called the _channel_); \\(y=Ax+v\\) includes noise as well as the action of the channel. Guessing what \\(x\\) is, given \\(y\\), can be thought of as un-doing the effects of the channel. In this context, estimation is called _equalization_.\n#### Estimating a periodic time series\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Is 7 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | http://mathonline.wikidot.com/example-questions-regarding-gauss-s-lemma",
            "id": "wenhuchen/gauss_lemma.json",
            "explanation": "NONE",
            "theorem": "gauss's lemma",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "mingyin/Sylveete-rank-inequality1.json",
        "question": "Both A, B are n-by-n matrices with rank(A)=n, rank(A*B)=0. What is rank(B)?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nBoth A, B are n-by-n matrices with rank(A)=n, rank(A*B)=0. What is rank(B)?\n\nDocument 1:\n----\nwhere \\(x_{j}\\) is the \\(j\\)th column of \\(X\\) and \\(b_{j}\\) is the \\(j\\)th column of \\(B\\). (Here we use the property that the square of the matrix norm is the sum of the squared norms of the columns of the matrix.) So the objective is a sum of \\(k\\) terms, with each term depending on only one column of \\(X\\). It follows that we can choose the columns \\(x_{j}\\) independently, each one by minimizing its associated term \\(\\|Ax_{j}-b_{j}\\|^{2}\\). Assuming that \\(A\\) has linearly independent columns, the solution is \\(\\hat{x}_{j}=A^{\\dagger}b_{j}\\). The solution of the matrix least squares problem is therefore\n\\[\\hat{X} = \\left[\\begin{array}{cccc}\\hat{x}_{1}&\\cdots&\\hat{x}_{k}\\end{array}\\right]\\] (12.12) \\[= \\left[\\begin{array}{cccc}A^{\\dagger}b_{1}&\\cdots&A^{\\dagger}b_{ k}\\end{array}\\right]\\] \\[= A^{\\dagger}\\left[\\begin{array}{cccc}b_{1}&\\cdots&b_{k}\\end{array}\\right]\\] \\[= A^{\\dagger}B.\\]\nThe very simple solution \\(\\hat{X}=A^{\\dagger}B\\) of the matrix least squares problem agrees with the solution of the ordinary least squares problem when \\(k=1\\) (as it must). Many software packages for linear algebra use the backslash operator \\(A\\backslash B\\) to denote \\(A^{\\dagger}B\\), but this is not standard mathematical notation.\nThe matrix least squares problem can be solved efficiently by exploiting the fact that algorithm 12.1 is another example of a factor-solve algorithm. To compute \\(\\hat{X}=A^{\\dagger}B\\) we carry out the QR factorization of \\(A\\) once; we carry out steps 2 and 3 of algorithm 12.1 for each of the \\(k\\) columns of \\(B\\). The total cost is \\(2mn^{2}+k(2mn+n^{2})\\) flops. When \\(k\\) is small compared to \\(n\\) this is roughly \\(2mn^{2}\\) flops, the same cost as solving a single least squares problem (_i.e._, one with a vector right-hand side).\n\nDocument 2:\n----\nThus, the columns of \\(AB\\) are the matrix-vector products of \\(A\\) and the columns of \\(B\\). The product \\(AB\\) can be interpreted as the matrix obtained by 'applying' \\(A\\) to each of the columns of \\(B\\).\nMultiple sets of linear equations.We can use the column interpretation of matrix multiplication to express a set of \\(k\\) linear equations with the same \\(m\\times n\\) coefficient matrix \\(A\\),\n\\[Ax_{i}=b_{i},\\quad i=1,\\ldots,k,\\]\nin the compact form\n\\[AX=B,\\]\nwhere \\(X=[x_{1}\\;\\cdots\\;x_{k}]\\) and \\(B=[b_{1}\\;\\cdots\\;b_{k}]\\). The matrix equation \\(AX=B\\) is sometimes called a _linear equation with matrix right-hand side_, since it looks like \\(Ax=b\\), but \\(X\\) (the variable) and \\(B\\) (the right-hand side) are now \\(n\\times k\\) matrices, instead of \\(n\\)-vectors (which are \\(n\\times 1\\) matrices).\nRow interpretation of matrix-matrix product.We can give an analogous row interpretation of the product \\(AB\\), by partitioning \\(A\\) and \\(AB\\) as block matrices with row vector blocks. Let \\(a_{1}^{T},\\ldots,a_{m}^{T}\\) be the rows of \\(A\\). Then we have\n\\[AB=\\left[\\begin{array}{c}a_{1}^{T}\\\\ a_{2}^{T}\\\\ \\vdots\\\\ a_{m}^{T}\\end{array}\\right]B=\\left[\\begin{array}{c}a_{1}^{T}B\\\\ a_{2}^{T}B\\\\ \\vdots\\\\ a_{m}^{T}B\\end{array}\\right]=\\left[\\begin{array}{c}(B^{T}a_{1})^{T}\\\\ (B^{T}a_{2})^{T}\\\\ \\vdots\\\\ (B^{T}a_{m})^{T}\\end{array}\\right].\\]\nThis shows that the rows of \\(AB\\) are obtained by applying \\(B^{T}\\) to the transposed row vectors \\(a_{k}\\) of \\(A\\), and transposing the result.\nInner product representation.From the definition of the \\(i,j\\) element of \\(AB\\) in (10.1), we also see that the elements of \\(AB\\) are the inner products of the rows of \\(A\\) with the columns of \\(B\\):\n\n\n",
        "answer": 0.0,
        "rationale": {
            "Question": "Both A, B are n-by-n matrices with rank(A)=n, rank(A*B)=0. What is rank(B)?",
            "Answer": 0.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 4.6 example 6",
            "id": "mingyin/Sylveete-rank-inequality1.json",
            "explanation": "NONE",
            "theorem": "sylveeter rank inequality",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_7.json",
        "question": "Let A be an invertible n * n matrix and v and eigenvector of both A and B, is v necesarily an eigenvector of A + B?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet A be an invertible n * n matrix and v and eigenvector of both A and B, is v necesarily an eigenvector of A + B?\n\nDocument 1:\n----\n\\[x=A^{-1}b\\] (11.1)\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\nInvertibility conditions.For square matrices, left-invertibility, right-invertibility, and invertibility are equivalent: If a matrix is square and left-invertible, then it is also right-invertible (and therefore invertible) and vice-versa.\nTo see this, suppose \\(A\\) is an \\(n\\times n\\) matrix and left-invertible. This implies that the \\(n\\) columns of \\(A\\) are linearly independent. Therefore they form a basis and so any \\(n\\)-vector can be expressed as a linear combination of the columns of \\(A\\). In particular, each of the \\(n\\) unit vectors \\(e_{i}\\) can be expressed as \\(e_{i}=Ab_{i}\\) for some \\(n\\)-vector \\(b_{i}\\). The matrix \\(B=\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\) satisfies\n\\[AB=\\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right]= \\left[\\begin{array}{cccc}e_{1}&e_{2}&\\cdots&e_{n}\\end{array}\\right]=I.\\]\nSo \\(B\\) is a right inverse of \\(A\\).\nWe have just shown that for a square matrix \\(A\\),\n\nDocument 2:\n----\n**11.10**: _Reverse-time linear dynamical system._ A linear dynamical system has the form \\[x_{t+1}=Ax_{t},\\] where \\(x_{t}\\) in the (\\(n\\)-vector) state in period \\(t\\), and \\(A\\) is the \\(n\\times n\\) dynamics matrix. This formula gives the state in the next period as a function of the current state. We want to derive a recursion of the form \\[x_{t-1}=A^{\\rm rev}x_{t},\\] which gives the previous state as a function of the current state. We call this the _reverse time linear dynamical system_. 1. When is this possible? When it is possible, what is \\(A^{\\rm rev}\\)? 2. For the specific linear dynamical system with dynamics matrix \\[A=\\left[\\begin{array}{cc}3&2\\\\ -1&4\\end{array}\\right],\\] find \\(A^{\\rm rev}\\), or explain why the reverse time linear dynamical system doesn't exist.\n**11.11**: _Interpolation of rational functions._ (Continuation of exercise 8.8.) Find a rational function \\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}}\\] that satisfies the following interpolation conditions: \\[f(1)=2,\\qquad f(2)=5,\\qquad f(3)=9,\\qquad f(4)=-1,\\qquad f(5)=-4.\\] In exercise 8.8 these conditions were expressed as a set of linear equations in the coefficients \\(c_{1}\\), \\(c_{2}\\), \\(c_{3}\\), \\(d_{1}\\) and \\(d_{2}\\); here we are asking you to form and (numerically) solve the system of equations. Plot the rational function you find over the range \\(x=0\\) to \\(x=6\\). Your plot should include markers at the interpolation points \\((1,2),\\ldots,(5,-4)\\). (Your rational function graph should pass through these points.)\n**11.12**: _Combinations of invertible matrices._ Suppose the \\(n\\times n\\) matrices \\(A\\) and \\(B\\) are both invertible. Determine whether each of the matrices given below is invertible, without any further assumptions about \\(A\\) and \\(B\\).\n1. \\(A+B\\).\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Let A be an invertible n * n matrix and v and eigenvector of both A and B, is v necesarily an eigenvector of A + B?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "wenhuchen/vieta's_formula2.json",
        "question": "The polynomial $x^3 - Ax + 15$ has three real roots. Two of these roots sum to 5. What is |A|?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nThe polynomial $x^3 - Ax + 15$ has three real roots. Two of these roots sum to 5. What is |A|?\n\nDocument 1:\n----\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\nDocument 2:\n----\n1. Show that every vector \\(x\\) can be decomposed in a unique way as a sum \\(x=x_{\\rm s}+x_{\\rm a}\\) of a symmetric vector \\(x_{\\rm s}\\) and an anti-symmetric vector \\(x_{\\rm a}\\).\n2. Show that the symmetric and anti-symmetric parts \\(x_{\\rm s}\\) and \\(x_{\\rm a}\\) are linear functions of \\(x\\). Give matrices \\(A_{\\rm s}\\) and \\(A_{\\rm a}\\) such that \\(x_{\\rm s}=A_{\\rm s}x\\) and \\(x_{\\rm a}=A_{\\rm a}x\\) for all \\(x\\).\n**8.6**: _Linear functions._ For each description of \\(y\\) below, express it as \\(y=Ax\\) for some \\(A\\). (You should specify \\(A\\).)\n1. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average of \\(x_{1},\\ldots,x_{i-1}\\). (We take \\(y_{1}=x_{1}\\).)\n2. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average value of all other \\(x_{j}\\)s, _i.e._, the average of \\(x_{1},\\ldots,x_{i-1},x_{i+1},\\ldots,x_{n}\\).\n**8.7**: _Interpolation of polynomial values and derivatives._ The 5-vector \\(c\\) represents the coefficients of a quartic polynomial \\(p(x)=c_{1}+c_{2}x+c_{3}x^{2}+c_{4}x^{3}+c_{5}x^{4}\\). Express the conditions \\[p(0)=0,\\quad p^{\\prime}(0)=0,\\quad p(1)=1,\\quad p^{\\prime}(1)=0,\\] as a set of linear equations of the form \\(Ac=b\\). Is the system of equations underdetermined, over-determined, or square?\n**8.8**: _Interpolation of rational functions._ A _rational function_ of degree two has the form\n\\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}},\\]\nwhere \\(c_{1},c_{2},c_{3},d_{1},d_{2}\\) are coefficients. ('Rational' refers to the fact that \\(f\\) is a ratio of polynomials. Another name for \\(f\\) is _bi-quadratic_.) Consider the interpolation conditions\n\\[f(t_{i})=y_{i},\\quad i=1,\\ldots,K,\\]\n\n\n",
        "answer": 22.0,
        "rationale": {
            "Question": "The polynomial $x^3 - Ax + 15$ has three real roots. Two of these roots sum to 5. What is |A|?",
            "Answer": 22.0,
            "Answer_type": "float",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula2.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "mingyin/linear-dependence2.json",
        "question": "suppose a,b,c,\\alpha,\\beta,\\gamma are six real numbers with a^2+b^2+c^2>0.  In addition, $a=b*cos(\\gamma)+c*cos(\\beta), b=c*cos(\\alpha)+a*cos(\\gamma), c=a*cos(\\beta)+b*cos(\\alpha)$. What is the value of $cos^2(\\alpha)+cos^2(\\beta)+cos^2(\\gamma)+2*cos(\\alpha)*cos(\\beta)*cos(\\gamma)? return the numeric.",
        "prediction": 1,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nsuppose a,b,c,\\alpha,\\beta,\\gamma are six real numbers with a^2+b^2+c^2>0.  In addition, $a=b*cos(\\gamma)+c*cos(\\beta), b=c*cos(\\alpha)+a*cos(\\gamma), c=a*cos(\\beta)+b*cos(\\alpha)$. What is the value of $cos^2(\\alpha)+cos^2(\\beta)+cos^2(\\gamma)+2*cos(\\alpha)*cos(\\beta)*cos(\\gamma)? return the numeric.\n\nDocument 1:\n----\nProperties.The inner product satisfies some simple properties that are easily verified from the definition. If \\(a\\), \\(b\\), and \\(c\\) are vectors of the same size, and \\(\\gamma\\) is a scalar, we have the following.\n* _Commutativity_. \\(a^{T}b=b^{T}a\\). The order of the two vector arguments in the inner product does not matter.\n* _Associativity with scalar multiplication_. \\((\\gamma a)^{T}b=\\gamma(a^{T}b)\\), so we can write both as \\(\\gamma a^{T}b\\).\n* _Distributivity with vector addition_. \\((a+b)^{T}c=a^{T}c+b^{T}c\\). The inner product can be distributed across vector addition.\nThese can be combined to obtain other identities, such as \\(a^{T}(\\gamma b)=\\gamma(a^{T}b)\\), or \\(a^{T}(b+\\gamma c)=a^{T}b+\\gamma a^{T}c\\). As another useful example, we have, for any vectors \\(a,b,c,d\\) of the same size,\n\\[(a+b)^{T}(c+d)=a^{T}c+a^{T}d+b^{T}c+b^{T}d.\\]\nFigure 1.12: The affine combination \\((1-\\theta)a+\\theta b\\) for different values of \\(\\theta\\). These points are on the line passing through \\(a\\) and \\(b\\); for \\(\\theta\\) between \\(0\\) and \\(1\\), the points are on the line segment between \\(a\\) and \\(b\\).\nThis formula expresses an inner product on the left-hand side as a sum of four inner products on the right-hand side, and is analogous to expanding a product of sums in algebra. Note that on the left-hand side, the two addition symbols refer to vector addition, whereas on the right-hand side, the three addition symbols refer to scalar (number) addition.\n\nDocument 2:\n----\nSuppose you have observed the values \\(z_{1},\\ldots,z_{T}\\), and wish to choose the sinusoid amplitudes \\(a_{1},\\ldots,a_{K}\\) and phases \\(\\phi_{1},\\ldots,\\phi_{K}\\) so as to minimize the RMS value of the approximation error \\((\\hat{z}_{1}-z_{1},\\ldots,\\hat{z}_{T}-z_{T})\\). (We assume that the frequencies are given.) Explain how to solve this using least squares model fitting.\n_Hint._ A sinusoid with amplitude \\(a\\), frequency \\(\\omega\\), and phase \\(\\phi\\) can be described by its cosine and sine coefficients \\(\\alpha\\) and \\(\\beta\\), where\n\\[a\\cos(\\omega t-\\phi)=\\alpha\\cos(\\omega t)+\\beta\\sin(\\omega t),\\]\nwhere (using the cosine of sum formula) \\(\\alpha=a\\cos\\phi\\), \\(\\beta=a\\sin\\phi\\). We can recover the amplitude and phase from the cosine and sine coefficients as\n\\[a=\\sqrt{\\alpha^{2}+\\beta^{2}},\\qquad\\phi=\\arctan(\\beta/\\alpha).\\]\nExpress the problem in terms of the cosine and sine coefficients.\n* 13.20_Fitting with continuous and discontinuous piecewise-linear functions._ Consider a fitting problem with \\(n=1\\), so \\(x^{(1)},\\ldots,x^{(N)}\\) and \\(y^{(1)},\\ldots,y^{(N)}\\) are numbers. We consider two types of closely related models. The first is a piecewise-linear model with knot points at \\(-1\\) and \\(1\\), as described on page 256, and illustrated in figure 13.8. The second is a stratified model (see page 272), with three independent affine models, one for \\(x<-1\\), one for \\(-1\\leq x\\leq 1\\), and one for \\(x>1\\). (In other words, we stratify on \\(x\\) taking low, middle, or high values.) Are these two models the same? Is one more general than the other? How many parameters does each model have? _Hint._ See problem title. What can you say about the training set RMS error and test set RMS error that would be achieved using least squares with these two models?\n\n\n",
        "answer": 1.0,
        "rationale": {
            "Question": "suppose a,b,c,\\alpha,\\beta,\\gamma are six real numbers with a^2+b^2+c^2>0.  In addition, $a=b*cos(\\gamma)+c*cos(\\beta), b=c*cos(\\alpha)+a*cos(\\gamma), c=a*cos(\\beta)+b*cos(\\alpha)$. What is the value of $cos^2(\\alpha)+cos^2(\\beta)+cos^2(\\gamma)+2*cos(\\alpha)*cos(\\beta)*cos(\\gamma)? return the numeric.",
            "Answer": 1.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 3.5 example 7",
            "id": "mingyin/linear-dependence2.json",
            "explanation": "NONE",
            "theorem": "linear dependence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": true
    },
    {
        "id": "wenhuchen/determinant2.json",
        "question": "For matrix A = [[2, 4, 3], [3, 3, 1], [42, 20, 51]], what is its determinant?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[2, 4, 3], [3, 3, 1], [42, 20, 51]], what is its determinant?\n\nDocument 1:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\nDocument 2:\n----\n* _Entries of matrix triple product_. (See page 10.) Suppose \\(A\\) has dimensions \\(m\\times n\\), \\(B\\) has dimensions \\(n\\times p\\), \\(C\\) has dimensions \\(p\\times q\\), and let \\(D=ABC\\). Show that \\[D_{ij}=\\sum_{k=1}^{n}\\sum_{l=1}^{p}A_{ik}B_{kl}C_{lj}.\\] This is the formula analogous to (10.1) for the product of two matrices.\n* _Multiplication by a diagonal matrix_. Suppose that \\(A\\) is an \\(m\\times n\\) matrix, \\(D\\) is a diagonal matrix, and \\(B=DA\\). Describe \\(B\\) in terms of \\(A\\) and the entries of \\(D\\). You can refer to the rows or columns or entries of \\(A\\).\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n\n\n",
        "answer": -376,
        "rationale": {
            "Question": "For matrix A = [[2, 4, 3], [3, 3, 1], [42, 20, 51]], what is its determinant?",
            "Answer": -376,
            "Picture": null,
            "Answer_type": "integer",
            "source": "self",
            "id": "wenhuchen/determinant2.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "wenhuchen/vieta's_formula3.json",
        "question": "Let $x_1$ and $x_2$ be the roots of the equation $x^2 + 3x + 1 =0$. Compute $(x_1/(x_2 + 1))^2 + (x_2 / (x_1 + 1))^2$.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet $x_1$ and $x_2$ be the roots of the equation $x^2 + 3x + 1 =0$. Compute $(x_1/(x_2 + 1))^2 + (x_2 / (x_1 + 1))^2$.\n\nDocument 1:\n----\n* A list consisting of a single vector is linearly dependent only if the vector is zero. It is linearly independent only if the vector is nonzero.\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\n\nDocument 2:\n----\nTriangular matrix.A triangular matrix with nonzero diagonal elements is invertible. We first discuss this for a lower triangular matrix. Let \\(L\\) be \\(n\\times n\\) and lower triangular with nonzero diagonal elements. We show that the columns are linearly independent, _i.e._, \\(Lx=0\\) is only possible if \\(x=0\\). Expanding the matrix-vector product, we can write \\(Lx=0\\) as\n\\[L_{11}x_{1} = 0\\] \\[L_{21}x_{1}+L_{22}x_{2} = 0\\] \\[L_{31}x_{1}+L_{32}x_{2}+L_{33}x_{3} = 0\\] \\[\\vdots\\] \\[L_{n1}x_{1}+L_{n2}x_{2}+\\cdots+L_{n,n-1}x_{n-1}+L_{nn}x_{n} = 0.\\]\nSince \\(L_{11}\\neq 0\\), the first equation implies \\(x_{1}=0\\). Using \\(x_{1}=0\\), the second equation reduces to \\(L_{22}x_{2}=0\\). Since \\(L_{22}\\neq 0\\), we conclude that \\(x_{2}=0\\). Using \\(x_{1}=x_{2}=0\\), the third equation now reduces to \\(L_{33}x_{3}=0\\), and since \\(L_{33}\\) is assumed to be nonzero, we have \\(x_{3}=0\\). Continuing this argument, we find that all entries of \\(x\\) are zero, and this shows that the columns of \\(L\\) are linearly independent. It follows that \\(L\\) is invertible.\nA similar argument can be followed to show that an upper triangular matrix with nonzero diagonal elements is invertible. One can also simply note that if \\(R\\) is upper triangular, then \\(L=R^{T}\\) is lower triangular with the same diagonal, and use the formula \\((L^{T})^{-1}=(L^{-1})^{T}\\) for the inverse of the transpose.\nInverse via QR factorization.The QR factorization gives a simple expression for the inverse of an invertible matrix. If \\(A\\) is square and invertible, its columns are linearly independent, so it has a QR factorization \\(A=QR\\). The matrix \\(Q\\) is orthogonal and \\(R\\) is upper triangular with positive diagonal entries. Hence \\(Q\\) and \\(R\\) are invertible, and the formula for the inverse product gives\n\n\n",
        "answer": 18.0,
        "rationale": {
            "Question": "Let $x_1$ and $x_2$ be the roots of the equation $x^2 + 3x + 1 =0$. Compute $(x_1/(x_2 + 1))^2 + (x_2 / (x_1 + 1))^2$.",
            "Answer": 18.0,
            "Answer_type": "float",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula3.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_3_3.json",
        "question": "What is the vector that spans the kernel of A = [[1, 0, 2, 4], [0, 1, -3, -1], [3, 4, -6, 8], [0, -1, 3, 4]]?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the vector that spans the kernel of A = [[1, 0, 2, 4], [0, 1, -3, -1], [3, 4, -6, 8], [0, -1, 3, 4]]?\n\nDocument 1:\n----\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\nNow suppose that \\(x\\) is any \\(n\\)-vector. It can be expressed as a linear combination of the basis vectors \\(a_{1},\\ldots,a_{n}\\):\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{n}a_{n}.\\]\nThe dual basis gives us a simple way to find the coefficients \\(\\beta_{1},\\ldots,\\beta_{n}\\).\nWe start with \\(AB=I\\), and multiply by \\(x\\) to get\n\\[x=ABx=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right]\\left[ \\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right]x=(b_{1}^{T}x)a_{1}+\\cdots+(b_{n}^{T}x)a_{n}.\\]\nThis means (since the vectors \\(a_{1},\\ldots,a_{n}\\) are linearly independent) that \\(\\beta_{i}=b_{i}^{T}x\\). In words: The coefficients in the expansion of a vector in a basis are given by the inner products with the dual basis vectors. Using matrix notation, we can say that \\(\\beta=B^{T}x=(A^{-1})^{T}x\\) is the vector of coefficients of \\(x\\) in the basis given by the columns of \\(A\\).\n\nDocument 2:\n----\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\\[l_{i}=\\left[\\begin{array}{c}0_{i-1}\\\\ 1\\\\ -(1+r)\\\\ 0_{n-i-1}\\end{array}\\right],\\quad i=1,\\ldots,n-1,\\]\nwhere \\(r\\geq 0\\) is the per-period interest rate. The cash flow \\(l_{i}\\) represents a loan of $1 in period \\(i\\), which is paid back in period \\(i+1\\) with interest \\(r\\). (The subscripts on the zero vectors above give their dimensions.) Scaling \\(l_{i}\\) changes the loan amount; scaling \\(l_{i}\\) by a negative coefficient converts it into a loan _to_ another entity (which is paid back in period \\(i+1\\) with interest).\n\n\n",
        "answer": [
            -2,
            3,
            1,
            0
        ],
        "rationale": {
            "Question": "What is the vector that spans the kernel of A = [[1, 0, 2, 4], [0, 1, -3, -1], [3, 4, -6, 8], [0, -1, 3, 4]]?",
            "Answer": [
                -2,
                3,
                1,
                0
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_3.json",
            "explanation": "NONE",
            "theorem": "kernel of linear transformations",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "mingyin/gaussian-elimination2.json",
        "question": "V is a vector space over the real field R. It is known that the vector group u_1, u_2, u_3 in V are linearly independent. Finding the rank of vector group ${u_1-\\lambda u_2, u_2-\\lambda u_3, u_3-\\lambda u_1}$ for $\\lambda=\\sqrt{5}$ and $\\lambda=1$ separately. Return the answer as a list.",
        "prediction": [
            3,
            3
        ],
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nV is a vector space over the real field R. It is known that the vector group u_1, u_2, u_3 in V are linearly independent. Finding the rank of vector group ${u_1-\\lambda u_2, u_2-\\lambda u_3, u_3-\\lambda u_1}$ for $\\lambda=\\sqrt{5}$ and $\\lambda=1$ separately. Return the answer as a list.\n\nDocument 1:\n----\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\nDocument 2:\n----\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\nNow suppose that \\(x\\) is any \\(n\\)-vector. It can be expressed as a linear combination of the basis vectors \\(a_{1},\\ldots,a_{n}\\):\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{n}a_{n}.\\]\nThe dual basis gives us a simple way to find the coefficients \\(\\beta_{1},\\ldots,\\beta_{n}\\).\nWe start with \\(AB=I\\), and multiply by \\(x\\) to get\n\\[x=ABx=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right]\\left[ \\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right]x=(b_{1}^{T}x)a_{1}+\\cdots+(b_{n}^{T}x)a_{n}.\\]\nThis means (since the vectors \\(a_{1},\\ldots,a_{n}\\) are linearly independent) that \\(\\beta_{i}=b_{i}^{T}x\\). In words: The coefficients in the expansion of a vector in a basis are given by the inner products with the dual basis vectors. Using matrix notation, we can say that \\(\\beta=B^{T}x=(A^{-1})^{T}x\\) is the vector of coefficients of \\(x\\) in the basis given by the columns of \\(A\\).\n\n\n",
        "answer": [
            3,
            2
        ],
        "rationale": {
            "Question": "V is a vector space over the real field R. It is known that the vector group u_1, u_2, u_3 in V are linearly independent. Finding the rank of vector group ${u_1-\\lambda u_2, u_2-\\lambda u_3, u_3-\\lambda u_1}$ for $\\lambda=\\sqrt{5}$ and $\\lambda=1$ separately. Return the answer as a list.",
            "Answer": [
                3,
                2
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "linear algebra 2.6 example 1(2)",
            "id": "mingyin/gaussian-elimination2.json",
            "explanation": "NONE",
            "theorem": "gaussian elimination",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_7_2.json",
        "question": "Is there an eigenbasis for the identity matrix I_n?",
        "prediction": true,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs there an eigenbasis for the identity matrix I_n?\n\nDocument 1:\n----\nThis identity gives us a simple way to check if an \\(n\\)-vector \\(y\\) is a linear combination of the orthonormal vectors \\(a_{1},\\ldots,a_{k}\\). If the identity (5.4) holds for \\(y\\), _i.e._,\n\\[y=(a_{1}^{T}y)a_{1}+\\cdots+(a_{k}^{T}y)a_{k},\\]\nthen (evidently) \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); conversely, if \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\), the identity (5.4) holds for \\(y\\).\nOrthonormal basis.If the \\(n\\)-vectors \\(a_{1},\\ldots,a_{n}\\) are orthonormal, they are linearly independent, and therefore also a basis. In this case they are called an _orthonormal basis_. The three examples above (on page 5.2) are orthonormal bases.\nIf \\(a_{1},\\ldots,a_{n}\\) is an orthonormal basis, then we have, for any \\(n\\)-vector \\(x\\), the identity\n\\[x=(a_{1}^{T}x)a_{1}+\\cdots+(a_{n}^{T}x)a_{n}.\\] (5.5)\nTo see this, we note that since \\(a_{1},\\ldots,a_{n}\\) are a basis, \\(x\\) can be expressed as a linear combination of them; hence the identity (5.4) above holds. The equation above is sometimes called the _orthonormal expansion formula_; the right-hand side is called the _expansion of \\(x\\) in the basis \\(a_{1},\\ldots,a_{n}\\)_. It shows that any \\(n\\)-vector can be expressed as a linear combination of the basis elements, with the coefficients given by taking the inner product of \\(x\\) with the elements of the basis.\nAs an example, we express the 3-vector \\(x=(1,2,3)\\) as a linear combination of the orthonormal basis given in (5.3). The inner products of \\(x\\) with these vectors\n\nDocument 2:\n----\n* The rows of \\(A\\) are linearly independent.\n* \\(A\\) has a left inverse.\n* \\(A\\) has a right inverse.\n**Examples.**\n* The identity matrix \\(I\\) is invertible, with inverse \\(I^{-1}=I\\), since \\(II=I\\).\n* A diagonal matrix \\(A\\) is invertible if and only if its diagonal entries are nonzero. The inverse of an \\(n\\times n\\) diagonal matrix \\(A\\) with nonzero diagonal entries is \\[A^{-1}=\\left[\\begin{array}{cccc}1/A_{11}&0&\\cdots&0\\\\ 0&1/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&1/A_{nn}\\end{array}\\right],\\] since \\[AA^{-1}=\\left[\\begin{array}{cccc}A_{11}/A_{11}&0&\\cdots&0\\\\ 0&A_{22}/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&A_{nn}/A_{nn}\\end{array}\\right]=I.\\] In compact notation, we have \\[\\mathbf{diag}(A_{11},\\ldots,A_{nn})^{-1}=\\mathbf{diag}(A_{11}^{-1},\\ldots,A_{ nn}^{-1}).\\] Note that the inverse on the left-hand side of this equation is the matrix inverse, while the inverses appearing on the right-hand side are scalar inverses.\n* As a non-obvious example, the matrix \\[A=\\left[\\begin{array}{cccc}1&-2&3\\\\ 0&2&2\\\\ -3&-4&-4\\end{array}\\right]\\] is invertible, with inverse \\[A^{-1}=\\frac{1}{30}\\left[\\begin{array}{cccc}0&-20&-10\\\\ -6&5&-2\\\\ 6&10&2\\end{array}\\right].\\] This can be verified by checking that \\(AA^{-1}=I\\) (or that \\(A^{-1}A=I\\), since either of these implies the other).\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Is there an eigenbasis for the identity matrix I_n?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_2.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_7_5.json",
        "question": "For which 2 * 2 matrices A does there exist a nonzero matrix M such that AM = MD, where D = [[2, 0], [0, 3]]? Give your answer in terms of eigenvalues of A.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor which 2 * 2 matrices A does there exist a nonzero matrix M such that AM = MD, where D = [[2, 0], [0, 3]]? Give your answer in terms of eigenvalues of A.\n\nDocument 1:\n----\n\\[x_{t+1}=Ax_{t}+Bu_{t},\\qquad y_{t}=Cx_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(x_{t}\\), input \\(u_{t}\\), and output \\(y_{t}\\). The second is given by\n\\[\\tilde{x}_{t+1}=\\tilde{A}\\tilde{x}_{t}+\\tilde{B}w_{t},\\qquad v_{t}=\\tilde{C} \\tilde{x}_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(\\tilde{x}_{t}\\), input \\(w_{t}\\), and output \\(v_{t}\\). We now connect the output of the first linear dynamical system to the input of the second one, which means we take \\(w_{t}=y_{t}\\). (This is called the _composition_ of the two systems.) Show that this composition can also be expressed as a linear dynamical system with state \\(z_{t}=(x_{t},\\tilde{x}_{t})\\), input \\(u_{t}\\), and output \\(v_{t}\\). (Give the state transition matrix, input matrix, and output matrix.)\n**10.23**: Suppose \\(A\\) is an \\(n\\times n\\) matrix that satisfies \\(A^{2}=0\\). Does this imply that \\(A=0\\)? (This is the case when \\(n=1\\).) If this is (always) true, explain why. If it is not, give a specific counterexample, _i.e._, a matrix \\(A\\) that is nonzero but satisfies \\(A^{2}=0\\).\n**10.24**: _Matrix power identity._ A student says that for any square matrix \\(A\\),\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n\nDocument 2:\n----\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\nTwo matrices are equal if they have the same size, and the corresponding entries are all equal. As with vectors, we normally deal with matrices with entries that\nare real numbers, which will be our assumption unless we state otherwise. The set of real \\(m\\times n\\) matrices is denoted \\(\\mathbf{R}^{m\\times n}\\). But matrices with complex entries, for example, do arise in some applications.\nMatrix indexing.As with vectors, standard mathematical notation indexes the rows and columns of a matrix starting from 1. In computer languages, matrices are often (but not always) stored as 2-dimensional arrays, which can be indexed in a variety of ways, depending on the language. Lower level languages typically use indices starting from 0; higher level languages and packages that support matrix operations usually use standard mathematical indexing, starting from 1.\nSquare, tall, and wide matrices.A _square_ matrix has an equal number of rows and columns. A square matrix of size \\(n\\times n\\) is said to be of _order_\\(n\\). A _tall_ matrix has more rows than columns (size \\(m\\times n\\) with \\(m>n\\)). A _wide_ matrix has more columns than rows (size \\(m\\times n\\) with \\(n>m\\)).\n\n\n",
        "answer": [
            2,
            3
        ],
        "rationale": {
            "Question": "For which 2 * 2 matrices A does there exist a nonzero matrix M such that AM = MD, where D = [[2, 0], [0, 3]]? Give your answer in terms of eigenvalues of A.",
            "Answer": [
                2,
                3
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_5.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_2.json",
        "question": "Is the transformation [[-1, 0], [0, -1]] invertible?",
        "prediction": true,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs the transformation [[-1, 0], [0, -1]] invertible?\n\nDocument 1:\n----\n### 11.5 Pseudo-inverse\nLinearly independent columns and Gram invertibility.We first show that an \\(m\\times n\\) matrix \\(A\\) has linearly independent columns if and only if its \\(n\\times n\\) Gram matrix \\(A^{T}A\\) is invertible.\nFirst suppose that the columns of \\(A\\) are linearly independent. Let \\(x\\) be an \\(n\\)-vector which satisfies \\((A^{T}A)x=0\\). Multiplying on the left by \\(x^{T}\\) we get\n\\[0=x^{T}0=x^{T}(A^{T}Ax)=x^{T}A^{T}Ax=\\|Ax\\|^{2},\\]\nwhich implies that \\(Ax=0\\). Since the columns of \\(A\\) are linearly independent, we conclude that \\(x=0\\). Since the only solution of \\((A^{T}A)x=0\\) is \\(x=0\\), we conclude that \\(A^{T}A\\) is invertible.\nNow let's show the converse. Suppose the columns of \\(A\\) are linearly dependent, which means there is a nonzero \\(n\\)-vector \\(x\\) which satisfies \\(Ax=0\\). Multiply on the left by \\(A^{T}\\) to get \\((A^{T}A)x=0\\). This shows that the Gram matrix \\(A^{T}A\\) is singular.\nPseudo-inverse of square or tall matrix.We show here that if \\(A\\) has linearly independent columns (and therefore, is square or tall) then it has a left inverse. (We already have observed the converse, that a matrix with a left inverse has linearly independent columns.) Assuming \\(A\\) has linearly independent columns, we know that \\(A^{T}A\\) is invertible. We now observe that the matrix \\((A^{T}A)^{-1}A^{T}\\) is a left inverse of \\(A\\):\n\\[\\left((A^{T}A)^{-1}A^{T}\\right)A=(A^{T}A)^{-1}(A^{T}A)=I.\\]\n\nDocument 2:\n----\n**Examples.**\n* The identity matrix \\(I\\) is invertible, with inverse \\(I^{-1}=I\\), since \\(II=I\\).\n* A diagonal matrix \\(A\\) is invertible if and only if its diagonal entries are nonzero. The inverse of an \\(n\\times n\\) diagonal matrix \\(A\\) with nonzero diagonal entries is \\[A^{-1}=\\left[\\begin{array}{cccc}1/A_{11}&0&\\cdots&0\\\\ 0&1/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&1/A_{nn}\\end{array}\\right],\\] since \\[AA^{-1}=\\left[\\begin{array}{cccc}A_{11}/A_{11}&0&\\cdots&0\\\\ 0&A_{22}/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&A_{nn}/A_{nn}\\end{array}\\right]=I.\\] In compact notation, we have \\[\\mathbf{diag}(A_{11},\\ldots,A_{nn})^{-1}=\\mathbf{diag}(A_{11}^{-1},\\ldots,A_{ nn}^{-1}).\\] Note that the inverse on the left-hand side of this equation is the matrix inverse, while the inverses appearing on the right-hand side are scalar inverses.\n* As a non-obvious example, the matrix \\[A=\\left[\\begin{array}{cccc}1&-2&3\\\\ 0&2&2\\\\ -3&-4&-4\\end{array}\\right]\\] is invertible, with inverse \\[A^{-1}=\\frac{1}{30}\\left[\\begin{array}{cccc}0&-20&-10\\\\ -6&5&-2\\\\ 6&10&2\\end{array}\\right].\\] This can be verified by checking that \\(AA^{-1}=I\\) (or that \\(A^{-1}A=I\\), since either of these implies the other).\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\nInverse of matrix transpose.If \\(A\\) is invertible, its transpose \\(A^{T}\\) is also invertible and its inverse is \\((A^{-1})^{T}\\):\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Is the transformation [[-1, 0], [0, -1]] invertible?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_2.json",
            "explanation": "solutions/math_algebra_2.txt",
            "theorem": "invertible matrix theorem",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "wenhuchen/eigen_value2.json",
        "question": "For matrix A = [[3, 1, 1], [2, 4, 2], [1, 1, 3]], what are its eigen values?",
        "prediction": [
            1,
            3,
            6
        ],
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[3, 1, 1], [2, 4, 2], [1, 1, 3]], what are its eigen values?\n\nDocument 1:\n----\nAn important attribute of a matrix is its _size_ or _dimensions_, _i.e._, the numbers of rows and columns. The matrix above has 3 rows and 4 columns, so its size is \\(3\\times 4\\). A matrix of size \\(m\\times n\\) is called an \\(m\\times n\\) matrix.\nThe _elements_ (or _entries_ or _coefficients_) of a matrix are the values in the array. The \\(i,j\\) element is the value in the \\(i\\)th row and \\(j\\)th column, denoted by double subscripts: the \\(i,j\\) element of a matrix \\(A\\) is denoted \\(A_{ij}\\) (or \\(A_{i,j}\\), when \\(i\\) or \\(j\\) is more than one digit or character). The positive integers \\(i\\) and \\(j\\) are called the (row and column) _indices_. If \\(A\\) is an \\(m\\times n\\) matrix, then the row index \\(i\\) runs from 1 to \\(m\\) and the column index \\(j\\) runs from 1 to \\(n\\). Row indices go from top to bottom, so row 1 is the top row and row \\(m\\) is the bottom row. Column indices go from left to right, so column 1 is the left column and column \\(n\\) is the right column.\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\n\nDocument 2:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\n\n",
        "answer": [
            2,
            6
        ],
        "rationale": {
            "Question": "For matrix A = [[3, 1, 1], [2, 4, 2], [1, 1, 3]], what are its eigen values?",
            "Answer": [
                2,
                6
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/eigen_value2.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "wenhuchen/definite_matrix1.json",
        "question": "Consider the matrix of A=[[1, 4], [4, 1]], is this a positive definite matrix?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider the matrix of A=[[1, 4], [4, 1]], is this a positive definite matrix?\n\nDocument 1:\n----\n\\[x_{t+1}=Ax_{t}+Bu_{t},\\qquad y_{t}=Cx_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(x_{t}\\), input \\(u_{t}\\), and output \\(y_{t}\\). The second is given by\n\\[\\tilde{x}_{t+1}=\\tilde{A}\\tilde{x}_{t}+\\tilde{B}w_{t},\\qquad v_{t}=\\tilde{C} \\tilde{x}_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(\\tilde{x}_{t}\\), input \\(w_{t}\\), and output \\(v_{t}\\). We now connect the output of the first linear dynamical system to the input of the second one, which means we take \\(w_{t}=y_{t}\\). (This is called the _composition_ of the two systems.) Show that this composition can also be expressed as a linear dynamical system with state \\(z_{t}=(x_{t},\\tilde{x}_{t})\\), input \\(u_{t}\\), and output \\(v_{t}\\). (Give the state transition matrix, input matrix, and output matrix.)\n**10.23**: Suppose \\(A\\) is an \\(n\\times n\\) matrix that satisfies \\(A^{2}=0\\). Does this imply that \\(A=0\\)? (This is the case when \\(n=1\\).) If this is (always) true, explain why. If it is not, give a specific counterexample, _i.e._, a matrix \\(A\\) that is nonzero but satisfies \\(A^{2}=0\\).\n**10.24**: _Matrix power identity._ A student says that for any square matrix \\(A\\),\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n\nDocument 2:\n----\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n**10.38**: _Orthogonal matrix with nonnegative entries._ Suppose the \\(n\\times n\\) matrix \\(A\\) is orthogonal, and all of its entries are nonnegative, _i.e._, \\(A_{ij}\\geq 0\\) for \\(i,j=1,\\ldots,n\\). Show that \\(A\\) must be a permutation matrix, _i.e._, each entry is either \\(0\\) or \\(1\\), each row has exactly one entry with value one, and each column has exactly one entry with value one. (See page 132.)\n**10.39**: _Gram matrix and QR factorization._ Suppose the matrix \\(A\\) has linearly independent columns and QR factorization \\(A=QR\\). What is the relationship between the Gram matrix of \\(A\\) and the Gram matrix of \\(R\\)? What can you say about the angles between the columns of \\(A\\) and the angles between the columns of \\(R\\)?\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Consider the matrix of A=[[1, 4], [4, 1]], is this a positive definite matrix?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | https://www.math.usm.edu/lambers/mat419/lecture3.pdf",
            "id": "wenhuchen/definite_matrix1.json",
            "explanation": "NONE",
            "theorem": "definite matrix criteria",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_6_4.json",
        "question": "What is the determinant of the matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the determinant of the matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]?\n\nDocument 1:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\nDocument 2:\n----\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n**10.11**: _Trace of matrix-matrix product_. The sum of the diagonal entries of a square matrix is called the _trace_ of the matrix, denoted \\(\\mathbf{tr}(A)\\).\n1. Suppose \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices. Show that \\[\\mathbf{tr}(A^{T}B)=\\sum_{i=1}^{m}\\sum_{j=1}^{n}A_{ij}B_{ij}.\\] What is the complexity of calculating \\(\\mathbf{tr}(A^{T}B)\\)?\n2. The number \\(\\mathbf{tr}(A^{T}B)\\) is sometimes referred to as the inner product of the matrices \\(A\\) and \\(B\\). (This allows us to extend concepts like angle to matrices.) Show that \\(\\mathbf{tr}(A^{T}B)=\\mathbf{tr}(B^{T}A)\\).\n3. Show that \\(\\mathbf{tr}(A^{T}A)=\\left\\|A\\right\\|^{2}\\). In other words, the square of the norm of a matrix is the trace of its Gram matrix.\n\n\n",
        "answer": -3,
        "rationale": {
            "Question": "What is the determinant of the matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]?",
            "Answer": -3,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_4.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "mingyin/orthogonal-similarity1.json",
        "question": "If A and B are both orthogonal square matrices, and det A = -det B. What is det(A+B)? Return the numerical value.",
        "prediction": 1,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIf A and B are both orthogonal square matrices, and det A = -det B. What is det(A+B)? Return the numerical value.\n\nDocument 1:\n----\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\nInverse of matrix transpose.If \\(A\\) is invertible, its transpose \\(A^{T}\\) is also invertible and its inverse is \\((A^{-1})^{T}\\):\n\\[(A^{T})^{-1}=(A^{-1})^{T}.\\]\nSince the order of the transpose and inverse operations does not matter, this matrix is sometimes written as \\(A^{-T}\\).\nInverse of matrix product.If \\(A\\) and \\(B\\) are invertible (hence, square) and of the same size, then \\(AB\\) is invertible, and\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\n\nDocument 2:\n----\n4. Express \\(2x_{1}^{2}-3x_{1}x_{2}-x_{2}^{2}\\) as a quadratic form, with symmetric coefficient matrix \\(A\\).\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n**10.38**: _Orthogonal matrix with nonnegative entries._ Suppose the \\(n\\times n\\) matrix \\(A\\) is orthogonal, and all of its entries are nonnegative, _i.e._, \\(A_{ij}\\geq 0\\) for \\(i,j=1,\\ldots,n\\). Show that \\(A\\) must be a permutation matrix, _i.e._, each entry is either \\(0\\) or \\(1\\), each row has exactly one entry with value one, and each column has exactly one entry with value one. (See page 132.)\n**10.39**: _Gram matrix and QR factorization._ Suppose the matrix \\(A\\) has linearly independent columns and QR factorization \\(A=QR\\). What is the relationship between the Gram matrix of \\(A\\) and the Gram matrix of \\(R\\)? What can you say about the angles between the columns of \\(A\\) and the angles between the columns of \\(R\\)?\n\n\n",
        "answer": 0.0,
        "rationale": {
            "Question": "If A and B are both orthogonal square matrices, and det A = -det B. What is det(A+B)? Return the numerical value.",
            "Answer": 0.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 9.3.3",
            "id": "mingyin/orthogonal-similarity1.json",
            "explanation": "NONE",
            "theorem": "orthogonal similarity",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "wenhuchen/gauss_lemma2.json",
        "question": "Is 10 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs 10 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.\n\nDocument 1:\n----\n1. \\(\\tilde{q}_{i}\\neq 0\\), so the linear dependence test in step 2 is not satisfied, and we do not have a divide-by-zero error in step 3.\n2. \\(q_{1},\\ldots,q_{i}\\) are orthonormal.\n3. \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i}\\).\n4. \\(q_{i}\\) is a linear combination of \\(a_{1},\\ldots,a_{i}\\).\nWe show this by induction. For \\(i=1\\), we have \\(\\tilde{q}_{1}=a_{1}\\). Since \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we must have \\(a_{1}\\neq 0\\), and therefore \\(\\tilde{q}_{1}\\neq 0\\), so assertion 1 holds. The single vector \\(q_{1}\\) (considered as a list with one element) is evidently orthonormal, since \\(\\|q_{1}\\|=1\\), so assertion 2 holds. We have \\(a_{1}=\\|\\tilde{q}_{1}\\|q_{1}\\), and \\(q_{1}=(1/\\|\\tilde{q}_{1}\\|)a_{1}\\), so assertions 3 and 4 hold.\nSuppose our assertion holds for some \\(i-1\\), with \\(i<k\\); we will show it holds for \\(i\\). If \\(\\tilde{q}_{i}=0\\), then \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i-1}\\) (from the first step in the algorithm); but each of these is (by the induction hypothesis) a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), so it follows that \\(a_{i}\\) is a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), which contradicts our assumption that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. So assertion 1 holds for \\(i\\).\nStep 3 of the algorithm ensures that \\(q_{1},\\ldots,q_{i}\\) are normalized; to show they are orthogonal we will show that \\(q_{i}\\perp q_{j}\\) for \\(j=1,\\ldots,i-1\\). (Our induction hypothesis tells us that \\(q_{r}\\perp q_{s}\\) for \\(r,s<i\\).) For any \\(j=1,\\ldots,i-1\\), we have (using step 1)\n\\[q_{j}^{T}\\tilde{q}_{i} = q_{j}^{T}a_{i}-(q_{1}^{T}a_{i})(q_{j}^{T}q_{1})-\\cdots-(q_{i-1}^ {T}a_{i})(q_{j}^{T}q_{i-1})\\] \\[= q_{j}^{T}a_{i}-q_{j}^{T}a_{i}=0,\\]\n\nDocument 2:\n----\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is 10 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | http://mathonline.wikidot.com/example-questions-regarding-gauss-s-lemma",
            "id": "wenhuchen/gauss_lemma2.json",
            "explanation": "NONE",
            "theorem": "gauss's lemma",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "jianyu_xu/integer_programming_2.json",
        "question": "Solve the following linear programming problems: maximize 3x + y subject to (1) -x + y <= 1, (2) 2x + y <= 4, (3) x>= 0 and y >= 0. What's [x, y] for the optimal solution?",
        "prediction": [
            1,
            2
        ],
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nSolve the following linear programming problems: maximize 3x + y subject to (1) -x + y <= 1, (2) 2x + y <= 4, (3) x>= 0 and y >= 0. What's [x, y] for the optimal solution?\n\nDocument 1:\n----\n* Show that \\(\\|x-\\tilde{x}\\|\\) does not depend on \\(b\\); it only depends on the matrix \\(A\\), \\(\\epsilon\\), and \\(j\\).\n* How would you find the index \\(j\\) that maximizes the value of \\(\\|x-\\tilde{x}\\|\\)? By part (a), your answer should be in terms of \\(A\\) (or quantities derived from \\(A\\)) and \\(\\epsilon\\) only. _Remark._ If a small change in the right-hand side vector \\(b\\) can lead to a large change in the solution, we say that the linear equations \\(Ax=b\\) are _poorly conditioned_ or _ill-conditioned_. As a practical matter it means that unless you are very confident in what the entries of \\(b\\) are, the solution \\(A^{-1}b\\) may not be useful in practice.\n* _Timing test._ Generate a random \\(n\\times n\\) matrix \\(A\\) and an \\(n\\)-vector \\(b\\), for \\(n=500\\), \\(n=1000\\), and \\(n=2000\\). For each of these, compute the solution \\(x=A^{-1}b\\) (for example using the backslash operator, if the software you are using supports it), and verify that \\(Ax-b\\) is (very) small. Report the time it takes to solve each of these three sets of linear equations, and for each one work out the implied speed of your processor in Gflop/s, based on the \\(2n^{3}\\) complexity of solving equations using the QR factorization.\n\nDocument 2:\n----\n## Chapter 18 Nonlinear least squares\nIn previous chapters we studied the problems of solving a set of linear equations or finding a least squares approximate solution to them. In this chapter we study extensions of these problems in which linear is replaced with nonlinear. These nonlinear problems are in general hard to solve exactly, but we describe a heuristic algorithm that often works well in practice.\n\n\n",
        "answer": [
            2,
            0
        ],
        "rationale": {
            "Question": "Solve the following linear programming problems: maximize 3x + y subject to (1) -x + y <= 1, (2) 2x + y <= 4, (3) x>= 0 and y >= 0. What's [x, y] for the optimal solution?",
            "Answer": [
                2,
                0
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "self",
            "id": "jianyu_xu/integer_programming_2.json",
            "explanation": "NONE",
            "theorem": "integer programming",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "wenhuchen/definite_matrix2.json",
        "question": "Consider the matrix of A=[[1, -1], [-1, 4]], is this a positive definite matrix?",
        "prediction": true,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider the matrix of A=[[1, -1], [-1, 4]], is this a positive definite matrix?\n\nDocument 1:\n----\n\\[x_{t+1}=Ax_{t}+Bu_{t},\\qquad y_{t}=Cx_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(x_{t}\\), input \\(u_{t}\\), and output \\(y_{t}\\). The second is given by\n\\[\\tilde{x}_{t+1}=\\tilde{A}\\tilde{x}_{t}+\\tilde{B}w_{t},\\qquad v_{t}=\\tilde{C} \\tilde{x}_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(\\tilde{x}_{t}\\), input \\(w_{t}\\), and output \\(v_{t}\\). We now connect the output of the first linear dynamical system to the input of the second one, which means we take \\(w_{t}=y_{t}\\). (This is called the _composition_ of the two systems.) Show that this composition can also be expressed as a linear dynamical system with state \\(z_{t}=(x_{t},\\tilde{x}_{t})\\), input \\(u_{t}\\), and output \\(v_{t}\\). (Give the state transition matrix, input matrix, and output matrix.)\n**10.23**: Suppose \\(A\\) is an \\(n\\times n\\) matrix that satisfies \\(A^{2}=0\\). Does this imply that \\(A=0\\)? (This is the case when \\(n=1\\).) If this is (always) true, explain why. If it is not, give a specific counterexample, _i.e._, a matrix \\(A\\) that is nonzero but satisfies \\(A^{2}=0\\).\n**10.24**: _Matrix power identity._ A student says that for any square matrix \\(A\\),\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n\nDocument 2:\n----\nwhere \\(\\lambda^{(k)}\\) is a positive parameter. We add an iteration superscript to the parameter \\(\\lambda\\) since it can take different values in different iterations. For \\(\\lambda^{(k)}\\) small, we primarily minimize the first term, the squared norm of the approximation; for \\(\\lambda^{(k)}\\) large, we choose \\(x^{(k+1)}\\) near \\(x^{(k)}\\). (For \\(\\lambda^{(k)}=0\\), this coincides with the next iterate in the basic Gauss-Newton algorithm.) The second term in (18.11) is sometimes called a _trust penalty_ term, since it penalizes choices of \\(x\\) that are far from \\(x^{(k)}\\), where we cannot trust the affine approximation. The parameter \\(\\lambda^{(k)}\\) is sometimes called the _trust parameter_ (although 'distrust parameter' is perhaps more accurate).\nComputing the minimizer of (18.11) is a multi-objective least squares or regularized least squares problem, and equivalent to minimizing\n\\[\\left\\|\\left[\\begin{array}{c}Df(x^{(k)})\\\\ \\sqrt{\\lambda^{(k)}}I\\end{array}\\right]x-\\left[\\begin{array}{c}Df(x^{(k)})x ^{(k)}-f(x^{(k)})\\\\ \\sqrt{\\lambda^{(k)}}x^{(k)}\\end{array}\\right]\\right\\|^{2}.\\]\nSince \\(\\lambda^{(k)}\\) is positive, the stacked matrix in this least squares problem has linearly independent columns, even when \\(Df(x^{(k)})\\) does not. It follows that the solution of the least squares problem exists and is unique. From the normal equations of the least squares problem we can derive a useful expression for \\(x^{(k+1)}\\):\n\\[\\left(Df(x^{(k)})^{T}Df(x^{(k)})+\\lambda^{(k)}I\\right)x^{(k+1)}\\] \\[= Df(x^{(k)})^{T}\\left(Df(x^{(k)})x^{(k)}-f(x^{(k)})\\right)+\\lambda ^{(k)}x^{(k)}\\] \\[= \\left(Df(x^{(k)})^{T}Df(x^{(k)})+\\lambda^{(k)}I\\right)x^{(k)}-Df( x^{(k)})^{T}f(x^{(k)}),\\]\nand therefore \\[x^{(k+1)}=x^{(k)}-\\left(Df(x^{(k)})^{T}Df(x^{(k)})+\\lambda^{(k)}I\\right)^{-1}Df(x^ {(k)})^{T}f(x^{(k)}).\\] (18.12) The matrix inverse here always exists. From (18.12), we see that \\(x^{(k+1)}=x^{(k)}\\) only if \\(2Df(x^{(k)})^{T}f(x^{(k)})=0\\), _i.e._, only when the optimality condition (18.3) holds for \\(x^{(k)}\\). So like the Gauss-Newton algorithm, the Levenberg-Marquardt algorithm stops (or more accurately, repeats itself with \\(x^{(k+1)}=x^{(k)}\\)) only when the optimality condition (18.3) holds.\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Consider the matrix of A=[[1, -1], [-1, 4]], is this a positive definite matrix?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | https://www.math.usm.edu/lambers/mat419/lecture3.pdf",
            "id": "wenhuchen/definite_matrix2.json",
            "explanation": "NONE",
            "theorem": "definite matrix criteria",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "wenhuchen/vieta's_formula.json",
        "question": "Let m and n be the roots of the equation 2x^2 + 15x + 16 = 0. What is the value of 1/m + 1/n?",
        "prediction": -4,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet m and n be the roots of the equation 2x^2 + 15x + 16 = 0. What is the value of 1/m + 1/n?\n\nDocument 1:\n----\nThe second step cannot fail, provided the assumption (16.5) holds. Let us analyze the complexity of this algorithm. The first step, forming the Gram matrix, requires \\(mn^{2}\\) flops (see page 16.3). The second step requires the solution of a square system of \\(n+p\\) equations, which costs \\(2(n+p)^{3}\\) flops, so the total is\n\\[mn^{2}+2(n+p)^{3}\\]\nflops. This grows linearly in \\(m\\) and cubicly in \\(n\\) and \\(p\\). The assumption (16.5) implies \\(p\\leq n\\), so in terms of order, \\((n+p)^{3}\\) can be replaced with \\(n^{3}\\).\nSolving constrained least squares problems via QR factorization.We now give a method for solving the constrained least squares problem that generalizes the QR factorization method for least squares problems (algorithm 12.1). We assume that \\(A\\) and \\(C\\) satisfy the conditions (16.5).\nWe start by rewriting the KKT equations (16.4) as\n\\[2(A^{T}A+C^{T}C)\\hat{x}+C^{T}w=2A^{T}b,\\qquad C\\hat{x}=d\\] (16.7)\nwith a new variable \\(w=\\hat{z}-2d\\). To obtain (16.7) we multiplied the equation \\(C\\hat{x}=d\\) on the left by \\(2C^{T}\\), then added the result to the first equation of (16.4), and replaced the variable \\(\\hat{z}\\) with \\(w+2d\\).\nNext we use the QR factorization\n\\[\\left[\\begin{array}{c}A\\\\ C\\end{array}\\right]=QR=\\left[\\begin{array}{c}Q_{1}\\\\ Q_{2}\\end{array}\\right]R\\] (16.8)\nto simplify (16.7). This factorization exists because the stacked matrix has linearly independent columns, by our assumption (16.5). In (16.8) we also partition \\(Q\\) in two blocks \\(Q_{1}\\) and \\(Q_{2}\\), of size \\(m\\times n\\) and \\(p\\times n\\), respectively. If we make the substitutions \\(A=Q_{1}R\\), \\(C=Q_{2}R\\), and \\(A^{T}A+C^{T}C=R^{T}R\\) in (16.7) we obtain\n\\[2R^{T}R\\hat{x}+R^{T}Q_{2}^{T}w=2R^{T}Q_{1}^{T}b,\\qquad Q_{2}R\\hat{x}=d.\\]\nWe multiply the first equation on the left by \\(R^{-T}\\) (which we know exists) to get\n\nDocument 2:\n----\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\nA simple solution of these equations is \\(a=0\\), \\(b=0\\). But we seek a nonzero solution. We can set one of the coefficients, say \\(a_{1}\\), to be one. (This might cause the other quantities to be fractional-valued.) We can add the condition that \\(a_{1}=1\\) to our system of linear equations as\n\\[\\left[\\begin{array}{cc}R&-P\\\\ e_{1}^{T}&0\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=e_{m+1}.\\]\nFinally, we have a set of \\(m+1\\) equations in \\(p+q\\) variables that expresses the requirement that the chemical reaction balances. Finding a solution of this set of equations is called _balancing_ the chemical reaction.\nFor the example of electrolysis of water described above, we have \\(p=1\\) reactant (water) and \\(q=2\\) products (molecular hydrogen and oxygen). The reaction involves \\(m=2\\) atoms, hydrogen and oxygen. The reactant and product matrices are\n\n\n",
        "answer": -0.9375,
        "rationale": {
            "Question": "Let m and n be the roots of the equation 2x^2 + 15x + 16 = 0. What is the value of 1/m + 1/n?",
            "Answer": -0.9375,
            "Answer_type": "float",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_7_4.json",
        "question": "What are the real eigenvalues of the matrix [[3, -2, 5], [1, 0, 7], [0, 0, 2]]?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat are the real eigenvalues of the matrix [[3, -2, 5], [1, 0, 7], [0, 0, 2]]?\n\nDocument 1:\n----\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\nTwo matrices are equal if they have the same size, and the corresponding entries are all equal. As with vectors, we normally deal with matrices with entries that\nare real numbers, which will be our assumption unless we state otherwise. The set of real \\(m\\times n\\) matrices is denoted \\(\\mathbf{R}^{m\\times n}\\). But matrices with complex entries, for example, do arise in some applications.\nMatrix indexing.As with vectors, standard mathematical notation indexes the rows and columns of a matrix starting from 1. In computer languages, matrices are often (but not always) stored as 2-dimensional arrays, which can be indexed in a variety of ways, depending on the language. Lower level languages typically use indices starting from 0; higher level languages and packages that support matrix operations usually use standard mathematical indexing, starting from 1.\nSquare, tall, and wide matrices.A _square_ matrix has an equal number of rows and columns. A square matrix of size \\(n\\times n\\) is said to be of _order_\\(n\\). A _tall_ matrix has more rows than columns (size \\(m\\times n\\) with \\(m>n\\)). A _wide_ matrix has more columns than rows (size \\(m\\times n\\) with \\(n>m\\)).\n\nDocument 2:\n----\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\n\n\n",
        "answer": [
            1,
            2,
            2
        ],
        "rationale": {
            "Question": "What are the real eigenvalues of the matrix [[3, -2, 5], [1, 0, 7], [0, 0, 2]]?",
            "Answer": [
                1,
                2,
                2
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_4.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "wenhuchen/determinant1.json",
        "question": "For matrix A = [[2, 4, 3], [3, 0, 1], [1, 2, 5]], what is its determinant?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[2, 4, 3], [3, 0, 1], [1, 2, 5]], what is its determinant?\n\nDocument 1:\n----\n* _Entries of matrix triple product_. (See page 10.) Suppose \\(A\\) has dimensions \\(m\\times n\\), \\(B\\) has dimensions \\(n\\times p\\), \\(C\\) has dimensions \\(p\\times q\\), and let \\(D=ABC\\). Show that \\[D_{ij}=\\sum_{k=1}^{n}\\sum_{l=1}^{p}A_{ik}B_{kl}C_{lj}.\\] This is the formula analogous to (10.1) for the product of two matrices.\n* _Multiplication by a diagonal matrix_. Suppose that \\(A\\) is an \\(m\\times n\\) matrix, \\(D\\) is a diagonal matrix, and \\(B=DA\\). Describe \\(B\\) in terms of \\(A\\) and the entries of \\(D\\). You can refer to the rows or columns or entries of \\(A\\).\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n\nDocument 2:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\n\n",
        "answer": -42,
        "rationale": {
            "Question": "For matrix A = [[2, 4, 3], [3, 0, 1], [1, 2, 5]], what is its determinant?",
            "Answer": -42,
            "Picture": null,
            "Answer_type": "integer",
            "source": "self",
            "id": "wenhuchen/determinant1.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "wenhuchen/vieta's_formula5.json",
        "question": "The product of two of the four roots of the quartic equation x^4 - 18x^3 +kx2 + 200x - 1984 = 0 is -32. Determine the value of k.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nThe product of two of the four roots of the quartic equation x^4 - 18x^3 +kx2 + 200x - 1984 = 0 is -32. Determine the value of k.\n\nDocument 1:\n----\n\\[2 = (1)(2)\\] \\[1 = (1)(1)+(0)(2)\\] \\[-3 = (1)(-1)+(0)(1)+(-1)(2)\\] \\[-1 = (0)(-1)+(-1)(1)\\] \\[1 = (-1)(-1).\\]\nPolynomial multiplication.If \\(a\\) and \\(b\\) represent the coefficients of two polynomials\n\\[p(x)=a_{1}+a_{2}x+\\cdots+a_{n}x^{n-1},\\qquad q(x)=b_{1}+b_{2}x+\\cdots+b_{m}x^{ m-1},\\]\nthen the coefficients of the product polynomial \\(p(x)q(x)\\) are represented by \\(c=a*b\\):\n\\[p(x)q(x)=c_{1}+c_{2}x+\\cdots+c_{n+m-1}x^{n+m-2}.\\]\nTo see this we will show that \\(c_{k}\\) is the coefficient of \\(x^{k-1}\\) in \\(p(x)q(x)\\). We expand the product polynomial into \\(mn\\) terms, and collect those terms associated with \\(x^{k-1}\\). These terms have the form \\(a_{i}b_{j}x^{i+j-2}\\), for \\(i\\) and \\(j\\) that satisfy \\(i+j-2=k-1\\), _i.e._, \\(i+j=k-1\\). It follows that \\(c_{k}=\\sum_{i+j=k+1}a_{i}b_{j}\\), which agrees with the convolution formula (7.2).\nProperties of convolution.Convolution is symmetric: We have \\(a*b=b*a\\). It is also associative: We have \\((a*b)*c=a*(b*c)\\), so we can write both as \\(a*b*c\\). Another property is that \\(a*b=0\\) implies that either \\(a=0\\) or \\(b=0\\). These properties follow from the polynomial coefficient property above, and can also be directly shown. As an example, let us show that \\(a*b=b*a\\). Suppose \\(p\\) is the polynomial with coefficients \\(a\\), and \\(q\\) is the polynomial with coefficients \\(b\\). The two polynomials \\(p(x)q(x)\\) and \\(q(x)p(x)\\) are the same (since multiplication of numbers is commutative), so they have the same coefficients. The coefficients of \\(p(x)q(x)\\) are \\(a*b\\) and the coefficients of \\(q(x)p(x)\\) are \\(b*a\\). These must be the same.\n\nDocument 2:\n----\n1. Show that every vector \\(x\\) can be decomposed in a unique way as a sum \\(x=x_{\\rm s}+x_{\\rm a}\\) of a symmetric vector \\(x_{\\rm s}\\) and an anti-symmetric vector \\(x_{\\rm a}\\).\n2. Show that the symmetric and anti-symmetric parts \\(x_{\\rm s}\\) and \\(x_{\\rm a}\\) are linear functions of \\(x\\). Give matrices \\(A_{\\rm s}\\) and \\(A_{\\rm a}\\) such that \\(x_{\\rm s}=A_{\\rm s}x\\) and \\(x_{\\rm a}=A_{\\rm a}x\\) for all \\(x\\).\n**8.6**: _Linear functions._ For each description of \\(y\\) below, express it as \\(y=Ax\\) for some \\(A\\). (You should specify \\(A\\).)\n1. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average of \\(x_{1},\\ldots,x_{i-1}\\). (We take \\(y_{1}=x_{1}\\).)\n2. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average value of all other \\(x_{j}\\)s, _i.e._, the average of \\(x_{1},\\ldots,x_{i-1},x_{i+1},\\ldots,x_{n}\\).\n**8.7**: _Interpolation of polynomial values and derivatives._ The 5-vector \\(c\\) represents the coefficients of a quartic polynomial \\(p(x)=c_{1}+c_{2}x+c_{3}x^{2}+c_{4}x^{3}+c_{5}x^{4}\\). Express the conditions \\[p(0)=0,\\quad p^{\\prime}(0)=0,\\quad p(1)=1,\\quad p^{\\prime}(1)=0,\\] as a set of linear equations of the form \\(Ac=b\\). Is the system of equations underdetermined, over-determined, or square?\n**8.8**: _Interpolation of rational functions._ A _rational function_ of degree two has the form\n\\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}},\\]\nwhere \\(c_{1},c_{2},c_{3},d_{1},d_{2}\\) are coefficients. ('Rational' refers to the fact that \\(f\\) is a ratio of polynomials. Another name for \\(f\\) is _bi-quadratic_.) Consider the interpolation conditions\n\\[f(t_{i})=y_{i},\\quad i=1,\\ldots,K,\\]\n\n\n",
        "answer": 86,
        "rationale": {
            "Question": "The product of two of the four roots of the quartic equation x^4 - 18x^3 +kx2 + 200x - 1984 = 0 is -32. Determine the value of k.",
            "Answer": 86,
            "Answer_type": "integer",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula5.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_4.json",
        "question": "Is the set of 3 * 3 matrices in reduced row-echelon form a subspace of R^{3 * 3}?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs the set of 3 * 3 matrices in reduced row-echelon form a subspace of R^{3 * 3}?\n\nDocument 1:\n----\nWe have now reduced the problem of minimizing the weighted sum least squares objective to a standard least squares problem. Provided the columns of \\(\\tilde{A}\\) are linearly independent, the minimizer is unique, and given by\n\\[\\hat{x} = (\\tilde{A}^{T}\\tilde{A})^{-1}\\tilde{A}^{T}\\tilde{b}\\] (15.3) \\[= (\\lambda_{1}A_{1}^{T}A_{1}+\\cdots+\\lambda_{k}A_{k}^{T}A_{k})^{-1 }(\\lambda_{1}A_{1}^{T}b_{1}+\\cdots+\\lambda_{k}A_{k}^{T}b_{k}).\\]\nThis reduces to our standard formula for the solution of a least squares problem when \\(k=1\\) and \\(\\lambda_{1}=1\\). (In fact, when \\(k=1\\), \\(\\lambda_{1}\\) does not matter.) We can compute \\(\\hat{x}\\) via the QR factorization of \\(\\tilde{A}\\).\nIndependent columns of stacked matrix.Our assumption (12.2) that the columns of \\(\\tilde{A}\\) in (15.2) are linearly independent is not the same as assuming that each of \\(A_{1},\\ldots,A_{k}\\) has linearly independent columns. We can state the condition that \\(\\tilde{A}\\) has linearly independent columns as: There is no nonzero vector \\(x\\) that satisfies\n\\(A_{i}x=0\\) for \\(i=1,\\ldots,k\\). This implies that if just _one_ of the matrices \\(A_{1},\\ldots,A_{k}\\) has linearly independent columns, then \\(\\tilde{A}\\) does.\nThe stacked matrix \\(\\tilde{A}\\) can have linearly independent columns even when none of the matrices \\(A_{1},\\ldots,A_{k}\\) do. This can happen when \\(m_{i}<n\\) for all \\(i\\), _i.e._, all \\(A_{i}\\) are wide. However, we must have \\(m_{1}+\\cdots+m_{k}\\geq n\\), since \\(\\tilde{A}\\) must be tall or square for the linearly independent columns assumption to hold.\nOptimal trade-off curve.We start with the special case of two objectives (also called the _bi-criterion problem_), and write the weighted sum objective as\n\\[J=J_{1}+\\lambda J_{2}=\\|A_{1}x-b_{1}\\|^{2}+\\lambda\\|A_{2}x-b_{2}\\|^{2},\\]\n\nDocument 2:\n----\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\nInvertibility conditions.For square matrices, left-invertibility, right-invertibility, and invertibility are equivalent: If a matrix is square and left-invertible, then it is also right-invertible (and therefore invertible) and vice-versa.\nTo see this, suppose \\(A\\) is an \\(n\\times n\\) matrix and left-invertible. This implies that the \\(n\\) columns of \\(A\\) are linearly independent. Therefore they form a basis and so any \\(n\\)-vector can be expressed as a linear combination of the columns of \\(A\\). In particular, each of the \\(n\\) unit vectors \\(e_{i}\\) can be expressed as \\(e_{i}=Ab_{i}\\) for some \\(n\\)-vector \\(b_{i}\\). The matrix \\(B=\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\) satisfies\n\\[AB=\\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right]= \\left[\\begin{array}{cccc}e_{1}&e_{2}&\\cdots&e_{n}\\end{array}\\right]=I.\\]\nSo \\(B\\) is a right inverse of \\(A\\).\nWe have just shown that for a square matrix \\(A\\),\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is the set of 3 * 3 matrices in reduced row-echelon form a subspace of R^{3 * 3}?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_4.json",
            "explanation": "NONE",
            "theorem": "linear subspaces",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_3_5.json",
        "question": "Consider an m * n matrix A and an n * m matrix B (with n != m) such that AB = I_m. Are the columns of A linearly independent?",
        "prediction": true,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider an m * n matrix A and an n * m matrix B (with n != m) such that AB = I_m. Are the columns of A linearly independent?\n\nDocument 1:\n----\nto simplify (16.7). This factorization exists because the stacked matrix has linearly independent columns, by our assumption (16.5). In (16.8) we also partition \\(Q\\) in two blocks \\(Q_{1}\\) and \\(Q_{2}\\), of size \\(m\\times n\\) and \\(p\\times n\\), respectively. If we make the substitutions \\(A=Q_{1}R\\), \\(C=Q_{2}R\\), and \\(A^{T}A+C^{T}C=R^{T}R\\) in (16.7) we obtain\n\\[2R^{T}R\\hat{x}+R^{T}Q_{2}^{T}w=2R^{T}Q_{1}^{T}b,\\qquad Q_{2}R\\hat{x}=d.\\]\nWe multiply the first equation on the left by \\(R^{-T}\\) (which we know exists) to get\n\\[R\\hat{x}=Q_{1}^{T}b-(1/2)Q_{2}^{T}w.\\] (16.9)\nSubstituting this expression into \\(Q_{2}R\\hat{x}=d\\) gives an equation in \\(w\\):\n\\[Q_{2}Q_{2}^{T}w=2Q_{2}Q_{1}^{T}b-2d.\\] (16.10)\nWe now use the second part of the assumption (16.5) to show that the matrix \\(Q_{2}^{T}=R^{-T}C^{T}\\) has linearly independent columns. Suppose \\(Q_{2}^{T}z=R^{-T}C^{T}z=0\\). Multiplying with \\(R^{T}\\) gives \\(C^{T}z=0\\). Since \\(C\\) has linearly independent rows, this implies \\(z=0\\), and we conclude that the columns of \\(Q_{2}^{T}\\) are linearly independent.\nThe matrix \\(Q_{2}^{T}\\) therefore has a QR factorization \\(Q_{2}^{T}=\\tilde{Q}\\tilde{R}\\). Substituting this into (16.10) gives\n\\[\\tilde{R}^{T}\\tilde{R}w=2\\tilde{R}^{T}\\tilde{Q}^{T}Q_{1}^{T}b-2d,\\]\nwhich we can write as\n\\[\\tilde{R}w=2\\tilde{Q}^{T}Q_{1}^{T}b-2\\tilde{R}^{-T}d.\\]\nWe can use this to compute \\(w\\), first by computing \\(\\tilde{R}^{-T}d\\) (by forward substitution), then forming the right-hand side, and then solving for \\(w\\) using back substitution. Once we know \\(w\\), we can find \\(\\hat{x}\\) from (16.9). The method is summarized in the following algorithm.\n**Algorithm 16.2** Constrained least squares via QR factorization\n\nDocument 2:\n----\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\nInvertibility conditions.For square matrices, left-invertibility, right-invertibility, and invertibility are equivalent: If a matrix is square and left-invertible, then it is also right-invertible (and therefore invertible) and vice-versa.\nTo see this, suppose \\(A\\) is an \\(n\\times n\\) matrix and left-invertible. This implies that the \\(n\\) columns of \\(A\\) are linearly independent. Therefore they form a basis and so any \\(n\\)-vector can be expressed as a linear combination of the columns of \\(A\\). In particular, each of the \\(n\\) unit vectors \\(e_{i}\\) can be expressed as \\(e_{i}=Ab_{i}\\) for some \\(n\\)-vector \\(b_{i}\\). The matrix \\(B=\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\) satisfies\n\\[AB=\\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right]= \\left[\\begin{array}{cccc}e_{1}&e_{2}&\\cdots&e_{n}\\end{array}\\right]=I.\\]\nSo \\(B\\) is a right inverse of \\(A\\).\nWe have just shown that for a square matrix \\(A\\),\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Consider an m * n matrix A and an n * m matrix B (with n != m) such that AB = I_m. Are the columns of A linearly independent?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_5.json",
            "explanation": "NONE",
            "theorem": "linear independence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "mingyin/linear-dependence1.json",
        "question": "Finding all the real roots of the equation $\\sqrt{x^2+x+1}+\\sqrt{2 x^2+x+5}=\\sqrt{x^2-3 x+13}$. Return the answer as a list with ascending order.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFinding all the real roots of the equation $\\sqrt{x^2+x+1}+\\sqrt{2 x^2+x+5}=\\sqrt{x^2-3 x+13}$. Return the answer as a list with ascending order.\n\nDocument 1:\n----\n**11.10**: _Reverse-time linear dynamical system._ A linear dynamical system has the form \\[x_{t+1}=Ax_{t},\\] where \\(x_{t}\\) in the (\\(n\\)-vector) state in period \\(t\\), and \\(A\\) is the \\(n\\times n\\) dynamics matrix. This formula gives the state in the next period as a function of the current state. We want to derive a recursion of the form \\[x_{t-1}=A^{\\rm rev}x_{t},\\] which gives the previous state as a function of the current state. We call this the _reverse time linear dynamical system_. 1. When is this possible? When it is possible, what is \\(A^{\\rm rev}\\)? 2. For the specific linear dynamical system with dynamics matrix \\[A=\\left[\\begin{array}{cc}3&2\\\\ -1&4\\end{array}\\right],\\] find \\(A^{\\rm rev}\\), or explain why the reverse time linear dynamical system doesn't exist.\n**11.11**: _Interpolation of rational functions._ (Continuation of exercise 8.8.) Find a rational function \\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}}\\] that satisfies the following interpolation conditions: \\[f(1)=2,\\qquad f(2)=5,\\qquad f(3)=9,\\qquad f(4)=-1,\\qquad f(5)=-4.\\] In exercise 8.8 these conditions were expressed as a set of linear equations in the coefficients \\(c_{1}\\), \\(c_{2}\\), \\(c_{3}\\), \\(d_{1}\\) and \\(d_{2}\\); here we are asking you to form and (numerically) solve the system of equations. Plot the rational function you find over the range \\(x=0\\) to \\(x=6\\). Your plot should include markers at the interpolation points \\((1,2),\\ldots,(5,-4)\\). (Your rational function graph should pass through these points.)\n\nDocument 2:\n----\n**10.36**: _Quadratic form._ Suppose \\(A\\) is an \\(n\\times n\\) matrix and \\(x\\) is an \\(n\\)-vector. The triple product \\(x^{T}Ax\\), a \\(1\\times 1\\) matrix which we consider to be a scalar (_i.e._, number), is called a _quadratic form_ of the vector \\(x\\), with coefficient matrix \\(A\\). A quadratic form is the vector analog of a quadratic function \\(\\alpha u^{2}\\), where \\(\\alpha\\) and \\(u\\) are both numbers. Quadratic forms arise in many fields and applications.\n1. Show that \\(x^{T}Ax=\\sum_{i,j=1}^{n}A_{ij}x_{i}x_{j}\\).\n2. Show that \\(x^{T}(A^{T})x=x^{T}Ax\\). In other words, the quadratic form with the transposed coefficient matrix has the same value for any \\(x\\). _Hint._ Take the transpose of the triple product \\(x^{T}Ax\\).\n3. Show that \\(x^{T}((A+A^{T})/2)x=x^{T}Ax\\). In other words, the quadratic form with coefficient matrix equal to the symmetric part of a matrix (_i.e._, \\((A+A^{T})/2\\)) has the same value as the original quadratic form.\n4. Express \\(2x_{1}^{2}-3x_{1}x_{2}-x_{2}^{2}\\) as a quadratic form, with symmetric coefficient matrix \\(A\\).\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n\n\n",
        "answer": [
            -1.7807764064,
            0.2807764064
        ],
        "rationale": {
            "Question": "Finding all the real roots of the equation $\\sqrt{x^2+x+1}+\\sqrt{2 x^2+x+5}=\\sqrt{x^2-3 x+13}$. Return the answer as a list with ascending order.",
            "Answer": [
                -1.7807764064,
                0.2807764064
            ],
            "Picture": null,
            "Answer_type": "list of float",
            "source": "linear algebra 2.8 example 2",
            "id": "mingyin/linear-dependence1.json",
            "explanation": "NONE",
            "theorem": "linear dependence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of float",
        "correct": false
    },
    {
        "id": "wenhuchen/vieta's_formula4.json",
        "question": "If the quartic x^4 + 3x^3 + 11x^2 + 9x + A has roots k, l, m, and n such that kl = mn, find A.",
        "prediction": -12,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIf the quartic x^4 + 3x^3 + 11x^2 + 9x + A has roots k, l, m, and n such that kl = mn, find A.\n\nDocument 1:\n----\nthen the coefficients of the product polynomial \\(p(x)q(x)\\) are represented by \\(c=a*b\\):\n\\[p(x)q(x)=c_{1}+c_{2}x+\\cdots+c_{n+m-1}x^{n+m-2}.\\]\nTo see this we will show that \\(c_{k}\\) is the coefficient of \\(x^{k-1}\\) in \\(p(x)q(x)\\). We expand the product polynomial into \\(mn\\) terms, and collect those terms associated with \\(x^{k-1}\\). These terms have the form \\(a_{i}b_{j}x^{i+j-2}\\), for \\(i\\) and \\(j\\) that satisfy \\(i+j-2=k-1\\), _i.e._, \\(i+j=k-1\\). It follows that \\(c_{k}=\\sum_{i+j=k+1}a_{i}b_{j}\\), which agrees with the convolution formula (7.2).\nProperties of convolution.Convolution is symmetric: We have \\(a*b=b*a\\). It is also associative: We have \\((a*b)*c=a*(b*c)\\), so we can write both as \\(a*b*c\\). Another property is that \\(a*b=0\\) implies that either \\(a=0\\) or \\(b=0\\). These properties follow from the polynomial coefficient property above, and can also be directly shown. As an example, let us show that \\(a*b=b*a\\). Suppose \\(p\\) is the polynomial with coefficients \\(a\\), and \\(q\\) is the polynomial with coefficients \\(b\\). The two polynomials \\(p(x)q(x)\\) and \\(q(x)p(x)\\) are the same (since multiplication of numbers is commutative), so they have the same coefficients. The coefficients of \\(p(x)q(x)\\) are \\(a*b\\) and the coefficients of \\(q(x)p(x)\\) are \\(b*a\\). These must be the same.\nA basic property is that for fixed \\(a\\), the convolution \\(a*b\\) is a linear function of \\(b\\); and for fixed \\(b\\), it is a linear function of \\(a\\). This means we can express \\(a*b\\) as a matrix-vector product:\n\nDocument 2:\n----\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n\n\n",
        "answer": 9,
        "rationale": {
            "Question": "If the quartic x^4 + 3x^3 + 11x^2 + 9x + A has roots k, l, m, and n such that kl = mn, find A.",
            "Answer": 9,
            "Answer_type": "integer",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula4.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_1_2.json",
        "question": "Suppose the graph of a polynomial f(t) = a + bt + ct^2 passes through points (1, -1), (2, 3), and (3, 13). What is f(-1)?",
        "prediction": 7,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nSuppose the graph of a polynomial f(t) = a + bt + ct^2 passes through points (1, -1), (2, 3), and (3, 13). What is f(-1)?\n\nDocument 1:\n----\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\nDocument 2:\n----\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\n\n\n",
        "answer": 9,
        "rationale": {
            "Question": "Suppose the graph of a polynomial f(t) = a + bt + ct^2 passes through points (1, -1), (2, 3), and (3, 13). What is f(-1)?",
            "Answer": 9,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_1_2.json",
            "explanation": "solutions/math_algebra_1_2.txt",
            "theorem": "linear systems",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "mingyin/minimal-polynomial1.json",
        "question": "matrix $A=(\\begin{array}{rrrr} -2 & -1 & -1 & -1 \\ 2 & 1 & 3 & 2 \\ 1 & 1 & 0 & 1 \\ -1 & -1 & -2 & -2 \\end{array})$. Suppose f is the minimal polynomial of A. What is f(99)? Return the numeric without explanation.",
        "prediction": -16,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nmatrix $A=(\\begin{array}{rrrr} -2 & -1 & -1 & -1 \\ 2 & 1 & 3 & 2 \\ 1 & 1 & 0 & 1 \\ -1 & -1 & -2 & -2 \\end{array})$. Suppose f is the minimal polynomial of A. What is f(99)? Return the numeric without explanation.\n\nDocument 1:\n----\n(Note that here, \\(x^{i}\\) means the generic scalar value \\(x\\) raised to the \\(i\\)th power; \\(x^{(i)}\\) means the \\(i\\)th observed scalar data value.) In this case the matrix \\(A\\) in (13.1) has the form\n\\[A=\\left[\\begin{array}{ccccc}1&x^{(1)}&\\cdots&(x^{(1)})^{p-1}\\\\ 1&x^{(2)}&\\cdots&(x^{(2)})^{p-1}\\\\ \\vdots&\\vdots&&\\vdots\\\\ 1&x^{(N)}&\\cdots&(x^{(N)})^{p-1}\\end{array}\\right],\\]\n_i.e._, it is a Vandermonde matrix (see (6.7)). Its columns are linearly independent provided the numbers \\(x^{(1)},\\ldots,x^{(N)}\\) include at least \\(p\\) different values. Figure 13.6 shows an example of the least squares fit of polynomials of degree 2, 6, 10, and 15 to a set of 100 data points. Since any polynomial of degree less than \\(r\\) is also a polynomial of degree less than \\(s\\), for \\(r\\leq s\\), it follows that the RMS fit attained by a polynomial with a larger degree is smaller (or at least, no larger) than that obtained by a fit with a smaller degree polynomial. This suggests that we should use the largest degree polynomial that we can, since this results in the smallest residual and the best RMS fit. But we will see in SS13.2 that this is not true, and explore rational methods for choosing a model from among several candidates.\nPiecewise-linear fit.A _piecewise-linear_ function, with _knot points_ or _kink points_\\(a_{1}<a_{2}<\\cdots<a_{k}\\), is a continuous function that is affine in between the knot points. (Such functions should be called piecewise-affine.) We can describe any\n\nDocument 2:\n----\nThe complexity of this algorithm is dominated by the cost of the QR factorization in step 1, _i.e._, \\(2np^{2}\\) flops.\n## 16 Constrained least squares\n### 16.1 Smallest right inverse\nSuppose the \\(m\\times n\\) matrix \\(A\\) is wide, with linearly independent rows. Its pseudo-inverse \\(A^{\\dagger}\\) is a right inverse of \\(A\\). In fact, there are many right inverses of \\(A\\) and it turns out that \\(A^{\\dagger}\\) is the smallest one among them, as measured by the matrix norm. In other words, if \\(X\\) satisfies \\(AX=I\\), then \\(\\|X\\|\\geq\\|A^{\\dagger}\\|\\). You will show this in this problem.\n1. Suppose \\(AX=I\\), and let \\(x_{1},\\ldots,x_{m}\\) denote the columns of \\(X\\). Let \\(b_{j}\\) denote the \\(j\\)th column of \\(A^{\\dagger}\\). Explain why \\(\\|x_{j}\\|^{2}\\geq\\|b_{j}\\|^{2}\\). _Hint_. Show that \\(z=b_{j}\\) is the vector of smallest norm that satisfies \\(Az=e_{j}\\), for \\(j=1,\\ldots,m\\).\n2. Use the inequalities from part (a) to establish \\(\\|X\\|\\geq\\|A^{\\dagger}\\|\\).\n3. _Matrix least norm problem_. The matrix least norm problem is minimize \\[\\|X\\|^{2}\\] subject to \\[CX=D,\\] where the variable to be chosen is the \\(n\\times k\\) matrix \\(X\\); the \\(p\\times n\\) matrix \\(C\\) and the \\(p\\times k\\) matrix \\(D\\) are given. Show that the solution of this problem is \\(\\hat{X}=C^{\\dagger}D\\), assuming the rows of \\(C\\) are linearly independent. _Hint_. Show that we can find the columns of \\(X\\) independently, by solving a least norm problem for each one.\n\n\n",
        "answer": 990000.0,
        "rationale": {
            "Question": "matrix $A=(\\begin{array}{rrrr} -2 & -1 & -1 & -1 \\ 2 & 1 & 3 & 2 \\ 1 & 1 & 0 & 1 \\ -1 & -1 & -2 & -2 \\end{array})$. Suppose f is the minimal polynomial of A. What is f(99)? Return the numeric without explanation.",
            "Answer": 990000.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 7.1 example 1",
            "id": "mingyin/minimal-polynomial1.json",
            "explanation": "NONE",
            "theorem": "minimal polynomial",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_6_2.json",
        "question": "Let V be the space spanned by functions cos(2x) and sin(2x). Find the determinant of the linear transformation D(f) = f' from V to V.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet V be the space spanned by functions cos(2x) and sin(2x). Find the determinant of the linear transformation D(f) = f' from V to V.\n\nDocument 1:\n----\nholds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). It is a good exercise to parse this simple looking equation, since it involves overloading of notation. On the left-hand side, the scalar-vector multiplications \\(\\alpha x\\) and \\(\\beta y\\) involve \\(n\\)-vectors, and the sum \\(\\alpha x+\\beta y\\) is the sum of two \\(n\\)-vectors. The function \\(f\\) maps \\(n\\)-vectors to \\(m\\)-vectors, so \\(f(\\alpha x+\\beta y)\\) is an \\(m\\)-vector. On the right-hand side, the scalar-vector multiplications and the sum are those for \\(m\\)-vectors. Finally, the equality sign is equality between two \\(m\\)-vectors.\nWe can verify that superposition holds for \\(f\\) using properties of matrix-vector and scalar-vector multiplication:\n\\[f(\\alpha x+\\beta y) = A(\\alpha x+\\beta y)\\] \\[= A(\\alpha x)+A(\\beta y)\\] \\[= \\alpha(Ax)+\\beta(Ay)\\] \\[= \\alpha f(x)+\\beta f(y)\\]\nThus we can associate with every matrix \\(A\\) a linear function \\(f(x)=Ax\\).\nThe converse is also true. Suppose \\(f\\) is a function that maps \\(n\\)-vectors to \\(m\\)-vectors, and is linear, _i.e._, (8.1) holds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). Then there exists an \\(m\\times n\\) matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\). This can be shown in the same way as for scalar-valued functions in SS2.1, by showing that if \\(f\\) is linear, then\n\\[f(x)=x_{1}f(e_{1})+x_{2}f(e_{2})+\\cdots+x_{n}f(e_{n}),\\] (8.2)\nwhere \\(e_{k}\\) is the \\(k\\)th unit vector of size \\(n\\). The right-hand side can also be written as a matrix-vector product \\(Ax\\), with\n\\[A=\\left[\\begin{array}{cccc}f(e_{1})&f(e_{2})&\\cdots&f(e_{n})\\end{array} \\right].\\]\n\nDocument 2:\n----\nNote that in each of the three equations above, the vectors appearing in the left- and right-hand sides have different dimensions, \\(m\\) on the left and \\(n\\) on the right.\nWe can verify these properties using simple matrix properties. We start with the second statement, that multiplication by \\(A\\) preserves the inner product. We have\n\\[(Ax)^{T}(Ay) = (x^{T}A^{T})(Ay)\\] \\[= x^{T}(A^{T}A)y\\] \\[= x^{T}Iy\\] \\[= x^{T}y.\\]\nIn the first line, we use the transpose-of-product rule; in the second, we re-associate a product of 4 matrices (considering the row vector \\(x^{T}\\) and column vector \\(x\\) as matrices); in the third line, we use \\(A^{T}A=I\\); and in the fourth line, we use \\(Iy=y\\).\nFrom the second property we can derive the first one: By taking \\(y=x\\) we get \\((Ax)^{T}(Ax)=x^{T}x\\); taking the squareroot of each side gives \\(\\|Ax\\|=\\|x\\|\\). The third property, angle preservation, follows from the first two, since\n\\[\\angle(Ax,Ay)=\\arccos\\left(\\frac{(Ax)^{T}(Ay)}{\\|Ax\\|\\|Ay\\|}\\right)=\\arccos \\left(\\frac{x^{T}y}{\\|x\\|\\|y\\|}\\right)=\\angle(x,y).\\]\nQR factorization.We can express the result of the Gram-Schmidt algorithm described in SS5.4 in a compact form using matrices. Let \\(A\\) be an \\(n\\times k\\) matrix with linearly independent columns \\(a_{1},\\ldots,a_{k}\\). By the independence-dimension inequality, \\(A\\) is tall or square. Let \\(Q\\) be the \\(n\\times k\\) matrix with columns \\(q_{1},\\ldots,q_{k}\\), the orthonormal vectors produced by the Gram-Schmidt algorithm applied to the \\(n\\)-vectors \\(a_{1},\\ldots,a_{k}\\). Orthonormality of \\(q_{1},\\ldots,q_{k}\\) is expressed in matrix form as \\(Q^{T}Q=I\\). We express the equation relating \\(a_{i}\\) and \\(q_{i}\\),\n\\[a_{i}=(q_{1}^{T}a_{i})q_{1}+\\cdots+(q_{i-1}^{T}a_{i})q_{i-1}+\\|\\tilde{q}_{i}\\| q_{i},\\]\n\n\n",
        "answer": 4,
        "rationale": {
            "Question": "Let V be the space spanned by functions cos(2x) and sin(2x). Find the determinant of the linear transformation D(f) = f' from V to V.",
            "Answer": 4,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_2.json",
            "explanation": "NONE",
            "theorem": "basis",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "jianyu_xu/geometric_counting_1.json",
        "question": "How many triangles are there whose sides are all integers and whose maximum side length equals 11?",
        "prediction": 22,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nHow many triangles are there whose sides are all integers and whose maximum side length equals 11?\n\nDocument 1:\n----\nAs an example, consider the 4-vectors\n\\[u=\\left[\\begin{array}{c}1.8\\\\ 2.0\\\\ -3.7\\\\ 4.7\\end{array}\\right],\\qquad v=\\left[\\begin{array}{c}0.6\\\\ 2.1\\\\ 1.9\\\\ -1.4\\end{array}\\right],\\qquad w=\\left[\\begin{array}{c}2.0\\\\ 1.9\\\\ -4.0\\\\ 4.6\\end{array}\\right].\\]\nThe distances between pairs of them are\n\\[\\|u-v\\|=8.368,\\qquad\\|u-w\\|=0.387,\\qquad\\|v-w\\|=8.533,\\]\nso we can say that \\(u\\) is much nearer (or closer) to \\(w\\) than it is to \\(v\\). We can also say that \\(w\\) is much nearer to \\(u\\) than it is to \\(v\\).\nTriangle inequality.We can now explain where the triangle inequality gets its name. Consider a triangle in two or three dimensions, whose vertices have coordinates \\(a\\), \\(b\\), and \\(c\\). The lengths of the sides are the distances between the vertices,\n\\[\\mathbf{dist}(a,b)=\\|a-b\\|,\\qquad\\mathbf{dist}(b,c)=\\|b-c\\|,\\qquad\\mathbf{dist }(a,c)=\\|a-c\\|.\\]\nGeometric intuition tells us that the length of any side of a triangle cannot exceed the sum of the lengths of the other two sides. For example, we have\n\\[\\|a-c\\|\\leq\\|a-b\\|+\\|b-c\\|.\\] (3.3)\nThis follows from the triangle inequality, since\n\\[\\|a-c\\|=\\|(a-b)+(b-c)\\|\\leq\\|a-b\\|+\\|b-c\\|.\\]\nThis is illustrated in figure 3.2.\nFigure 3.2: Triangle inequality.\n#### Examples.\n* _Feature distance._ If \\(x\\) and \\(y\\) represent vectors of \\(n\\) features of two objects, the quantity \\(\\|x-y\\|\\) is called the _feature distance_, and gives a measure of how different the objects are (in terms of their feature values). Suppose for example the feature vectors are associated with patients in a hospital, with entries such as weight, age, presence of chest pain, difficulty breathing, and the results of tests. We can use feature vector distance to say that one patient case is near another one (at least in terms of their feature vectors).\n\nDocument 2:\n----\nWhen the distance between two \\(n\\)-vectors \\(x\\) and \\(y\\) is small, we say they are 'close' or 'nearby', and when the distance \\(\\|x-y\\|\\) is large, we say they are 'far'. The particular numerical values of \\(\\|x-y\\|\\) that correspond to 'close' or 'far' depend on\nFigure 3.1: The norm of the displacement \\(b-a\\) is the distance between the points with coordinates \\(a\\) and \\(b\\).\nAs an example, consider the 4-vectors\n\\[u=\\left[\\begin{array}{c}1.8\\\\ 2.0\\\\ -3.7\\\\ 4.7\\end{array}\\right],\\qquad v=\\left[\\begin{array}{c}0.6\\\\ 2.1\\\\ 1.9\\\\ -1.4\\end{array}\\right],\\qquad w=\\left[\\begin{array}{c}2.0\\\\ 1.9\\\\ -4.0\\\\ 4.6\\end{array}\\right].\\]\nThe distances between pairs of them are\n\\[\\|u-v\\|=8.368,\\qquad\\|u-w\\|=0.387,\\qquad\\|v-w\\|=8.533,\\]\nso we can say that \\(u\\) is much nearer (or closer) to \\(w\\) than it is to \\(v\\). We can also say that \\(w\\) is much nearer to \\(u\\) than it is to \\(v\\).\nTriangle inequality.We can now explain where the triangle inequality gets its name. Consider a triangle in two or three dimensions, whose vertices have coordinates \\(a\\), \\(b\\), and \\(c\\). The lengths of the sides are the distances between the vertices,\n\\[\\mathbf{dist}(a,b)=\\|a-b\\|,\\qquad\\mathbf{dist}(b,c)=\\|b-c\\|,\\qquad\\mathbf{dist }(a,c)=\\|a-c\\|.\\]\nGeometric intuition tells us that the length of any side of a triangle cannot exceed the sum of the lengths of the other two sides. For example, we have\n\\[\\|a-c\\|\\leq\\|a-b\\|+\\|b-c\\|.\\] (3.3)\nThis follows from the triangle inequality, since\n\\[\\|a-c\\|=\\|(a-b)+(b-c)\\|\\leq\\|a-b\\|+\\|b-c\\|.\\]\nThis is illustrated in figure 3.2.\n\n\n",
        "answer": 36,
        "rationale": {
            "Question": "How many triangles are there whose sides are all integers and whose maximum side length equals 11?",
            "Answer": 36,
            "Answer_type": "integer",
            "Picture": null,
            "source": "self",
            "id": "jianyu_xu/geometric_counting_1.json",
            "explanation": "NONE",
            "theorem": "integer programming",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "mingyin/gaussian-elimination3.json",
        "question": "Suppose V is a finite-dimensional vector space on F. $M1={a_1,a_2,a_3}$ is a basis of V, $M2={b_1,b_2,b_3}$ is another basis of V. Suppose the coordinates of b_1,b_2,b_3 under M1 are $c_1=(1,1,-1),c_2=(1,-1,1),c_3=(-1,1,1)$. Suppose the coordinate of $d\\in V$ under M1 is (1,3,5). What is the coordinate of d under M2? Return the three coordinate values as a list.",
        "prediction": [
            4,
            2,
            2
        ],
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nSuppose V is a finite-dimensional vector space on F. $M1={a_1,a_2,a_3}$ is a basis of V, $M2={b_1,b_2,b_3}$ is another basis of V. Suppose the coordinates of b_1,b_2,b_3 under M1 are $c_1=(1,1,-1),c_2=(1,-1,1),c_3=(-1,1,1)$. Suppose the coordinate of $d\\in V$ under M1 is (1,3,5). What is the coordinate of d under M2? Return the three coordinate values as a list.\n\nDocument 1:\n----\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\\[l_{i}=\\left[\\begin{array}{c}0_{i-1}\\\\ 1\\\\ -(1+r)\\\\ 0_{n-i-1}\\end{array}\\right],\\quad i=1,\\ldots,n-1,\\]\nwhere \\(r\\geq 0\\) is the per-period interest rate. The cash flow \\(l_{i}\\) represents a loan of $1 in period \\(i\\), which is paid back in period \\(i+1\\) with interest \\(r\\). (The subscripts on the zero vectors above give their dimensions.) Scaling \\(l_{i}\\) changes the loan amount; scaling \\(l_{i}\\) by a negative coefficient converts it into a loan _to_ another entity (which is paid back in period \\(i+1\\) with interest).\n\nDocument 2:\n----\nthen (evidently) \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); conversely, if \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\), the identity (5.4) holds for \\(y\\).\nOrthonormal basis.If the \\(n\\)-vectors \\(a_{1},\\ldots,a_{n}\\) are orthonormal, they are linearly independent, and therefore also a basis. In this case they are called an _orthonormal basis_. The three examples above (on page 5.2) are orthonormal bases.\nIf \\(a_{1},\\ldots,a_{n}\\) is an orthonormal basis, then we have, for any \\(n\\)-vector \\(x\\), the identity\n\\[x=(a_{1}^{T}x)a_{1}+\\cdots+(a_{n}^{T}x)a_{n}.\\] (5.5)\nTo see this, we note that since \\(a_{1},\\ldots,a_{n}\\) are a basis, \\(x\\) can be expressed as a linear combination of them; hence the identity (5.4) above holds. The equation above is sometimes called the _orthonormal expansion formula_; the right-hand side is called the _expansion of \\(x\\) in the basis \\(a_{1},\\ldots,a_{n}\\)_. It shows that any \\(n\\)-vector can be expressed as a linear combination of the basis elements, with the coefficients given by taking the inner product of \\(x\\) with the elements of the basis.\nAs an example, we express the 3-vector \\(x=(1,2,3)\\) as a linear combination of the orthonormal basis given in (5.3). The inner products of \\(x\\) with these vectors\nare\n\\[\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right]^{T}x=-3,\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ 1\\\\ 0\\end{array}\\right]^{T}x=\\frac{3}{\\sqrt{2}},\\qquad\\frac{1}{\\sqrt{2}}\\left[ \\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right]^{T}x=\\frac{-1}{\\sqrt{2}}.\\]\nIt can be verified that the expansion of \\(x\\) in this basis is\n\\[x=(-3)\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right]+\\frac{3}{\\sqrt{2}}\\left(\\frac{1}{\\sqrt{2}}\\left[\\begin{array} []{c}1\\\\ 1\\\\ 0\\end{array}\\right]\\right)+\\frac{-1}{\\sqrt{2}}\\left(\\frac{1}{\\sqrt{2}}\\left[ \\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right]\\right).\\]\n\n\n",
        "answer": [
            2,
            3,
            4
        ],
        "rationale": {
            "Question": "Suppose V is a finite-dimensional vector space on F. $M1={a_1,a_2,a_3}$ is a basis of V, $M2={b_1,b_2,b_3}$ is another basis of V. Suppose the coordinates of b_1,b_2,b_3 under M1 are $c_1=(1,1,-1),c_2=(1,-1,1),c_3=(-1,1,1)$. Suppose the coordinate of $d\\in V$ under M1 is (1,3,5). What is the coordinate of d under M2? Return the three coordinate values as a list.",
            "Answer": [
                2,
                3,
                4
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "linear algebra 2.6 example 2",
            "id": "mingyin/gaussian-elimination3.json",
            "explanation": "NONE",
            "theorem": "gaussian elimination",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "xinyi/linear_projection.json",
        "question": "Which of the following matrices takes any vector $v$ and projects it onto the space spanned by the columns of $\\Phi$? (a) $(\\Phi^T\\Phi)^{-1}$. (b) $\\Phi(\\Phi^T\\Phi)^{-1}$. (c) $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$. (d) $\\Phi^T(\\Phi^T\\Phi)^{-1}\\Phi^T$.",
        "prediction": "(c)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhich of the following matrices takes any vector $v$ and projects it onto the space spanned by the columns of $\\Phi$? (a) $(\\Phi^T\\Phi)^{-1}$. (b) $\\Phi(\\Phi^T\\Phi)^{-1}$. (c) $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$. (d) $\\Phi^T(\\Phi^T\\Phi)^{-1}\\Phi^T$.\n\nDocument 1:\n----\nare _compatible_.) Then \\(C=AB\\) can be expressed as the \\(m\\times n\\) block matrix with entries \\(C_{ij}\\), given by the formula (10.1). For example, we have\n\\[\\left[\\begin{array}{cc}A&B\\\\ C&D\\end{array}\\right]\\left[\\begin{array}{cc}E&F\\\\ G&H\\end{array}\\right]=\\left[\\begin{array}{cc}AE+BG&AF+BH\\\\ CE+DG&CF+DH\\end{array}\\right],\\]\nfor any matrices \\(A,B,\\ldots,H\\) for which the matrix products above make sense. This formula is the same as the formula for multiplying two \\(2\\times 2\\) matrices (_i.e._, with scalar entries); but when the entries of the matrix are themselves matrices (as in the block matrix above), we must be careful to preserve the multiplication order.\nColumn interpretation of matrix-matrix product.We can derive some additional insight into matrix multiplication by interpreting the operation in terms of the columns of the second matrix. Consider the matrix product of an \\(m\\times p\\) matrix \\(A\\) and a \\(p\\times n\\) matrix \\(B\\), and denote the columns of \\(B\\) by \\(b_{k}\\). Using block-matrix notation, we can write the product \\(AB\\) as\n\\[AB=A\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\,= \\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right].\\]\nThus, the columns of \\(AB\\) are the matrix-vector products of \\(A\\) and the columns of \\(B\\). The product \\(AB\\) can be interpreted as the matrix obtained by 'applying' \\(A\\) to each of the columns of \\(B\\).\nMultiple sets of linear equations.We can use the column interpretation of matrix multiplication to express a set of \\(k\\) linear equations with the same \\(m\\times n\\) coefficient matrix \\(A\\),\n\\[Ax_{i}=b_{i},\\quad i=1,\\ldots,k,\\]\nin the compact form\n\\[AX=B,\\]\n\nDocument 2:\n----\n\\[A_{p:q,r:s}=\\left[\\begin{array}{cccc}A_{pr}&A_{p,r+1}&\\cdots&A_{ps}\\\\ A_{p+1,r}&A_{p+1,r+1}&\\cdots&A_{p+1,s}\\\\ \\vdots&\\vdots&&\\vdots\\\\ A_{qr}&A_{q,r+1}&\\cdots&A_{qs}\\end{array}\\right].\\]\nThis submatrix has size \\((q-p+1)\\times(s-r+1)\\) and is obtained by extracting from \\(A\\) the elements in rows \\(p\\) through \\(q\\) and columns \\(r\\) through \\(s\\).\nFor the specific matrix \\(A\\) in (6.1), we have\n\\[A_{2:3,3:4}=\\left[\\begin{array}{cc}1&4\\\\ 5&4\\end{array}\\right].\\]\nColumn and row representation of a matrix.Using block matrix notation we can write an \\(m\\times n\\) matrix \\(A\\) as a block matrix with one block row and \\(n\\) block columns,\n\\[A=\\left[\\begin{array}{cccc}a_{1}&a_{2}&\\cdots&a_{n}\\end{array}\\right],\\]\nwhere \\(a_{j}\\), which is an \\(m\\)-vector, is the \\(j\\)th column of \\(A\\). Thus, an \\(m\\times n\\) matrix can be viewed as its \\(n\\) columns, concatenated.\nSimilarly, an \\(m\\times n\\) matrix \\(A\\) can be written as a block matrix with one block column and \\(m\\) block rows:\n\\[A=\\left[\\begin{array}{c}b_{1}\\\\ b_{2}\\\\ \\vdots\\\\ b_{m}\\end{array}\\right],\\]\nwhere \\(b_{i}\\), which is a row \\(n\\)-vector, is the \\(i\\)th row of \\(A\\). In this notation, the matrix \\(A\\) is interpreted as its \\(m\\) rows, stacked.\n## Examples\nTable interpretation.The most direct interpretation of a matrix is as a table of numbers that depend on two indices, \\(i\\) and \\(j\\). (A vector is a list of numbers that depend on only one index.) In this case the rows and columns of the matrix usually have some simple interpretation. Some examples are given below.\n\n\n",
        "answer": "(c)",
        "rationale": {
            "Question": "Which of the following matrices takes any vector $v$ and projects it onto the space spanned by the columns of $\\Phi$? (a) $(\\Phi^T\\Phi)^{-1}$. (b) $\\Phi(\\Phi^T\\Phi)^{-1}$. (c) $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$. (d) $\\Phi^T(\\Phi^T\\Phi)^{-1}\\Phi^T$.",
            "Answer": "(c)",
            "Answer_type": "option",
            "Picture": null,
            "source": "textbook 3.2",
            "id": "xinyi/linear_projection.json",
            "explanation": "NONE",
            "theorem": "projection theory",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "option",
        "correct": true
    }
]