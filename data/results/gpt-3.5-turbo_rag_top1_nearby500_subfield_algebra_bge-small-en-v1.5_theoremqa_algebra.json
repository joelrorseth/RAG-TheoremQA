[
    {
        "id": "elainewan/math_algebra_1.json",
        "question": "Find the x value of the solutions to the linear system: 7x - y = 15x, -6x + 8y = 15y.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFind the x value of the solutions to the linear system: 7x - y = 15x, -6x + 8y = 15y.\n\nDocument 1:\n----\n\\[Ax=b.\\] (8.5)\nIn this context, the \\(m\\times n\\) matrix \\(A\\) is called the _coefficient matrix_, and the \\(m\\)-vector \\(b\\) is called the _right-hand side_. An \\(n\\)-vector \\(x\\) is called a _solution_ of the linear equations if \\(Ax=b\\) holds. A set of linear equations can have no solutions, one solution, or multiple solutions.\nExamples.\n* The set of linear equations \\[x_{1}+x_{2}=1,\\quad x_{1}=-1,\\quad x_{1}-x_{2}=0\\] is written as \\(Ax=b\\) with \\[A=\\left[\\begin{array}{cc}1&1\\\\ 1&0\\\\ 1&-1\\end{array}\\right],\\qquad b=\\left[\\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right].\\] It has no solutions.\n* The set of linear equations \\[x_{1}+x_{2}=1,\\quad x_{2}+x_{3}=2\\] is written as \\(Ax=b\\) with \\[A=\\left[\\begin{array}{cc}1&1&0\\\\ 0&1&1\\end{array}\\right],\\qquad b=\\left[\\begin{array}{c}1\\\\ 2\\end{array}\\right].\\] It has multiple solutions, including \\(x=(1,0,2)\\) and \\(x=(0,1,1)\\).\nOver-determined and under-determined systems of linear equations.The set of linear equations is called _over-determined_ if \\(m>n\\), _under-determined_ if \\(m<n\\), and _square_ if \\(m=n\\); these correspond to the coefficient matrix being tall, wide, and square, respectively. When the system of linear equations is over-determined, there are more equations than variables or unknowns. When the system of linear equations is under-determined, there are more unknowns than equations. When the system of linear equations is square, the numbers of unknowns and equations is the same. A set of equations with zero right-hand side, \\(Ax=0\\), is called a _homogeneous_ set of equations. Any homogeneous set of equations has \\(x=0\\) as a solution.\nIn chapter 11 we will address the question of how to determine if a system of linear equations has a solution, and how to find one when it does. For now, we give a few interesting examples.\n#### Examples\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\nBalancing chemical reactions.A chemical reaction involves \\(p\\) reactants (molecules) and \\(q\\) products, and can be written as\n\\[a_{1}R_{1}+\\cdots+a_{p}R_{p}\\longrightarrow b_{1}P_{1}+\\cdots+b_{q}P_{q}.\\]\nHere \\(R_{1},\\ldots,R_{p}\\) are the reactants, \\(P_{1},\\ldots,P_{q}\\) are the products, and the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\) are positive numbers that tell us how many of each of these molecules is involved in the reaction. They are typically integers, but can be scaled arbitrarily; we could double all of these numbers, for example, and we still have the same reaction. As a simple example, we have the electrolysis of water,\n\\[2\\mathrm{H}_{2}\\mathrm{O}\\longrightarrow 2\\mathrm{H}_{2}+\\mathrm{O}_{2},\\]\nwhich has one reactant, water (\\(\\mathrm{H}_{2}\\mathrm{O}\\)), and two products, molecular hydrogen (\\(\\mathrm{H}_{2}\\)) and molecular oxygen (\\(\\mathrm{O}_{2}\\)). The coefficients tell us that 2 water molecules create 2 hydrogen molecules and 1 oxygen molecule. The coefficients in a reaction can be multiplied by any nonzero numbers; for example, we could write the reaction above as \\(3\\mathrm{H}_{2}\\mathrm{O}\\longrightarrow 3\\mathrm{H}_{2}+(3/2)\\mathrm{O}_{2}\\). By convention reactions are written with all coefficients integers, with least common divisor one.\n\n\n",
        "answer": 0,
        "rationale": {
            "Question": "Find the x value of the solutions to the linear system: 7x - y = 15x, -6x + 8y = 15y.",
            "Answer": 0,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_1.json",
            "explanation": "solutions/math_algebra_1.txt",
            "theorem": "linear systems",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_3.json",
        "question": "Is W = {[x, y] in R^2: x >= 0 and y >= 0} a subspace of R^2?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs W = {[x, y] in R^2: x >= 0 and y >= 0} a subspace of R^2?\n\nDocument 1:\n----\n\\[J_{\\text{meas}}=\\sum_{t\\in\\mathcal{T}}\\|Cx_{t}-y_{t}\\|^{2},\\]\n_i.e._, we only sum the measurement errors over the measurements we have. For each value of \\(\\lambda\\) we compute the RMS train and test errors\n\\[E_{\\text{train}}=\\frac{1}{\\sqrt{80p}}\\left(\\sum_{t\\in\\mathcal{T}}\\|C\\hat{x}_{t }-y_{t}\\|^{2}\\right)^{1/2},\\qquad E_{\\text{test}}=\\frac{1}{\\sqrt{20p}}\\left( \\sum_{t\\notin\\mathcal{T}}\\|C\\hat{x}_{t}-y_{t}\\|^{2}\\right)^{1/2}.\\]\nThe training error (squared and scaled) appears directly in our minimization problem. The test error, however, is a good test of our estimation method, since it compares predictions of positions (in this example) with measurements of position that were not used to form the estimates. The errors are shown in figure 17.9, as functions of the parameter \\(\\lambda\\). We can clearly see that for \\(\\lambda<100\\) or so, we are over-fit, since the test RMS error substantially exceeds the train RMS error. We can also see that \\(\\lambda\\) around \\(10^{3}\\) is a good choice.\nFigure 17.9: Training and test errors for the state estimation example.\n## 17 Constrained least squares applications\n### 17.1 A variation on the portfolio optimization formulation\nConsider the following variation on the linearly constrained least squares problem (17.2):\n\\[\\begin{array}{ll}\\mbox{minimize}&\\|Rw\\|^{2}\\\\ \\mbox{subject to}&\\left[\\begin{array}{c}\\mathbf{1}^{T}\\\\ \\mu^{T}\\end{array}\\right]w=\\left[\\begin{array}{c}1\\\\ \\rho\\end{array}\\right],\\end{array}\\] (17.12)\nwith variable \\(w\\). (The difference is that here we drop the term \\(\\rho\\mathbf{1}\\) that appears inside the norm square objective in (17.2).) Show that this problem is equivalent to (17.2). This means \\(w\\) is a solution of (17.12) if and only if it is a solution of (17.2).\n_Hint._ You can argue directly by expanding the objective in (17.2) or via the KKT systems of the two problems.\n### 17.2 A more conventional formulation of the portfolio optimization problem\nIn this problem we derive an equivalent formulation of the portfolio optimization problem (17.2) that appears more frequently in the literature than our version. (Equivalent means that the two problems always have the same solution.) This formulation is based on the _return covariance matrix_, which we define below. (See also exercise 10.16.)\nThe means of the columns of the asset return matrix \\(R\\) are the entries of the vector \\(\\mu\\). The _de-meaned returns matrix_ is given by \\(\\tilde{R}=R-\\mathbf{1}\\mu^{T}\\). (The columns of the matrix \\(\\tilde{R}=R-\\mathbf{1}\\mu^{T}\\) are the de-meaned return time series for the assets.) The return covariance matrix, traditionally denoted \\(\\Sigma\\), is its Gram matrix \\(\\Sigma=(1/T)\\tilde{R}^{T}\\tilde{R}\\).\n1. Show that \\(\\sigma_{i}=\\sqrt{\\Sigma_{ii}}\\) is the standard deviation (risk) of asset \\(i\\) return. (The symbol \\(\\sigma_{i}\\) is a traditional one for the standard deviation of asset \\(i\\).)\n2. Show that the correlation coefficient between asset \\(i\\) and asset \\(j\\) returns is given by \\(\\rho_{ij}=\\Sigma_{ij}/(\\sigma_{i}\\sigma_{j})\\). (Assuming neither asset has constant return; if either one does, they are uncorrelated.)\n3. _Portfolio optimization using the return covariance matrix._ Show that the following problem is equivalent to our portfolio optimization problem (17.2): \\[\\begin{array}{ll}\\mbox{minimize}&w^{T}\\Sigma w\\\\ \\mbox{subject to}&\\left[\\begin{array}{c}\\mathbf{1}^{T}\\\\ \\mu^{T}\\end{array}\\right]w=\\left[\\begin{array}{c}1\\\\ \\rho\\end{array}\\right],\\end{array}\\] (17.13) with variable \\(w\\). This is the form of the portfolio optimization problem that you will find in the literature. _Hint._ Show that the objective is the same as \\(\\|\\tilde{R}w\\|^{2}\\), and that this is the same as \\(\\|Rw-\\rho\\mathbf{1}\\|^{2}\\) for any feasible \\(w\\).\n### 17.3 A simple portfolio optimization problem\n1. Find an analytical solution for the portfolio optimization problem with \\(n=2\\) assets. You can assume that \\(\\mu_{1}\\neq\\mu_{2}\\), _i.e._, the two assets have different mean returns. _Hint._ The optimal weights depend only on \\(\\mu\\) and \\(\\rho\\), and not (directly) on the return matrix \\(R\\).\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is W = {[x, y] in R^2: x >= 0 and y >= 0} a subspace of R^2?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3.json",
            "explanation": "solutions/math_algebra_3.png",
            "theorem": "linear subspaces",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_3_6.json",
        "question": "Consider the basis B of R^2 consisting of vectors v_1 = [3,1] and v_2 = [-1, 3]. If x = [10, 10], find the B-coordinate vector of x",
        "prediction": [
            4,
            2
        ],
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider the basis B of R^2 consisting of vectors v_1 = [3,1] and v_2 = [-1, 3]. If x = [10, 10], find the B-coordinate vector of x\n\nDocument 1:\n----\nwhich, since \\(a_{1},\\ldots,a_{n}\\) are linearly independent, implies that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\). But then all the \\(\\beta_{i}\\) are zero, a contradiction. So we conclude that \\(\\beta_{n+1}\\neq 0\\). It follows that\n\\[b=(-\\beta_{1}/\\beta_{n+1})a_{1}+\\cdots+(-\\beta_{n}/\\beta_{n+1})a_{n},\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\).\nCombining this result with the observation above that any linear combination of linearly independent vectors can be expressed in only one way, we conclude:\n_Any \\(n\\)-vector \\(b\\) can be written in a unique way as a linear combination of a basis \\(a_{1},\\ldots,a_{n}\\)._\nExpansion in a basis.When we express an \\(n\\)-vector \\(b\\) as a linear combination of a basis \\(a_{1},\\ldots,a_{n}\\), we refer to\n\\[b=\\alpha_{1}a_{1}+\\cdots+\\alpha_{n}a_{n},\\]\nas the _expansion of \\(b\\) in the \\(a_{1},\\ldots,a_{n}\\) basis_. The numbers \\(\\alpha_{1},\\ldots,\\alpha_{n}\\) are called the _coefficients_ of the expansion of \\(b\\) in the basis \\(a_{1},\\ldots,a_{n}\\). (We will see later how to find the coefficients in the expansion of a vector in a basis.)\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\\[l_{i}=\\left[\\begin{array}{c}0_{i-1}\\\\ 1\\\\ -(1+r)\\\\ 0_{n-i-1}\\end{array}\\right],\\quad i=1,\\ldots,n-1,\\]\nwhere \\(r\\geq 0\\) is the per-period interest rate. The cash flow \\(l_{i}\\) represents a loan of $1 in period \\(i\\), which is paid back in period \\(i+1\\) with interest \\(r\\). (The subscripts on the zero vectors above give their dimensions.) Scaling \\(l_{i}\\) changes the loan amount; scaling \\(l_{i}\\) by a negative coefficient converts it into a loan _to_ another entity (which is paid back in period \\(i+1\\) with interest).\nThe vectors \\(e_{1},l_{1},\\ldots,l_{n-1}\\) are a basis. (The first vector \\(e_{1}\\) represents income of $1 in period 1.) To see this, we show that they are linearly independent. Suppose that\n\\[\\beta_{1}e_{1}+\\beta_{2}l_{1}+\\cdots+\\beta_{n}l_{n-1}=0.\\]\nWe can express this as\n\\[\\left[\\begin{array}{c}\\beta_{1}+\\beta_{2}\\\\ \\beta_{3}-(1+r)\\beta_{2}\\\\ \\vdots\\\\ \\beta_{n}-(1+r)\\beta_{n-1}\\\\ -(1+r)\\beta_{n}\\end{array}\\right]=0.\\]\nThe last entry is \\(-(1+r)\\beta_{n}=0\\), which implies that \\(\\beta_{n}=0\\) (since \\(1+r>0\\)). Using \\(\\beta_{n}=0\\), the second to last entry becomes \\(-(1+r)\\beta_{n-1}=0\\), so we conclude that \\(\\beta_{n-1}=0\\). Continuing this way we find that \\(\\beta_{n-2},\\ldots,\\beta_{2}\\) are all zero. The\n## Chapter 5 Linear independence\n### 5.1 Linear independence\nLinear independence is a generalization of the theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of\n\n\n",
        "answer": [
            4,
            2
        ],
        "rationale": {
            "Question": "Consider the basis B of R^2 consisting of vectors v_1 = [3,1] and v_2 = [-1, 3]. If x = [10, 10], find the B-coordinate vector of x",
            "Answer": [
                4,
                2
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_6.json",
            "explanation": "solutions/math_algebra_3_6.txt",
            "theorem": "basis",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_4_2.json",
        "question": "Let V be the space of all infinite sequences of real numbers. Consider the transformation T(x_0, x_1, x_2, ...) = (x_1, x_2, x_3, ...) from V to V. Is the sequence (1,2,3,...) in the image of T?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet V be the space of all infinite sequences of real numbers. Consider the transformation T(x_0, x_1, x_2, ...) = (x_1, x_2, x_3, ...) from V to V. Is the sequence (1,2,3,...) in the image of T?\n\nDocument 1:\n----\nbe the averages of these two lists of vectors. (Here \\(\\overline{x}\\) is an \\(n\\)-vector and \\(\\overline{y}\\) is an \\(m\\)-vector.) Show that we always have \\(\\overline{y}=G(\\overline{x})\\). In words: The average of an affine function applied to a list of vectors is the same as the affine function applied to the average of the list of vectors.\n### Cross-product\nThe cross product of two 3-vectors \\(a=(a_{1},a_{2},a_{3})\\) and \\(x=(x_{1},x_{2},x_{3})\\) is defined as the vector\n\\[a\\times x=\\left[\\begin{array}{c}a_{2}x_{3}-a_{3}x_{2}\\\\ a_{3}x_{1}-a_{1}x_{3}\\\\ a_{1}x_{2}-a_{2}x_{1}\\end{array}\\right].\\]\nThe cross product comes up in physics, for example in electricity and magnetism, and in dynamics of mechanical systems like robots or satellites. (You do not need to know this for this exercise.)\nAssume \\(a\\) is fixed. Show that the function \\(f(x)=a\\times x\\) is a linear function of \\(x\\), by giving a matrix \\(A\\) that satisfies \\(f(x)=Ax\\) for all \\(x\\).\n### Linear functions of images\nIn this problem we consider several linear functions of a monochrome image with \\(N\\times N\\) pixels. To keep the matrices small enough to work out by hand, we will consider the case with \\(N=3\\) (which would hardly qualify as an image). We represent a \\(3\\times 3\\) image as a 9-vector using the ordering of pixels shown below.\n\\begin{tabular}{|c|c|c|} \\hline\n1 & 4 & 7 \\\\ \\hline\n2 & 5 & 8 \\\\ \\hline\n3 & 6 & 9 \\\\ \\hline \\end{tabular} (This ordering is called _column-major_.) Each of the operations or transformations below defines a function \\(y=f(x)\\), where the 9-vector \\(x\\) represents the original image, and the 9-vector \\(y\\) represents the resulting or transformed image. For each of these operations, give the \\(9\\times 9\\) matrix \\(A\\) for which \\(y=Ax\\).\n* Turn the original image \\(x\\) upside-down.\n* Rotate the original image \\(x\\) clockwise \\(90^{\\circ}\\).\n* Translate the image up by 1 pixel and to the right by 1 pixel. In the translated image, assign the value \\(y_{i}=0\\) to the pixels in the first column and the last row.\n* Set each pixel value \\(y_{i}\\) to be the average of the neighbors of pixel \\(i\\) in the original image. By neighbors, we mean the pixels immediately above and below, and immediately to the left and right. The center pixel has 4 neighbors; corner pixels have 2 neighbors, and the remaining pixels have 3 neighbors.\n**8.5**: _Symmetric and anti-symmetric part._ An \\(n\\)-vector \\(x\\) is _symmetric_ if \\(x_{k}=x_{n-k+1}\\) for \\(k=1,\\ldots,n\\). It is _anti-symmetric_ if \\(x_{k}=-x_{n-k+1}\\) for \\(k=1,\\ldots,n\\).\n1. Show that every vector \\(x\\) can be decomposed in a unique way as a sum \\(x=x_{\\rm s}+x_{\\rm a}\\) of a symmetric vector \\(x_{\\rm s}\\) and an anti-symmetric vector \\(x_{\\rm a}\\).\n2. Show that the symmetric and anti-symmetric parts \\(x_{\\rm s}\\) and \\(x_{\\rm a}\\) are linear functions of \\(x\\). Give matrices \\(A_{\\rm s}\\) and \\(A_{\\rm a}\\) such that \\(x_{\\rm s}=A_{\\rm s}x\\) and \\(x_{\\rm a}=A_{\\rm a}x\\) for all \\(x\\).\n**8.6**: _Linear functions._ For each description of \\(y\\) below, express it as \\(y=Ax\\) for some \\(A\\). (You should specify \\(A\\).)\n1. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average of \\(x_{1},\\ldots,x_{i-1}\\). (We take \\(y_{1}=x_{1}\\).)\n2. \\(y_{i}\\) is the difference between \\(x_{i}\\) and the average value of all other \\(x_{j}\\)s, _i.e._, the average of \\(x_{1},\\ldots,x_{i-1},x_{i+1},\\ldots,x_{n}\\).\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Let V be the space of all infinite sequences of real numbers. Consider the transformation T(x_0, x_1, x_2, ...) = (x_1, x_2, x_3, ...) from V to V. Is the sequence (1,2,3,...) in the image of T?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_4_2.json",
            "explanation": "solutions/math_algebra_4_2.png",
            "theorem": "image of linear transformations",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "mingyin/linear-dependence3.json",
        "question": "suppose the 10-by-10 matrix A has the form: if i \\neq j, A_{i,j}=a_i*b_j; if i=j,  A_{i,j}=1+a_i*b_j for all 1<=i,j<=10. Here a_i = 1/i, b_i=1/(i+1). Find the determinant of A. return the numeric.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nsuppose the 10-by-10 matrix A has the form: if i \\neq j, A_{i,j}=a_i*b_j; if i=j,  A_{i,j}=1+a_i*b_j for all 1<=i,j<=10. Here a_i = 1/i, b_i=1/(i+1). Find the determinant of A. return the numeric.\n\nDocument 1:\n----\n### 15.5 Complexity\nIn the general case we can minimize the weighted sum objective (15.1) by creating the stacked matrix and vector \\(\\tilde{A}\\) and \\(\\tilde{b}\\) in (15.2), and then using the QR factorization to solve the resulting least squares problem. The complexity of this method is order \\(mn^{2}\\) flops, where \\(m=m_{1}+\\cdots+m_{k}\\) is the sum of heights of the matrices \\(A_{1},\\ldots,A_{k}\\).\nWhen using multi-objective least squares, it is common to minimize the weighted sum objective for some, or even many, different choices of weights. Assuming that the weighted sum objective is minimized for \\(L\\) different values of the weights, the total complexity is order \\(Lmn^{2}\\) flops.\n### 15.5 Complexity\nFigure 15.12: RMS training and test errors as a function of the regularization parameter \\(\\lambda\\). _Bottom_. The regularization path. The dashed horizontal lines show the values of the coefficients used to generate the data.\n#### Gram caching\nWe start from the formula (15.3) for the minimizer of the weighted sum objective,\n\\[\\hat{x}=(\\lambda_{1}A_{1}^{T}A_{1}+\\cdots+\\lambda_{k}A_{k}^{T}A_{k})^{-1}( \\lambda_{1}A_{1}^{T}b_{1}+\\cdots+\\lambda_{k}A_{k}^{T}b_{k}).\\]\nThe matrix appearing in the inverse is a weighted sum of the Gram matrices \\(G_{i}=A_{i}^{T}A_{i}\\) associated with the matrices \\(A_{i}\\). We can compute \\(\\hat{x}\\) by forming these Gram matrices \\(G_{i}\\), along with the vectors \\(h_{i}=A_{i}^{T}b_{i}\\), then forming the weighted sums\n\\[G=\\lambda_{1}G_{1}+\\cdots+\\lambda_{k}G_{k},\\qquad h=\\lambda_{1}h_{1}+\\cdots+ \\lambda_{k}h_{k},\\]\nand finally, solving the \\(n\\times n\\) set of equations \\(G\\hat{x}=h\\). Forming \\(G_{i}\\) and \\(h_{i}\\) costs \\(m_{i}n^{2}\\) and \\(2m_{i}n\\) flops, respectively. (We save a factor of two in forming the Gram matrix; see page 182.) Ignoring the second term and adding over \\(i=1,\\ldots,k\\) we get a total of \\(mn^{2}\\) flops. Forming the weighted sums \\(G\\) and \\(h\\) costs \\(2kn^{2}\\) flops. Solving \\(G\\hat{x}=h\\) costs order \\(2n^{3}\\) flops.\n_Gram caching_ is the simple trick of computing \\(G_{i}\\) (and \\(h_{i}\\)) just once, and re-using these matrices and vectors for the \\(L\\) different choices of weights. This leads to a complexity of\n\\[mn^{2}+L(k+2n)n^{2}\\]\nflops. When \\(m\\) is much larger than \\(k+n\\), which is a common occurrence, this cost is smaller than \\(Lmn^{2}\\), the cost for the simple method.\nAs a simple example consider Tikhonov regularization. We will compute\n\\[\\hat{x}^{(i)}=(A^{T}A+\\lambda^{(i)}I)^{-1}A^{T}b\\]\nfor \\(i=1,\\ldots,L\\), where \\(A\\) is \\(m\\times n\\). The cost of the simple method is \\(2Lmn^{2}\\) flops; using Gram caching the cost is \\(mn^{2}+2Ln^{3}=(m+2Ln)n^{2}\\) flops. (We drop the term \\(Lkn^{2}\\), since \\(k=2\\) here.) With \\(m=100n\\) and \\(L=100\\), Gram caching reduces the computational cost by more than a factor of 50. This means that the entire regularization path (_i.e._, the solution for 100 values of \\(\\lambda\\)) can be computed in not much more time than it takes to compute the solution for one value of \\(\\lambda\\).\n#### The kernel trick\nIn this section we focus on another special case, which arises in many applications:\n\\[J=\\|Ax-b\\|^{2}+\\lambda\\|x-x^{\\rm des}\\|^{2},\\] (15.9)\nwhere the \\(m\\times n\\) matrix \\(A\\) is wide, _i.e._, \\(m<n\\), and \\(\\lambda>0\\). (Here we drop the subscripts on \\(A\\), \\(b\\), and \\(m\\) since we have only one matrix in this problem.) The associated \\((m+n)\\times n\\) stacked matrix (see (15.2))\n\\[\\tilde{A}=\\left[\\begin{array}{c}A\\\\ \\sqrt{\\lambda}I\\end{array}\\right]\\]\nalways has linearly independent columns. Using the QR factorization to solve the stacked least squares problem requires \\(2(m+n)n^{2}\\) flops, which grows like \\(n^{3}\\). We\n\n\n",
        "answer": 1.9,
        "rationale": {
            "Question": "suppose the 10-by-10 matrix A has the form: if i \\neq j, A_{i,j}=a_i*b_j; if i=j,  A_{i,j}=1+a_i*b_j for all 1<=i,j<=10. Here a_i = 1/i, b_i=1/(i+1). Find the determinant of A. return the numeric.",
            "Answer": 1.9,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 4.5 example 4",
            "id": "mingyin/linear-dependence3.json",
            "explanation": "NONE",
            "theorem": "linear dependence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_7_3.json",
        "question": "Does the following transformation have an eigenvector: Counterclockwise rotation through an angle of 45 degrees followed by a scaling by 2 in R^2.",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nDoes the following transformation have an eigenvector: Counterclockwise rotation through an angle of 45 degrees followed by a scaling by 2 in R^2.\n\nDocument 1:\n----\n### 7.1 Geometric transformations\nSuppose the 2-vector (or 3-vector) \\(x\\) represents a position in 2-D (or 3-D) space. Several important geometric transformations or mappings from points to points can be expressed as matrix-vector products \\(y=Ax\\), with \\(A\\) a \\(2\\times 2\\) (or \\(3\\times 3\\)) matrix. In the examples below, we consider the mapping from \\(x\\) to \\(y\\), and focus on the 2-D case (for which some of the matrices are simpler to describe).\nScaling.Scaling is the mapping \\(y=ax\\), where \\(a\\) is a scalar. This can be expressed as \\(y=Ax\\) with \\(A=aI\\). This mapping stretches a vector by the factor \\(|a|\\) (or shrinks it when \\(|a|<1\\)), and it flips the vector (reverses its direction) if \\(a<0\\).\nDilation.Dilation is the mapping \\(y=Dx\\), where \\(D\\) is a diagonal matrix, \\(D=\\mathbf{diag}(d_{1},d_{2})\\). This mapping stretches the vector \\(x\\) by different factors along the two different axes. (Or shrinks, if \\(|d_{i}|<1\\), and flips, if \\(d_{i}<0\\).)\nRotation.Suppose that \\(y\\) is the vector obtained by rotating \\(x\\) by \\(\\theta\\) radians counterclockwise. Then we have\n\\[y=\\left[\\begin{array}{cc}\\cos\\theta&-\\sin\\theta\\\\ \\sin\\theta&\\cos\\theta\\end{array}\\right]x.\\] (7.1)\nThis matrix is called (for obvious reasons) a _rotation matrix_.\nReflection.Suppose that \\(y\\) is the vector obtained by reflecting \\(x\\) through the line that passes through the origin, inclined \\(\\theta\\) radians with respect to horizontal. Then we have\n\\[y=\\left[\\begin{array}{cc}\\cos(2\\theta)&\\sin(2\\theta)\\\\ \\sin(2\\theta)&-\\cos(2\\theta)\\end{array}\\right]x.\\]\nProjection onto a line.The projection of the point \\(x\\) onto a set is the point in the set that is closest to \\(x\\). Suppose \\(y\\) is the projection of \\(x\\) onto the line that passes through the origin, inclined \\(\\theta\\) radians with respect to horizontal. Then we have\n\\[y=\\left[\\begin{array}{cc}(1/2)(1+\\cos(2\\theta))&(1/2)\\sin(2\\theta)\\\\ (1/2)\\sin(2\\theta)&(1/2)(1-\\cos(2\\theta))\\end{array}\\right]x.\\]\nSome of these geometric transformations are illustrated in figure 7.1.\nFinding the matrix.When a geometric transformation is represented by matrix-vector multiplication (as in the examples above), a simple method to find the matrix is to find its columns. The \\(i\\)th column is the vector obtained by applying the transformation to \\(e_{i}\\). As a simple example consider clockwise rotation by \\(90^{\\circ}\\) in 2-D. Rotating the vector \\(e_{1}=(1,0)\\) by \\(90^{\\circ}\\) gives \\((0,-1)\\); rotating \\(e_{2}=(0,1)\\) by \\(90^{\\circ}\\) gives \\((1,0)\\). So rotation by \\(90^{\\circ}\\) is given by\n\\[y=\\left[\\begin{array}{cc}0&1\\\\ -1&0\\end{array}\\right]x.\\]\nChange of coordinates.In many applications multiple coordinate systems are used to describe locations or positions in 2-D or 3-D. For example in aerospace engineering we can describe a position using _earth-fixed_ coordinates or _body-fixed_ coordinates, where the body refers to an aircraft. Earth-fixed coordinates are with respect to a specific origin, with the three axes pointing East, North, and straight up, respectively. The origin of the body-fixed coordinates is a specific location on the aircraft (typically the center of gravity), and the three axes point forward (along the aircraft body), left (with respect to the aircraft body), and up (with respect to the aircraft body). Suppose the 3-vector \\(x^{\\text{body}}\\) describes a location using the body coordinates, and \\(x^{\\text{earth}}\\) describes the same location in earth-fixed coordinates. These are related by\n\\[x^{\\text{earth}}=p+Qx^{\\text{body}},\\]\nwhere \\(p\\) is the location of the airplane center (in earth-fixed coordinates) and \\(Q\\) is a \\(3\\times 3\\) matrix. The \\(i\\)th column of \\(Q\\) gives the earth-fixed coordinates for the \\(i\\)th\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Does the following transformation have an eigenvector: Counterclockwise rotation through an angle of 45 degrees followed by a scaling by 2 in R^2.",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_3.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "wenhuchen/factor's_theory.json",
        "question": "Is x-1 a factor of 2*x^4+3*x^2-5x+7?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs x-1 a factor of 2*x^4+3*x^2-5x+7?\n\nDocument 1:\n----\n\\[Ax=b.\\] (8.5)\nIn this context, the \\(m\\times n\\) matrix \\(A\\) is called the _coefficient matrix_, and the \\(m\\)-vector \\(b\\) is called the _right-hand side_. An \\(n\\)-vector \\(x\\) is called a _solution_ of the linear equations if \\(Ax=b\\) holds. A set of linear equations can have no solutions, one solution, or multiple solutions.\nExamples.\n* The set of linear equations \\[x_{1}+x_{2}=1,\\quad x_{1}=-1,\\quad x_{1}-x_{2}=0\\] is written as \\(Ax=b\\) with \\[A=\\left[\\begin{array}{cc}1&1\\\\ 1&0\\\\ 1&-1\\end{array}\\right],\\qquad b=\\left[\\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right].\\] It has no solutions.\n* The set of linear equations \\[x_{1}+x_{2}=1,\\quad x_{2}+x_{3}=2\\] is written as \\(Ax=b\\) with \\[A=\\left[\\begin{array}{cc}1&1&0\\\\ 0&1&1\\end{array}\\right],\\qquad b=\\left[\\begin{array}{c}1\\\\ 2\\end{array}\\right].\\] It has multiple solutions, including \\(x=(1,0,2)\\) and \\(x=(0,1,1)\\).\nOver-determined and under-determined systems of linear equations.The set of linear equations is called _over-determined_ if \\(m>n\\), _under-determined_ if \\(m<n\\), and _square_ if \\(m=n\\); these correspond to the coefficient matrix being tall, wide, and square, respectively. When the system of linear equations is over-determined, there are more equations than variables or unknowns. When the system of linear equations is under-determined, there are more unknowns than equations. When the system of linear equations is square, the numbers of unknowns and equations is the same. A set of equations with zero right-hand side, \\(Ax=0\\), is called a _homogeneous_ set of equations. Any homogeneous set of equations has \\(x=0\\) as a solution.\nIn chapter 11 we will address the question of how to determine if a system of linear equations has a solution, and how to find one when it does. For now, we give a few interesting examples.\n#### Examples\nCoefficients of linear combinations.Let \\(a_{1},\\ldots,a_{n}\\) denote the columns of \\(A\\). The system of linear equations \\(Ax=b\\) can be expressed as\n\\[x_{1}a_{1}+\\cdots+x_{n}a_{n}=b,\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\) with coefficients \\(x_{1},\\ldots,x_{n}\\). So solving \\(Ax=b\\) is the same as finding coefficients that express \\(b\\) as a linear combination of the vectors \\(a_{1},\\ldots,a_{n}\\).\nPolynomial interpolation.We seek a polynomial \\(p\\) of degree at most \\(n-1\\) that interpolates a set of \\(m\\) given points \\((t_{i},y_{i})\\), \\(i=1,\\ldots,m\\). (This means that \\(p(t_{i})=y_{i}\\).) We can express this as a set of \\(m\\) linear equations in the \\(n\\) unknowns \\(c\\), where \\(c\\) is the \\(n\\)-vector of coefficients: \\(Ac=y\\). Here the matrix \\(A\\) is the Vandermonde matrix (6.7), and the vector \\(c\\) is the vector of polynomial coefficients, as described in the example on page 120.\nBalancing chemical reactions.A chemical reaction involves \\(p\\) reactants (molecules) and \\(q\\) products, and can be written as\n\\[a_{1}R_{1}+\\cdots+a_{p}R_{p}\\longrightarrow b_{1}P_{1}+\\cdots+b_{q}P_{q}.\\]\nHere \\(R_{1},\\ldots,R_{p}\\) are the reactants, \\(P_{1},\\ldots,P_{q}\\) are the products, and the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\) are positive numbers that tell us how many of each of these molecules is involved in the reaction. They are typically integers, but can be scaled arbitrarily; we could double all of these numbers, for example, and we still have the same reaction. As a simple example, we have the electrolysis of water,\n\\[2\\mathrm{H}_{2}\\mathrm{O}\\longrightarrow 2\\mathrm{H}_{2}+\\mathrm{O}_{2},\\]\nwhich has one reactant, water (\\(\\mathrm{H}_{2}\\mathrm{O}\\)), and two products, molecular hydrogen (\\(\\mathrm{H}_{2}\\)) and molecular oxygen (\\(\\mathrm{O}_{2}\\)). The coefficients tell us that 2 water molecules create 2 hydrogen molecules and 1 oxygen molecule. The coefficients in a reaction can be multiplied by any nonzero numbers; for example, we could write the reaction above as \\(3\\mathrm{H}_{2}\\mathrm{O}\\longrightarrow 3\\mathrm{H}_{2}+(3/2)\\mathrm{O}_{2}\\). By convention reactions are written with all coefficients integers, with least common divisor one.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is x-1 a factor of 2*x^4+3*x^2-5x+7?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "self",
            "id": "wenhuchen/factor's_theory.json",
            "explanation": "NONE",
            "theorem": "factor's theorem",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "wenhuchen/cramer's_rule1.json",
        "question": "For the two linear equations $2 * x + 3 * y = 10$ and $4 * x + 4 * y = 12$ iwth variables x and y. Use cramer's rule to solve these two variables.",
        "prediction": 4,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor the two linear equations $2 * x + 3 * y = 10$ and $4 * x + 4 * y = 12$ iwth variables x and y. Use cramer's rule to solve these two variables.\n\nDocument 1:\n----\nNow suppose there is no \\(x\\) that satisfies the linear equations \\(Ax=b\\), and let \\(C\\) be a left inverse of \\(A\\). Then \\(x=Cb\\) does not satisfy \\(Ax=b\\), since no vector satisfies this equation by assumption. This gives a way to check if the linear equations \\(Ax=b\\) have a solution, and to find one when there is one, provided we have a left inverse of \\(A\\). We simply test whether \\(A(Cb)=b\\). If this holds, then we have found a solution of the linear equations; if it does not, then we can conclude that there is no solution of \\(Ax=b\\).\nIn summary, a left inverse can be used to determine whether or not a solution of an over-determined set of linear equations exists, and when it does, find the unique solution.\nRight inverse.Now we turn to the closely related concept of right inverse. A matrix \\(X\\) that satisfies\n\\[AX=I\\]\nis called a _right inverse_ of \\(A\\). The matrix \\(A\\) is _right-invertible_ if a right inverse exists. Any right inverse has the same dimensions as \\(A^{T}\\).\nLeft and right inverse of matrix transpose.If \\(A\\) has a right inverse \\(B\\), then \\(B^{T}\\) is a left inverse of \\(A^{T}\\), since \\(B^{T}A^{T}=(AB)^{T}=I\\). If \\(A\\) has a left inverse \\(C\\), then \\(C^{T}\\) is a right inverse of \\(A^{T}\\), since \\(A^{T}C^{T}=(CA)^{T}=I\\). This observation allows us to map all the results for left-invertibility given above to similar results for right-invertibility. Some examples are given below.\n* A matrix is right-invertible if and only if its rows are linearly independent.\n* A tall matrix cannot have a right inverse. Only square or wide matrices can be right-invertible.\nSolving linear equations with a right inverse.Consider the set of \\(m\\) linear equations in \\(n\\) variables \\(Ax=b\\). Suppose \\(A\\) is right-invertible, with right inverse \\(B\\). This implies that \\(A\\) is square or wide, so the linear equations \\(Ax=b\\) are square or under-determined.\nThen for _any_\\(m\\)-vector \\(b\\), the \\(n\\)-vector \\(x=Bb\\) satisfies the equation \\(Ax=b\\). To see this, we note that\n\\[Ax=A(Bb)=(AB)b=Ib=b.\\]\nWe can conclude that if \\(A\\) is right-invertible, then the linear equations \\(Ax=b\\) can be solved for _any_ vector \\(b\\). Indeed, \\(x=Bb\\) is a solution. (There can be other solutions of \\(Ax=b\\); the solution \\(x=Bb\\) is simply one of them.)\nIn summary, a right inverse can be used to find \\(a\\) solution of a square or under-determined set of linear equations, for any vector \\(b\\).\nExamples.Consider the matrix appearing in the example above on page 199,\n\\[A=\\left[\\begin{array}{rr}-3&-4\\\\ 4&6\\\\ 1&1\\end{array}\\right]\\]\nand the two left inverses\n\\[B=\\frac{1}{9}\\left[\\begin{array}{rr}-11&-10&16\\\\ 7&8&-11\\end{array}\\right],\\qquad C=\\frac{1}{2}\\left[\\begin{array}{rr}0&-1&6 \\\\ 0&1&-4\\end{array}\\right].\\]\n* The over-determined linear equations \\(Ax=(1,-2,0)\\) have the unique solution \\(x=(1,-1)\\), which can be obtained from _either_ left inverse: \\[x=B(1,-2,0)=C(1,-2,0).\\]\n* The over-determined linear equations \\(Ax=(1,-1,0)\\) do not have a solution, since \\(x=C(1,-1,0)=(1/2,-1/2)\\) does not satisfy \\(Ax=(1,-1,0)\\).\n* The under-determined linear equations \\(A^{T}y=(1,2)\\) has (different) solutions \\[B^{T}(1,2)=(1/3,2/3,38/9),\\qquad C^{T}(1,2)=(0,1/2,-1).\\] (Recall that \\(B^{T}\\) and \\(C^{T}\\) are both right inverses of \\(A^{T}\\).) We can find a solution of \\(A^{T}y=b\\) for any vector \\(b\\).\nLeft and right inverse of matrix product.Suppose \\(A\\) and \\(D\\) are compatible for the matrix product \\(AD\\) (_i.e._, the number of columns in \\(A\\) is equal to the number of rows in \\(D\\).) If \\(A\\) has a right inverse \\(B\\) and \\(D\\) has a right inverse \\(E\\), then \\(EB\\) is a right inverse of \\(AD\\). This follows from \\[(AD)(EB)=A(DE)B=A(IB)=AB=I.\\] If \\(A\\) has a left inverse \\(C\\) and \\(D\\) has a left inverse \\(F\\), then \\(FC\\) is a left inverse of \\(AD\\). This follows from \\[(FC)(AD)=F(CA)D=FD=I.\\]\n\n\n",
        "answer": [
            -1,
            4
        ],
        "rationale": {
            "Question": "For the two linear equations $2 * x + 3 * y = 10$ and $4 * x + 4 * y = 12$ iwth variables x and y. Use cramer's rule to solve these two variables.",
            "Answer": [
                -1,
                4
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/cramer's_rule1.json",
            "explanation": "NONE",
            "theorem": "cramer's rule",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_6_3.json",
        "question": "What is the determinant of matrix [[0, 1, 2], [7, 8, 3], [6, 5, 4]]?",
        "prediction": 61,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the determinant of matrix [[0, 1, 2], [7, 8, 3], [6, 5, 4]]?\n\nDocument 1:\n----\n2. Give an expression for a middle inverse, assuming the conditions in part (a) hold.\n**11.15**: _Invertibility of population dynamics matrix._ Consider the population dynamics matrix\n\\[A=\\left[\\begin{array}{cccccc}b_{1}&b_{2}&\\cdots&b_{99}&b_{100}\\\\ 1-d_{1}&0&\\cdots&0&0\\\\ 0&1-d_{2}&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 0&0&\\cdots&1-d_{99}&0\\end{array}\\right],\\]\nwhere \\(b_{i}\\geq 0\\) are the birth rates and \\(0\\leq d_{i}\\leq 1\\) are death rates. What are the conditions on \\(b_{i}\\) and \\(d_{i}\\) under which \\(A\\) is invertible? (If the matrix is never invertible or always invertible, say so.) Justify your answer.\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n**11.20**: _Immigration._ The population dynamics of a country is given by \\(x_{t+1}=Ax_{t}+u\\), \\(t=1,\\ldots,T-1\\), where the 100-vector \\(x_{t}\\) gives the population age distribution in year \\(t\\), and \\(u\\) gives the immigration age distribution (with negative entries meaning emigration), which we assume is constant (_i.e._, does not vary with \\(t\\)). You are given \\(A\\), \\(x_{1}\\), and \\(x^{\\rm des}\\), a 100-vector that represents a desired population distribution in year \\(T\\). We seek a constant level of immigration \\(u\\) that achieves \\(x_{T}=x^{\\rm des}\\).\nGive a matrix formula for \\(u\\). If your formula only makes sense when some conditions hold (for example invertibility of one or more matrices), say so.\n**11.21**: _Quadrature weights._ Consider a quadrature problem (see exercise 8.12) with \\(n=4\\), with points \\(t=(-0.6,-0.2,0.2,0.6)\\). We require that the quadrature rule be exact for all polynomials of degree up to \\(d=3\\).\nSet this up as a square system of linear equations in the weight vector. Numerically solve this system to get the weights. Compute the true value and the quadrature estimate,\n\n\n",
        "answer": -36,
        "rationale": {
            "Question": "What is the determinant of matrix [[0, 1, 2], [7, 8, 3], [6, 5, 4]]?",
            "Answer": -36,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_3.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "wenhuchen/eigen_value1.json",
        "question": "For matrix A = [[5, 4], [1, 2]], what are its eigen values?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[5, 4], [1, 2]], what are its eigen values?\n\nDocument 1:\n----\n* _Ones matrix_. There is no special notation for an \\(m\\times n\\) matrix all of whose entries are one. Give a simple expression for this matrix in terms of matrix multiplication, transpose, and the ones vectors \\(\\mathbf{1}_{m}\\), \\(\\mathbf{1}_{n}\\) (where the subscripts denote the dimension).\n* _Matrix sizes_. Suppose \\(A\\), \\(B\\), and \\(C\\) are matrices that satisfy \\(A+BB^{T}=C\\). Determine which of the following statements are necessarily true. (There may be more than one true statement.)\n* \\(A\\) is square.\n* \\(A\\) and \\(B\\) have the same dimensions.\n* \\(A\\), \\(B\\), and \\(C\\) have the same number of rows.\n* \\(B\\) is a tall matrix.\n* _Block matrix notation_. Consider the block matrix \\[A=\\left[\\begin{array}{ccc}I&B&0\\\\ B^{T}&0&0\\\\ 0&0&BB^{T}\\end{array}\\right],\\] where \\(B\\) is \\(10\\times 5\\). What are the dimensions of the four zero matrices and the identity matrix in the definition of \\(A\\)? What are the dimensions of \\(A\\)?\n* _When is the outer product symmetric?_ Let \\(a\\) and \\(b\\) be \\(n\\)-vectors. The inner product is symmetric, _i.e._, we have \\(a^{T}b=b^{T}a\\). The outer product of the two vectors is generally _not_ symmetric; that is, we generally have \\(ab^{T}\\neq ba^{T}\\). What are the conditions on \\(a\\) and \\(b\\) under which \\(ab=ba^{T}\\)? You can assume that all the entries of \\(a\\) and \\(b\\) are nonzero. (The conclusion you come to will hold even when some entries of \\(a\\) or \\(b\\) are zero.) _Hint_. Show that \\(ab^{T}=ba^{T}\\) implies that \\(a_{i}/b_{i}\\) is a constant (_i.e._, independent of \\(i\\)).\n* _Product of rotation matrices_. Let \\(A\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\theta\\) radians, defined in (7.1), and let \\(B\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\omega\\) radians. Show that \\(AB\\) is also a rotation matrix, and give the angle by which it rotates vectors. Verify that \\(AB=BA\\) in this case, and give a simple English explanation.\n* _Two rotations_. Two 3-vectors \\(x\\) and \\(y\\) are related as follows. First, the vector \\(x\\) is rotated \\(40^{\\circ}\\) around the \\(e_{3}\\) axis, counterclockwise (from \\(e_{1}\\) toward \\(e_{2}\\)), to obtain the 3-vector \\(z\\). Then, \\(z\\) is rotated \\(20^{\\circ}\\) around the \\(e_{1}\\) axis, counterclockwise (from \\(e_{2}\\) toward \\(e_{3}\\)), to form \\(y\\). Find the \\(3\\times 3\\) matrix \\(A\\) for which \\(y=Ax\\). Verify that \\(A\\) is an orthogonal matrix. _Hint_. Express \\(A\\) as a product of two matrices, which carry out the two rotations described above.\n* _Entries of matrix triple product_. (See page 10.) Suppose \\(A\\) has dimensions \\(m\\times n\\), \\(B\\) has dimensions \\(n\\times p\\), \\(C\\) has dimensions \\(p\\times q\\), and let \\(D=ABC\\). Show that \\[D_{ij}=\\sum_{k=1}^{n}\\sum_{l=1}^{p}A_{ik}B_{kl}C_{lj}.\\] This is the formula analogous to (10.1) for the product of two matrices.\n* _Multiplication by a diagonal matrix_. Suppose that \\(A\\) is an \\(m\\times n\\) matrix, \\(D\\) is a diagonal matrix, and \\(B=DA\\). Describe \\(B\\) in terms of \\(A\\) and the entries of \\(D\\). You can refer to the rows or columns or entries of \\(A\\).\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n\n\n",
        "answer": [
            1,
            6
        ],
        "rationale": {
            "Question": "For matrix A = [[5, 4], [1, 2]], what are its eigen values?",
            "Answer": [
                1,
                6
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/eigen_value1.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_5.json",
        "question": "Find the orthogonal projection of 9e_1 onto the subspace of R^4 spanned by [2, 2, 1, 0] and [-2, 2, 0, 1].",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFind the orthogonal projection of 9e_1 onto the subspace of R^4 spanned by [2, 2, 1, 0] and [-2, 2, 0, 1].\n\nDocument 1:\n----\n\\[\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}U&U\\\\ V&-V\\end{array}\\right]\\]\nare orthogonal.\n**10.36**: _Quadratic form._ Suppose \\(A\\) is an \\(n\\times n\\) matrix and \\(x\\) is an \\(n\\)-vector. The triple product \\(x^{T}Ax\\), a \\(1\\times 1\\) matrix which we consider to be a scalar (_i.e._, number), is called a _quadratic form_ of the vector \\(x\\), with coefficient matrix \\(A\\). A quadratic form is the vector analog of a quadratic function \\(\\alpha u^{2}\\), where \\(\\alpha\\) and \\(u\\) are both numbers. Quadratic forms arise in many fields and applications.\n1. Show that \\(x^{T}Ax=\\sum_{i,j=1}^{n}A_{ij}x_{i}x_{j}\\).\n2. Show that \\(x^{T}(A^{T})x=x^{T}Ax\\). In other words, the quadratic form with the transposed coefficient matrix has the same value for any \\(x\\). _Hint._ Take the transpose of the triple product \\(x^{T}Ax\\).\n3. Show that \\(x^{T}((A+A^{T})/2)x=x^{T}Ax\\). In other words, the quadratic form with coefficient matrix equal to the symmetric part of a matrix (_i.e._, \\((A+A^{T})/2\\)) has the same value as the original quadratic form.\n4. Express \\(2x_{1}^{2}-3x_{1}x_{2}-x_{2}^{2}\\) as a quadratic form, with symmetric coefficient matrix \\(A\\).\n**10.37**: _Orthogonal \\(2\\times 2\\) matrices._ In this problem, you will show that every \\(2\\times 2\\) orthogonal matrix is either a rotation or a reflection (see SS7.1).\n1. Let \\[Q=\\left[\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right]\\] be an orthogonal \\(2\\times 2\\) matrix. Show that the following equations hold: \\[a^{2}+c^{2}=1,\\qquad b^{2}+d^{2}=1,\\qquad ab+cd=0.\\] 2. Define \\(s=ad-bc\\). Combine the three equalities in part (a) to show that \\[|s|=1,\\qquad b=-sc,\\qquad d=sa.\\] 3. Suppose \\(a=\\cos\\theta\\). Show that there are two possible matrices \\(Q\\): A rotation (counterclockwise over \\(\\theta\\) radians), and a reflection (through the line that passes through the origin at an angle of \\(\\theta/2\\) radians with respect to horizontal).\n**10.38**: _Orthogonal matrix with nonnegative entries._ Suppose the \\(n\\times n\\) matrix \\(A\\) is orthogonal, and all of its entries are nonnegative, _i.e._, \\(A_{ij}\\geq 0\\) for \\(i,j=1,\\ldots,n\\). Show that \\(A\\) must be a permutation matrix, _i.e._, each entry is either \\(0\\) or \\(1\\), each row has exactly one entry with value one, and each column has exactly one entry with value one. (See page 132.)\n**10.39**: _Gram matrix and QR factorization._ Suppose the matrix \\(A\\) has linearly independent columns and QR factorization \\(A=QR\\). What is the relationship between the Gram matrix of \\(A\\) and the Gram matrix of \\(R\\)? What can you say about the angles between the columns of \\(A\\) and the angles between the columns of \\(R\\)?\n**10.40**: _QR factorization of first \\(i\\) columns of \\(A\\)._ Suppose the \\(n\\times k\\) matrix \\(A\\) has QR factorization \\(A=QR\\). We define the \\(n\\times i\\) matrices\n\\[A_{i}=\\left[\\begin{array}{cccc}a_{1}&\\cdots&a_{i}\\end{array}\\right],\\qquad Q _{i}=\\left[\\begin{array}{cccc}q_{1}&\\cdots&q_{i}\\end{array}\\right],\\]\nfor \\(i=1,\\ldots,k\\). Define the \\(i\\times i\\) matrix \\(R_{i}\\) as the submatrix of \\(R\\) containing its first \\(i\\) rows and columns, for \\(i=1,\\ldots,k\\). Using index range notation, we have\n\\[A_{i}=A_{1:n,1:i},\\quad Q_{i}=A_{1:n,1:i},\\quad R_{i}=R_{1:i,1:i}.\\]\nShow that \\(A_{i}=Q_{i}R_{i}\\) is the QR factorization of \\(A_{i}\\). This means that when you compute the QR factorization of \\(A\\), you are also computing the QR factorization of all submatrices \\(A_{1},\\ldots,A_{k}\\).\n**10.41**: _Clustering via \\(k\\)-means as an approximate matrix factorization._ Suppose we run the \\(k\\)-means algorithm on the \\(N\\)\\(n\\)-vectors \\(x_{1},\\ldots,x_{N}\\), to obtain the group representatives \\(z_{1},\\ldots,z_{k}\\). Define the matrices \\(X=[\\begin{array}{cccc}x_{1}&\\cdots&x_{N}\\end{array}]\\), \\(Z=[\\begin{array}{cccc}z_{1}&\\cdots&z_{N}\\end{array}]\\). \\(X\\) has size \\(n\\times N\\) and \\(Z\\) has size \\(n\\times k\\). We encode the assignment of vectors to groups by the \\(k\\times N\\) clustering matrix \\(C\\), with \\(C_{ij}=1\\) if \\(x_{j}\\) is assigned to group \\(i\\), and \\(C_{ij}=0\\) otherwise. Each column of \\(C\\) is a unit vector; its transpose is a selector matrix. 1. Give an interpretation of the columns of the matrix \\(X-ZC\\), and the squared norm \\(\\|X-ZC\\|^{2}\\). 2. Justify the following statement: The goal of the \\(k\\)-means algorithm is to find an \\(n\\times k\\) matrix \\(Z\\), and a \\(k\\times N\\) matrix \\(C\\), which is the transpose of a selector matrix, so that \\(\\|X-ZC\\|\\) is small, _i.e._, \\(X\\approx ZC\\).\n\n\n",
        "answer": [
            8,
            0,
            2,
            -2
        ],
        "rationale": {
            "Question": "Find the orthogonal projection of 9e_1 onto the subspace of R^4 spanned by [2, 2, 1, 0] and [-2, 2, 0, 1].",
            "Answer": [
                8,
                0,
                2,
                -2
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_5.json",
            "explanation": "NONE",
            "theorem": "projection theory",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_6.json",
        "question": "For a matrix A, is the function F(A) = det A from the linear space R^{3*3} to R a linear transformation?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor a matrix A, is the function F(A) = det A from the linear space R^{3*3} to R a linear transformation?\n\nDocument 1:\n----\nVector-valued functions of vectors.The notation \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}^{m}\\) means that \\(f\\) is a function that maps real \\(n\\)-vectors to real \\(m\\)-vectors. The value of the function \\(f\\), evaluated at an \\(n\\)-vector \\(x\\), is an \\(m\\)-vector \\(f(x)=(f_{1}(x),f_{2}(x),\\ldots,f_{m}(x))\\). Each of the components \\(f_{i}\\) of \\(f\\) is itself a scalar-valued function of \\(x\\). As with scalar-valued functions, we sometimes write \\(f(x)=f(x_{1},x_{2},\\ldots,x_{n})\\) to emphasize that \\(f\\) is a function of \\(n\\) scalar arguments. We use the same notation for each of the components of \\(f\\), writing \\(f_{i}(x)=f_{i}(x_{1},x_{2},\\ldots,x_{n})\\) to emphasize that \\(f_{i}\\) is a function mapping the scalar arguments \\(x_{1},\\ldots,x_{n}\\) into a scalar.\nThe matrix-vector product function.Suppose \\(A\\) is an \\(m\\times n\\) matrix. We can define a function \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}^{m}\\) by \\(f(x)=Ax\\). The inner product function \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}\\), defined as \\(f(x)=a^{T}x\\), discussed in SS2.1, is the special case with \\(m=1\\).\nSuperposition and linearity.The function \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}^{m}\\), defined by \\(f(x)=Ax\\), is _linear_, _i.e._, it satisfies the superposition property:\n\\[f(\\alpha x+\\beta y)=\\alpha f(x)+\\beta f(y)\\] (8.1)\nholds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). It is a good exercise to parse this simple looking equation, since it involves overloading of notation. On the left-hand side, the scalar-vector multiplications \\(\\alpha x\\) and \\(\\beta y\\) involve \\(n\\)-vectors, and the sum \\(\\alpha x+\\beta y\\) is the sum of two \\(n\\)-vectors. The function \\(f\\) maps \\(n\\)-vectors to \\(m\\)-vectors, so \\(f(\\alpha x+\\beta y)\\) is an \\(m\\)-vector. On the right-hand side, the scalar-vector multiplications and the sum are those for \\(m\\)-vectors. Finally, the equality sign is equality between two \\(m\\)-vectors.\nWe can verify that superposition holds for \\(f\\) using properties of matrix-vector and scalar-vector multiplication:\n\\[f(\\alpha x+\\beta y) = A(\\alpha x+\\beta y)\\] \\[= A(\\alpha x)+A(\\beta y)\\] \\[= \\alpha(Ax)+\\beta(Ay)\\] \\[= \\alpha f(x)+\\beta f(y)\\]\nThus we can associate with every matrix \\(A\\) a linear function \\(f(x)=Ax\\).\nThe converse is also true. Suppose \\(f\\) is a function that maps \\(n\\)-vectors to \\(m\\)-vectors, and is linear, _i.e._, (8.1) holds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). Then there exists an \\(m\\times n\\) matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\). This can be shown in the same way as for scalar-valued functions in SS2.1, by showing that if \\(f\\) is linear, then\n\\[f(x)=x_{1}f(e_{1})+x_{2}f(e_{2})+\\cdots+x_{n}f(e_{n}),\\] (8.2)\nwhere \\(e_{k}\\) is the \\(k\\)th unit vector of size \\(n\\). The right-hand side can also be written as a matrix-vector product \\(Ax\\), with\n\\[A=\\left[\\begin{array}{cccc}f(e_{1})&f(e_{2})&\\cdots&f(e_{n})\\end{array} \\right].\\]\nThe expression (8.2) is the same as (2.3), but here \\(f(x)\\) and \\(f(e_{k})\\) are vectors. The implications are exactly the same: A linear vector-valued function \\(f\\) is completely characterized by evaluating \\(f\\) at the \\(n\\) unit vectors \\(e_{1},\\ldots,e_{n}\\).\nAs in SS2.1 it is easily shown that the matrix-vector representation of a linear function is unique. If \\(f:{\\bf R}^{n}\\to{\\bf R}^{m}\\) is a linear function, then there exists exactly one matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\).\nExamples of linear functions.In the examples below we define functions \\(f\\) that map \\(n\\)-vectors \\(x\\) to \\(n\\)-vectors \\(f(x)\\). Each function is described in words, in terms of its effect on an arbitrary \\(x\\). In each case we give the associated matrix multiplication representation.\n* _Negation._\\(f\\) changes the sign of \\(x\\): \\(f(x)=-x\\). Negation can be expressed as \\(f(x)=Ax\\) with \\(A=-I\\).\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "For a matrix A, is the function F(A) = det A from the linear space R^{3*3} to R a linear transformation?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_3_4.json",
        "question": "Are the vectors [1, 2], [2, 3], and [3, 4] linearly independent?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nAre the vectors [1, 2], [2, 3], and [3, 4] linearly independent?\n\nDocument 1:\n----\nAs with linear dependence, we will say \"The vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent\" to mean \"The list of vectors \\(a_{1},\\ldots,a_{k}\\) is linearly independent\". But, like linear dependence, linear independence is an attribute of a collection of vectors, and not individual vectors.\nIt is generally not easy to determine by casual inspection whether or not a list of vectors is linearly dependent or linearly independent. But we will soon see an algorithm that does this.\n#### Examples.\n* A list consisting of a single vector is linearly dependent only if the vector is zero. It is linearly independent only if the vector is nonzero.\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\\[x=\\gamma_{1}a_{1}+\\cdots+\\gamma_{k}a_{k},\\]\nthen \\(\\beta_{i}=\\gamma_{i}\\) for \\(i=1,\\ldots,k\\). This tells us that, in principle at least, we can find the coefficients that form a vector \\(x\\) as a linear combination of linearly independent vectors.\nTo see this, we subtract the two equations above to get\n\\[0=(\\beta_{1}-\\gamma_{1})a_{1}+\\cdots+(\\beta_{k}-\\gamma_{k})a_{k}.\\]\nSince \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we conclude that \\(\\beta_{i}-\\gamma_{i}\\) are all zero.\nThe converse is also true: If each linear combination of a list of vectors can only be expressed as a linear combination with one set of coefficients, then the list of vectors is linearly independent. This gives a nice interpretation of linear independence: A list of vectors is linearly independent if and only if for any linear combination of them, we can infer or deduce the associated coefficients. (We will see later how to do this.)\nSupersets and subsets.If a collection of vectors is linearly dependent, then any superset of it is linearly dependent. In other words: If we add vectors to a linearly dependent collection of vectors, the new collection is also linearly dependent. Any nonempty subset of a linearly independent collection of vectors is linearly independent. In other words: Removing vectors from a collection of vectors preserves linear independence.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Are the vectors [1, 2], [2, 3], and [3, 4] linearly independent?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_4.json",
            "explanation": "solutions/math_algebra_3_4.txt",
            "theorem": "linear independence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_3_2.json",
        "question": "Are the vectors v_1 = [1,2,3], v_2 = [4,5,6], v_3 = [7,8,9] linearly independent?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nAre the vectors v_1 = [1,2,3], v_2 = [4,5,6], v_3 = [7,8,9] linearly independent?\n\nDocument 1:\n----\nAs with linear dependence, we will say \"The vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent\" to mean \"The list of vectors \\(a_{1},\\ldots,a_{k}\\) is linearly independent\". But, like linear dependence, linear independence is an attribute of a collection of vectors, and not individual vectors.\nIt is generally not easy to determine by casual inspection whether or not a list of vectors is linearly dependent or linearly independent. But we will soon see an algorithm that does this.\n#### Examples.\n* A list consisting of a single vector is linearly dependent only if the vector is zero. It is linearly independent only if the vector is nonzero.\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\\[x=\\gamma_{1}a_{1}+\\cdots+\\gamma_{k}a_{k},\\]\nthen \\(\\beta_{i}=\\gamma_{i}\\) for \\(i=1,\\ldots,k\\). This tells us that, in principle at least, we can find the coefficients that form a vector \\(x\\) as a linear combination of linearly independent vectors.\nTo see this, we subtract the two equations above to get\n\\[0=(\\beta_{1}-\\gamma_{1})a_{1}+\\cdots+(\\beta_{k}-\\gamma_{k})a_{k}.\\]\nSince \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we conclude that \\(\\beta_{i}-\\gamma_{i}\\) are all zero.\nThe converse is also true: If each linear combination of a list of vectors can only be expressed as a linear combination with one set of coefficients, then the list of vectors is linearly independent. This gives a nice interpretation of linear independence: A list of vectors is linearly independent if and only if for any linear combination of them, we can infer or deduce the associated coefficients. (We will see later how to do this.)\nSupersets and subsets.If a collection of vectors is linearly dependent, then any superset of it is linearly dependent. In other words: If we add vectors to a linearly dependent collection of vectors, the new collection is also linearly dependent. Any nonempty subset of a linearly independent collection of vectors is linearly independent. In other words: Removing vectors from a collection of vectors preserves linear independence.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Are the vectors v_1 = [1,2,3], v_2 = [4,5,6], v_3 = [7,8,9] linearly independent?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_2.json",
            "explanation": "solutions/math_algebra_3_2.png",
            "theorem": "linear independence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_additional_1.json",
        "question": "Let f_1, ..., f_n be polynomials. Do they span the space P of all polynomials?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet f_1, ..., f_n be polynomials. Do they span the space P of all polynomials?\n\nDocument 1:\n----\n### 11.4 Examples\nPolynomial interpolation.The 4-vector \\(c\\) gives the coefficients of a cubic polynomial,\n\\[p(x)=c_{1}+c_{2}x+c_{3}x^{2}+c_{4}x^{3}\\]\n(see pages 154 and 120). We seek the coefficients that satisfy\n\\[p(-1.1)=b_{1},\\qquad p(-0.4)=b_{2},\\qquad p(0.2)=b_{3},\\qquad p(0.8)=b_{4}.\\]\nWe can express this as the system of 4 equations in 4 variables \\(Ac=b\\), where\n\\[A=\\left[\\begin{array}{rrrr}1&-1.1&(-1.1)^{2}&(-1.1)^{3}\\\\ 1&-0.4&(-0.4)^{2}&(-0.4)^{3}\\\\ 1&0.2&(0.2)^{2}&(0.2)^{3}\\\\ 1&0.8&(0.8)^{2}&(0.8)^{3}\\end{array}\\right],\\]\nwhich is a specific Vandermonde matrix (see (6.7)). The unique solution is \\(c=A^{-1}b\\), where\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\nBalancing chemical reactions.(See page 154 for background.) We consider the problem of balancing the chemical reaction\n\\[a_{1}\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+a_{2}\\mathrm{Fe}^{2+}+a_{3}\\mathrm{H} ^{+}\\longrightarrow b_{1}\\mathrm{Cr}^{3+}+b_{2}\\mathrm{Fe}^{3+}+b_{3} \\mathrm{H}_{2}\\mathrm{O},\\]\nwhere the superscript gives the charge of each reactant and product. There are 4 atoms (Cr, O, Fe, H) and charge to balance. The reactant and product matrices are (using the order just listed)\n\\[R=\\left[\\begin{array}{ccc}2&0&0\\\\ 7&0&0\\\\ 0&1&0\\\\ 0&0&1\\\\ -2&2&1\\end{array}\\right],\\qquad P=\\left[\\begin{array}{ccc}1&0&0\\\\ 0&0&1\\\\ 0&1&0\\\\ 0&0&2\\\\ 3&3&0\\end{array}\\right].\\]\nFigure 11.1 Cubic interpolants through two sets of points, shown as circles and squares.\n## Chapter 11 Matrix inverses\n### 11.1 Matrix inverses\nThe matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.1)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.2)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.3)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.4)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.5)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.6)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.7)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.8)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Let f_1, ..., f_n be polynomials. Do they span the space P of all polynomials?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "self",
            "id": "elainewan/math_algebra_additional_1.json",
            "explanation": "NONE",
            "theorem": "linear span",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_6_5.json",
        "question": "What is the determinant of the matrix A = [[1, 0, 0, 0, 0, 0], [2, 7, 0, 0, 0, 0], [3, 8, 6, 0, 0, 0], [4, 9, 5, 2, 1, 4], [5, 8, 4, 0, 2, 5], [6, 7, 3, 0, 3, 6]]?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the determinant of the matrix A = [[1, 0, 0, 0, 0, 0], [2, 7, 0, 0, 0, 0], [3, 8, 6, 0, 0, 0], [4, 9, 5, 2, 1, 4], [5, 8, 4, 0, 2, 5], [6, 7, 3, 0, 3, 6]]?\n\nDocument 1:\n----\nHere \\(R_{1},\\ldots,R_{p}\\) are the reactants, \\(P_{1},\\ldots,P_{q}\\) are the products, and the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\) are positive numbers that tell us how many of each of these molecules is involved in the reaction. They are typically integers, but can be scaled arbitrarily; we could double all of these numbers, for example, and we still have the same reaction. As a simple example, we have the electrolysis of water,\n\\[2\\mathrm{H}_{2}\\mathrm{O}\\longrightarrow 2\\mathrm{H}_{2}+\\mathrm{O}_{2},\\]\nwhich has one reactant, water (\\(\\mathrm{H}_{2}\\mathrm{O}\\)), and two products, molecular hydrogen (\\(\\mathrm{H}_{2}\\)) and molecular oxygen (\\(\\mathrm{O}_{2}\\)). The coefficients tell us that 2 water molecules create 2 hydrogen molecules and 1 oxygen molecule. The coefficients in a reaction can be multiplied by any nonzero numbers; for example, we could write the reaction above as \\(3\\mathrm{H}_{2}\\mathrm{O}\\longrightarrow 3\\mathrm{H}_{2}+(3/2)\\mathrm{O}_{2}\\). By convention reactions are written with all coefficients integers, with least common divisor one.\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\nA simple solution of these equations is \\(a=0\\), \\(b=0\\). But we seek a nonzero solution. We can set one of the coefficients, say \\(a_{1}\\), to be one. (This might cause the other quantities to be fractional-valued.) We can add the condition that \\(a_{1}=1\\) to our system of linear equations as\n\\[\\left[\\begin{array}{cc}R&-P\\\\ e_{1}^{T}&0\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=e_{m+1}.\\]\nFinally, we have a set of \\(m+1\\) equations in \\(p+q\\) variables that expresses the requirement that the chemical reaction balances. Finding a solution of this set of equations is called _balancing_ the chemical reaction.\n\n\n",
        "answer": -252,
        "rationale": {
            "Question": "What is the determinant of the matrix A = [[1, 0, 0, 0, 0, 0], [2, 7, 0, 0, 0, 0], [3, 8, 6, 0, 0, 0], [4, 9, 5, 2, 1, 4], [5, 8, 4, 0, 2, 5], [6, 7, 3, 0, 3, 6]]?",
            "Answer": -252,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_5.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "wenhuchen/cramer's_rule2.json",
        "question": "For the two linear equations $2 * x + 3 * y + z = 8$ and $4 * x + 4 * y + 4z = 12$ and $x + y + 8z = 10$ with variables x, y and z. Use cramer's rule to solve these three variables.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor the two linear equations $2 * x + 3 * y + z = 8$ and $4 * x + 4 * y + 4z = 12$ and $x + y + 8z = 10$ with variables x, y and z. Use cramer's rule to solve these three variables.\n\nDocument 1:\n----\nNow suppose there is no \\(x\\) that satisfies the linear equations \\(Ax=b\\), and let \\(C\\) be a left inverse of \\(A\\). Then \\(x=Cb\\) does not satisfy \\(Ax=b\\), since no vector satisfies this equation by assumption. This gives a way to check if the linear equations \\(Ax=b\\) have a solution, and to find one when there is one, provided we have a left inverse of \\(A\\). We simply test whether \\(A(Cb)=b\\). If this holds, then we have found a solution of the linear equations; if it does not, then we can conclude that there is no solution of \\(Ax=b\\).\nIn summary, a left inverse can be used to determine whether or not a solution of an over-determined set of linear equations exists, and when it does, find the unique solution.\nRight inverse.Now we turn to the closely related concept of right inverse. A matrix \\(X\\) that satisfies\n\\[AX=I\\]\nis called a _right inverse_ of \\(A\\). The matrix \\(A\\) is _right-invertible_ if a right inverse exists. Any right inverse has the same dimensions as \\(A^{T}\\).\nLeft and right inverse of matrix transpose.If \\(A\\) has a right inverse \\(B\\), then \\(B^{T}\\) is a left inverse of \\(A^{T}\\), since \\(B^{T}A^{T}=(AB)^{T}=I\\). If \\(A\\) has a left inverse \\(C\\), then \\(C^{T}\\) is a right inverse of \\(A^{T}\\), since \\(A^{T}C^{T}=(CA)^{T}=I\\). This observation allows us to map all the results for left-invertibility given above to similar results for right-invertibility. Some examples are given below.\n* A matrix is right-invertible if and only if its rows are linearly independent.\n* A tall matrix cannot have a right inverse. Only square or wide matrices can be right-invertible.\nSolving linear equations with a right inverse.Consider the set of \\(m\\) linear equations in \\(n\\) variables \\(Ax=b\\). Suppose \\(A\\) is right-invertible, with right inverse \\(B\\). This implies that \\(A\\) is square or wide, so the linear equations \\(Ax=b\\) are square or under-determined.\nThen for _any_\\(m\\)-vector \\(b\\), the \\(n\\)-vector \\(x=Bb\\) satisfies the equation \\(Ax=b\\). To see this, we note that\n\\[Ax=A(Bb)=(AB)b=Ib=b.\\]\nWe can conclude that if \\(A\\) is right-invertible, then the linear equations \\(Ax=b\\) can be solved for _any_ vector \\(b\\). Indeed, \\(x=Bb\\) is a solution. (There can be other solutions of \\(Ax=b\\); the solution \\(x=Bb\\) is simply one of them.)\nIn summary, a right inverse can be used to find \\(a\\) solution of a square or under-determined set of linear equations, for any vector \\(b\\).\nExamples.Consider the matrix appearing in the example above on page 199,\n\\[A=\\left[\\begin{array}{rr}-3&-4\\\\ 4&6\\\\ 1&1\\end{array}\\right]\\]\nand the two left inverses\n\\[B=\\frac{1}{9}\\left[\\begin{array}{rr}-11&-10&16\\\\ 7&8&-11\\end{array}\\right],\\qquad C=\\frac{1}{2}\\left[\\begin{array}{rr}0&-1&6 \\\\ 0&1&-4\\end{array}\\right].\\]\n* The over-determined linear equations \\(Ax=(1,-2,0)\\) have the unique solution \\(x=(1,-1)\\), which can be obtained from _either_ left inverse: \\[x=B(1,-2,0)=C(1,-2,0).\\]\n* The over-determined linear equations \\(Ax=(1,-1,0)\\) do not have a solution, since \\(x=C(1,-1,0)=(1/2,-1/2)\\) does not satisfy \\(Ax=(1,-1,0)\\).\n* The under-determined linear equations \\(A^{T}y=(1,2)\\) has (different) solutions \\[B^{T}(1,2)=(1/3,2/3,38/9),\\qquad C^{T}(1,2)=(0,1/2,-1).\\] (Recall that \\(B^{T}\\) and \\(C^{T}\\) are both right inverses of \\(A^{T}\\).) We can find a solution of \\(A^{T}y=b\\) for any vector \\(b\\).\nLeft and right inverse of matrix product.Suppose \\(A\\) and \\(D\\) are compatible for the matrix product \\(AD\\) (_i.e._, the number of columns in \\(A\\) is equal to the number of rows in \\(D\\).) If \\(A\\) has a right inverse \\(B\\) and \\(D\\) has a right inverse \\(E\\), then \\(EB\\) is a right inverse of \\(AD\\). This follows from \\[(AD)(EB)=A(DE)B=A(IB)=AB=I.\\] If \\(A\\) has a left inverse \\(C\\) and \\(D\\) has a left inverse \\(F\\), then \\(FC\\) is a left inverse of \\(AD\\). This follows from \\[(FC)(AD)=F(CA)D=FD=I.\\]\n\n\n",
        "answer": [
            -1,
            3,
            1
        ],
        "rationale": {
            "Question": "For the two linear equations $2 * x + 3 * y + z = 8$ and $4 * x + 4 * y + 4z = 12$ and $x + y + 8z = 10$ with variables x, y and z. Use cramer's rule to solve these three variables.",
            "Answer": [
                -1,
                3,
                1
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/cramer's_rule2.json",
            "explanation": "NONE",
            "theorem": "cramer's rule",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "jianyu_xu/integer_programming_1.json",
        "question": "30 students from 5 classes solved 40 math problems. Each student must answer at least one question. Every two students in the same class solved the same number of questions. The number of questions answered by any two students in different classes is also different. Question: What's maximum possible number of students who only answered one question?",
        "prediction": 10,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\n30 students from 5 classes solved 40 math problems. Each student must answer at least one question. Every two students in the same class solved the same number of questions. The number of questions answered by any two students in different classes is also different. Question: What's maximum possible number of students who only answered one question?\n\nDocument 1:\n----\n1. Let the \\(k\\)-vector \\(\\mu\\) give the means of the columns, _i.e._, \\(\\mu_{i}=\\mathbf{avg}(a_{i})\\), \\(i=1,\\ldots,k\\). (The symbol \\(\\mu\\) is a traditional one to denote an average value.) Give an expression for \\(\\mu\\) in terms of the matrix \\(A\\).\n2. Let \\(\\tilde{a}_{1},\\ldots,\\tilde{a}_{k}\\) be the de-meaned versions of \\(a_{1},\\ldots,a_{k}\\), and define \\(\\tilde{A}\\) as the \\(n\\times k\\) matrix \\(\\tilde{A}=[\\tilde{a}_{1}\\;\\cdots\\;\\tilde{a}_{k}]\\). Give a matrix expression for \\(\\tilde{A}\\) in terms of \\(A\\) and \\(\\mu\\).\n3. The _covariance matrix_ of the vectors \\(a_{1},\\ldots,a_{k}\\) is the \\(k\\times k\\) matrix \\(\\Sigma=(1/N)\\tilde{A}^{T}\\tilde{A}\\), the Gram matrix of \\(\\tilde{A}\\) multiplied with \\(1/N\\). Show that \\[\\Sigma_{ij}=\\left\\{\\begin{array}{ll}\\mathbf{std}(a_{i})^{2}&i=j\\\\ \\mathbf{std}(a_{i})\\,\\mathbf{std}(a_{j})\\rho_{ij}&i\\neq j\\end{array}\\right.\\] where \\(\\rho_{ij}\\) is the correlation coefficient of \\(a_{i}\\) and \\(a_{j}\\). (The expression for \\(i\\neq j\\) assumes that \\(\\rho_{ij}\\) is defined, _i.e._, \\(\\mathbf{std}(a_{i})\\) and \\(\\mathbf{std}(a_{j})\\) are nonzero. If not, we interpret the formula as \\(\\Sigma_{ij}=0\\).) Thus the covariance matrix encodes the standard deviations of the vectors, as well as correlations between all pairs. The correlation matrix is widely used in probability and statistics.\n4. Let \\(z_{1},\\ldots,z_{k}\\) be the standardized versions of \\(a_{1},\\ldots,a_{k}\\). (We assume the de-meaned vectors are nonzero.) Derive a matrix expression for \\(Z=[z_{1}\\;\\cdots\\;z_{k}]\\), the matrix of standardized vectors. Your expression should use \\(A\\), \\(\\mu\\), and the numbers \\(\\mathbf{std}(a_{1}),\\ldots,\\mathbf{std}(a_{k})\\).\n**10.17**: _Patients and symptoms._ Each of a set of \\(N\\) patients can exhibit any number of a set of \\(n\\) symptoms. We express this as an \\(N\\times n\\) matrix \\(S\\), with \\[S_{ij}=\\left\\{\\begin{array}{ll}1&\\text{patient $i$ exhibits symptom $j$}\\\\ 0&\\text{patient $i$ does not exhibit symptom $j$}.\\end{array}\\right.\\] Give simple English descriptions of the following expressions. Include the dimensions, and describe the entries.\n1. \\(S\\mathbf{1}\\).\n2. \\(S^{T}\\mathbf{1}\\).\n3. \\(S^{T}S\\).\n4. \\(SS^{T}\\).\n**10.18**: _Students, classes, and majors._ We consider \\(m\\) students, \\(n\\) classes, and \\(p\\) majors. Each student can be in any number of the classes (although we'd expect the number to range from 3 to 6), and can have any number of the majors (although the common values would be 0, 1, or 2). The data about the students' classes and majors are given by an \\(m\\times n\\) matrix \\(C\\) and an \\(m\\times p\\) matrix \\(M\\), where \\[C_{ij}=\\left\\{\\begin{array}{ll}1&\\text{student $i$ is in class $j$}\\\\ 0&\\text{student $i$ is not in class $j$},\\end{array}\\right.\\] and \\[M_{ij}=\\left\\{\\begin{array}{ll}1&\\text{student $i$ is in major $j$}\\\\ 0&\\text{student $i$ is not in major $j$}.\\end{array}\\right.\\] 1. Let \\(E\\) be the \\(n\\)-vector with \\(E_{i}\\) being the enrollment in class \\(i\\). Express \\(E\\) using matrix notation, in terms of the matrices \\(C\\) and \\(M\\). 2. Define the \\(n\\times p\\) matrix \\(S\\) where \\(S_{ij}\\) is the total number of students in class \\(i\\) with major \\(j\\). Express \\(S\\) using matrix notation, in terms of the matrices \\(C\\) and \\(M\\).\n**10.19**: _Student group membership._ Let \\(G\\in{\\bf R}^{m\\times n}\\) represent a contingency matrix of \\(m\\) students who are members of \\(n\\) groups:\n\\[G_{ij}=\\left\\{\\begin{array}{ll}1&\\mbox{student $i$ is in group $j$}\\\\ 0&\\mbox{student $i$ is not in group $j$.}\\end{array}\\right.\\]\n(A student can be in any number of the groups.)\n1. What is the meaning of the 3rd column of \\(G\\)?\n2. What is the meaning of the 15th row of \\(G\\)?\n3. Give a simple formula (using matrices, vectors, etc.) for the \\(n\\)-vector \\(M\\), where \\(M_{i}\\) is the total membership (_i.e._, number of students) in group \\(i\\).\n4. Interpret \\((GG^{T})_{ij}\\) in simple English.\n5. Interpret \\((G^{T}G)_{ij}\\) in simple English.\n\n\n",
        "answer": 26,
        "rationale": {
            "Question": "30 students from 5 classes solved 40 math problems. Each student must answer at least one question. Every two students in the same class solved the same number of questions. The number of questions answered by any two students in different classes is also different. Question: What's maximum possible number of students who only answered one question?",
            "Answer": 26,
            "Answer_type": "integer",
            "Picture": null,
            "source": "self",
            "id": "jianyu_xu/integer_programming_1.json",
            "explanation": "NONE",
            "theorem": "integer programming",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "mingyin/gaussian-elimination1.json",
        "question": "for the matrix $A=(\\begin{array}{rrrrr} 1 & 2 & 3 & 4 & -3 \\1 & 2 & 0 & -5 & 1 \\2 & 4 & -3 & -19 & 6 \\3 & 6 & -3 & -24 & 7\\end{array})$, what is its row rank and column rank? return the two numbers as a list.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nfor the matrix $A=(\\begin{array}{rrrrr} 1 & 2 & 3 & 4 & -3 \\1 & 2 & 0 & -5 & 1 \\2 & 4 & -3 & -19 & 6 \\3 & 6 & -3 & -24 & 7\\end{array})$, what is its row rank and column rank? return the two numbers as a list.\n\nDocument 1:\n----\n### 6.1 Matrices\nA _matrix_ is a rectangular array of numbers written between rectangular brackets, as in\n\\[\\left[\\begin{array}{cccc}0&1&-2.3&0.1\\\\ 1.3&4&-0.1&0\\\\ 4.1&-1&0&1.7\\end{array}\\right].\\]\nIt is also common to use large parentheses instead of rectangular brackets, as in\n\\[\\left(\\begin{array}{cccc}0&1&-2.3&0.1\\\\ 1.3&4&-0.1&0\\\\ 4.1&-1&0&1.7\\end{array}\\right).\\]\nAn important attribute of a matrix is its _size_ or _dimensions_, _i.e._, the numbers of rows and columns. The matrix above has 3 rows and 4 columns, so its size is \\(3\\times 4\\). A matrix of size \\(m\\times n\\) is called an \\(m\\times n\\) matrix.\nThe _elements_ (or _entries_ or _coefficients_) of a matrix are the values in the array. The \\(i,j\\) element is the value in the \\(i\\)th row and \\(j\\)th column, denoted by double subscripts: the \\(i,j\\) element of a matrix \\(A\\) is denoted \\(A_{ij}\\) (or \\(A_{i,j}\\), when \\(i\\) or \\(j\\) is more than one digit or character). The positive integers \\(i\\) and \\(j\\) are called the (row and column) _indices_. If \\(A\\) is an \\(m\\times n\\) matrix, then the row index \\(i\\) runs from 1 to \\(m\\) and the column index \\(j\\) runs from 1 to \\(n\\). Row indices go from top to bottom, so row 1 is the top row and row \\(m\\) is the bottom row. Column indices go from left to right, so column 1 is the left column and column \\(n\\) is the right column.\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\nTwo matrices are equal if they have the same size, and the corresponding entries are all equal. As with vectors, we normally deal with matrices with entries that\nare real numbers, which will be our assumption unless we state otherwise. The set of real \\(m\\times n\\) matrices is denoted \\(\\mathbf{R}^{m\\times n}\\). But matrices with complex entries, for example, do arise in some applications.\nMatrix indexing.As with vectors, standard mathematical notation indexes the rows and columns of a matrix starting from 1. In computer languages, matrices are often (but not always) stored as 2-dimensional arrays, which can be indexed in a variety of ways, depending on the language. Lower level languages typically use indices starting from 0; higher level languages and packages that support matrix operations usually use standard mathematical indexing, starting from 1.\nSquare, tall, and wide matrices.A _square_ matrix has an equal number of rows and columns. A square matrix of size \\(n\\times n\\) is said to be of _order_\\(n\\). A _tall_ matrix has more rows than columns (size \\(m\\times n\\) with \\(m>n\\)). A _wide_ matrix has more columns than rows (size \\(m\\times n\\) with \\(n>m\\)).\nColumn and row vectors.An \\(n\\)-vector can be interpreted as an \\(n\\times 1\\) matrix; we do not distinguish between vectors and matrices with one column. A matrix with only one row, _i.e._, with size \\(1\\times n\\), is called a _row vector_; to give its size, we can refer to it as an _\\(n\\)-row-vector_. As an example,\n\\[\\left[\\begin{array}{ccc}-2.1&-3&0\\end{array}\\right]\\]\nis a 3-row-vector (or \\(1\\times 3\\) matrix). To distinguish them from row vectors, vectors are sometimes called _column vectors_. A \\(1\\times 1\\) matrix is considered to be the same as a scalar.\n\n\n",
        "answer": [
            2,
            2
        ],
        "rationale": {
            "Question": "for the matrix $A=(\\begin{array}{rrrrr} 1 & 2 & 3 & 4 & -3 \\1 & 2 & 0 & -5 & 1 \\2 & 4 & -3 & -19 & 6 \\3 & 6 & -3 & -24 & 7\\end{array})$, what is its row rank and column rank? return the two numbers as a list.",
            "Answer": [
                2,
                2
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "linear algebra 2.2 example 3",
            "id": "mingyin/gaussian-elimination1.json",
            "explanation": "NONE",
            "theorem": "gaussian elimination",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "wenhuchen/gauss_lemma.json",
        "question": "Is 7 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs 7 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.\n\nDocument 1:\n----\nWe refer to the function \\(\\|Ax-b\\|^{2}\\) as the _objective_ of the problem, and the set of \\(p\\) linear equality constraints \\(Cx=d\\) as the _constraints_ of the problem. They can be written out as \\(p\\) scalar constraints (equations)\n\\[c_{i}^{T}x=d_{i},\\quad i=1,\\ldots,p,\\]\nwhere \\(c_{i}^{T}\\) is the \\(i\\)th row of \\(C\\).\nAn \\(n\\)-vector \\(x\\) is called _feasible_ (for the problem (16.1)) if it satisfies the constraints, _i.e._, \\(Cx=d\\). An \\(n\\)-vector \\(\\hat{x}\\) is called an _optimal point_ or _solution_ of the optimization problem (16.1) if it is feasible, and if \\(\\|A\\hat{x}-b\\|^{2}\\leq\\|Ax-b\\|^{2}\\) holds for any feasible \\(x\\). In other words, \\(\\hat{x}\\) solves the problem (16.1) if it is feasible and has the smallest possible value of the objective function among all feasible vectors.\nThe constrained least squares problem combines the problems of solving a set of linear equations (find \\(x\\) that satisfies \\(Cx=d\\)) with the least squares problem\n(find \\(x\\) that minimizes \\(\\|Ax-b\\|^{2}\\)). Indeed each of these problems can be considered a special case of the constrained least squares problem (16.1).\nThe constrained least squares problem can also be thought of as a limit of a bi-objective least squares problem, with primary objective \\(\\|Ax-b\\|^{2}\\) and secondary objective \\(\\|Cx-d\\|^{2}\\). Roughly speaking, we put infinite weight on the second objective, so that any nonzero value is unacceptable (which forces \\(x\\) to satisfy \\(Cx=d\\)). So we would expect (and it can be verified) that minimizing the weighted objective\n\\[\\|Ax-b\\|^{2}+\\lambda\\|Cx-d\\|^{2},\\]\nfor a very large value of \\(\\lambda\\) yields a vector close to a solution of the constrained least squares problem (16.1). We will encounter this idea again in chapter 19, when we consider the nonlinear constrained least squares problem.\nExample.In figure 16.1 we fit a _piecewise-polynomial_ function \\(\\hat{f}(x)\\) to a set of \\(N=140\\) points \\((x_{i},y_{i})\\) in the plane. The function \\(\\hat{f}(x)\\) is defined as\n\\[\\hat{f}(x)=\\left\\{\\begin{array}{ll}p(x)&x\\leq a\\\\ q(x)&x>a,\\end{array}\\right.\\]\nwith \\(a\\) given, and \\(p(x)\\) and \\(q(x)\\) polynomials of degree three or less,\n\\[p(x)=\\theta_{1}+\\theta_{2}x+\\theta_{3}x^{2}+\\theta_{4}x^{3},\\qquad q(x)=\\theta _{5}+\\theta_{6}x+\\theta_{7}x^{2}+\\theta_{8}x^{3}.\\]\nWe also impose the condition that \\(p(a)=q(a)\\) and \\(p^{\\prime}(a)=q^{\\prime}(a)\\), so that \\(\\hat{f}(x)\\) is continuous and has a continuous first derivative at \\(x=a\\). Suppose the \\(N\\) data\nFigure 16.1: Least squares fit of two cubic polynomials to 140 points, with continuity constraints \\(p(a)=q(a)\\) and \\(p^{\\prime}(a)=q^{\\prime}(a)\\).\npoints \\((x_{i},y_{i})\\) are numbered so that \\(x_{1},\\ldots,x_{M}\\leq a\\) and \\(x_{M+1},\\ldots,x_{N}>a\\). The sum of squares of the prediction errors is\n\\[\\sum_{i=1}^{M}(\\theta_{1}+\\theta_{2}x_{i}+\\theta_{3}x_{i}^{2}+\\theta_{4}x_{i}^{ 3}-y_{i})^{2}+\\sum_{i=M+1}^{N}(\\theta_{5}+\\theta_{6}x_{i}+\\theta_{7}x_{i}^{2}+ \\theta_{8}x_{i}^{3}-y_{i})^{2}.\\]\nThe conditions \\(p(a)-q(a)=0\\) and \\(p^{\\prime}(a)-q^{\\prime}(a)=0\\) are two linear equations\n\\[\\theta_{1}+\\theta_{2}a+\\theta_{3}a^{2}+\\theta_{4}a^{3}-\\theta_{5} -\\theta_{6}a-\\theta_{7}a^{2}-\\theta_{8}a^{3} = 0\\] \\[\\theta_{2}+2\\theta_{3}a+3\\theta_{4}a^{2}-\\theta_{6}-2\\theta_{7}a- 3\\theta_{8}a^{2} = 0.\\]\nWe can determine the coefficients \\(\\hat{\\theta}=(\\hat{\\theta}_{1},\\ldots,\\hat{\\theta}_{8})\\) that minimize the sum of squares of the prediction errors, subject to the continuity constraints, by solving a constrained least squares problem\n\\[\\begin{array}{ll}\\mbox{minimize}&\\|A\\theta-b\\|^{2}\\\\ \\mbox{subject to}&C\\theta=d.\\end{array}\\]\nThe matrices and vectors \\(A\\), \\(b\\), \\(C\\), \\(d\\) are defined as\n\\[A=\\left[\\begin{array}{cccccccc}1&x_{1}&x_{1}^{2}&x_{1}^{3}&0&0&0&0\\\\ 1&x_{2}&x_{2}^{2}&x_{2}^{3}&0&0&0&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\ 1&x_{M}&x_{M}^{2}&x_{M}^{3}&0&0&0&0\\\\ 0&0&0&0&1&x_{M+1}&x_{M+1}^{2}&x_{M+1}^{3}\\\\ 0&0&0&0&1&x_{M+2}&x_{M+2}^{2}&x_{M+2}^{3}\\\\ \\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&0&0&1&x_{N}&x_{N}^{2}&x_{N}^{3}\\end{array}\\right],\\qquad b=\\left[ \\begin{array}{c}y_{1}\\\\ y_{2}\\\\ \\vdots\\\\ y_{M}\\\\ y_{M+1}\\\\ y_{M+2}\\\\ \\vdots\\\\ y_{N}\\end{array}\\right],\\]\nand\n\\[C=\\left[\\begin{array}{cccccccc}1&a&a^{2}&a^{3}&-1&-a&-a^{2}&-a^{3}\\\\ 0&1&2a&3a^{2}&0&-1&-2a&-3a^{2}\\end{array}\\right],\\qquad d=\\left[\\begin{array} []{c}0\\\\ 0\\end{array}\\right].\\]\nThis method is easily extended to piecewise-polynomial functions with more than two intervals. Functions of this kind are called _splines_.\nAdvertising budget allocation.We continue the example described on page 234, where the goal is to purchase advertising in \\(n\\) different channels so as to achieve (or approximately achieve) a target set of customer views or impressions in \\(m\\) different demographic groups. We denote the \\(n\\)-vector of channel spending as \\(s\\); this spending results in a set of views (across the demographic groups) given by the \\(m\\)-vector \\(Rs\\). We will minimize the sum of squares of the deviation from the target set of views, given by \\(v^{\\rm des}\\). In addition, we fix our total advertising spending, with the constraint \\({\\bf 1}^{T}s=B\\), where \\(B\\) is a given total advertising budget. (This can also be described as _allocating_ a total budget \\(B\\) across the \\(n\\) different channels.) This leads to the constrained least squares problem\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Is 7 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | http://mathonline.wikidot.com/example-questions-regarding-gauss-s-lemma",
            "id": "wenhuchen/gauss_lemma.json",
            "explanation": "NONE",
            "theorem": "gauss's lemma",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "mingyin/Sylveete-rank-inequality1.json",
        "question": "Both A, B are n-by-n matrices with rank(A)=n, rank(A*B)=0. What is rank(B)?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nBoth A, B are n-by-n matrices with rank(A)=n, rank(A*B)=0. What is rank(B)?\n\nDocument 1:\n----\nTo compute \\(\\hat{x}\\) we first multiply \\(b\\) by \\(Q^{T}\\); then we compute \\(R^{-1}(Q^{T}b)\\) using back substitution. This is summarized in the following algorithm, which computes the least squares approximate solution \\(\\hat{x}\\), given \\(A\\) and \\(b\\).\nFigure 12.2 Illustration of orthogonality principle for a least squares problem of size \\(m=3\\), \\(n=2\\). The optimal residual \\(\\hat{r}\\) is orthogonal to any linear combination of \\(a_{1}\\) and \\(a_{2}\\), the two columns of \\(A\\).\n**Algorithm 12.1** Least squares via QR factorization\n**Algorithm 12.\nAnother simple approach for exploiting sparsity of \\(A\\) is to solve the normal equations \\(A^{T}A\\hat{x}=A^{T}b\\) by solving a larger (but sparse) system of equations,\n\\[\\left[\\begin{array}{cc}0&A^{T}\\\\ A&I\\end{array}\\right]\\left[\\begin{array}{c}\\hat{x}\\\\ \\hat{y}\\end{array}\\right]=\\left[\\begin{array}{c}0\\\\ b\\end{array}\\right].\\] (12.11)\nThis is a square set of \\(m+n\\) linear equations. Its coefficient matrix is sparse when \\(A\\) is sparse. If \\((\\hat{x},\\hat{y})\\) satisfies these equations, it is easy to see that \\(\\hat{x}\\) satisfies (12.11); conversely, if \\(\\hat{x}\\) satisfies the normal equations, \\((\\hat{x},\\hat{y})\\) satisfies (12.11) with \\(\\hat{y}=b-A\\hat{x}\\). Any method for solving a sparse system of linear equations can be used to solve (12.11).\nMatrix least squares.A simple extension of the least squares problem is to choose the \\(n\\times k\\)_matrix_\\(X\\) so as to minimize \\(\\|AX-B\\|^{2}\\). Here \\(A\\) is an \\(m\\times n\\) matrix and \\(B\\) is an \\(m\\times k\\) matrix, and the norm is the matrix norm. This is sometimes called the _matrix least squares problem_. When \\(k=1\\), \\(x\\) and \\(b\\) are vectors, and the matrix least squares problem reduces to the usual least squares problem.\nThe matrix least squares problem is in fact nothing but a set of \\(k\\) ordinary least squares problems. To see this, we note that\n\\[\\|AX-B\\|^{2}=\\|Ax_{1}-b_{1}\\|^{2}+\\cdots+\\|Ax_{k}-b_{k}\\|^{2},\\]\nwhere \\(x_{j}\\) is the \\(j\\)th column of \\(X\\) and \\(b_{j}\\) is the \\(j\\)th column of \\(B\\). (Here we use the property that the square of the matrix norm is the sum of the squared norms of the columns of the matrix.) So the objective is a sum of \\(k\\) terms, with each term depending on only one column of \\(X\\). It follows that we can choose the columns \\(x_{j}\\) independently, each one by minimizing its associated term \\(\\|Ax_{j}-b_{j}\\|^{2}\\). Assuming that \\(A\\) has linearly independent columns, the solution is \\(\\hat{x}_{j}=A^{\\dagger}b_{j}\\). The solution of the matrix least squares problem is therefore\n\\[\\hat{X} = \\left[\\begin{array}{cccc}\\hat{x}_{1}&\\cdots&\\hat{x}_{k}\\end{array}\\right]\\] (12.12) \\[= \\left[\\begin{array}{cccc}A^{\\dagger}b_{1}&\\cdots&A^{\\dagger}b_{ k}\\end{array}\\right]\\] \\[= A^{\\dagger}\\left[\\begin{array}{cccc}b_{1}&\\cdots&b_{k}\\end{array}\\right]\\] \\[= A^{\\dagger}B.\\]\nThe very simple solution \\(\\hat{X}=A^{\\dagger}B\\) of the matrix least squares problem agrees with the solution of the ordinary least squares problem when \\(k=1\\) (as it must). Many software packages for linear algebra use the backslash operator \\(A\\backslash B\\) to denote \\(A^{\\dagger}B\\), but this is not standard mathematical notation.\nThe matrix least squares problem can be solved efficiently by exploiting the fact that algorithm 12.1 is another example of a factor-solve algorithm. To compute \\(\\hat{X}=A^{\\dagger}B\\) we carry out the QR factorization of \\(A\\) once; we carry out steps 2 and 3 of algorithm 12.1 for each of the \\(k\\) columns of \\(B\\). The total cost is \\(2mn^{2}+k(2mn+n^{2})\\) flops. When \\(k\\) is small compared to \\(n\\) this is roughly \\(2mn^{2}\\) flops, the same cost as solving a single least squares problem (_i.e._, one with a vector right-hand side).\n\n\n",
        "answer": 0.0,
        "rationale": {
            "Question": "Both A, B are n-by-n matrices with rank(A)=n, rank(A*B)=0. What is rank(B)?",
            "Answer": 0.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 4.6 example 6",
            "id": "mingyin/Sylveete-rank-inequality1.json",
            "explanation": "NONE",
            "theorem": "sylveeter rank inequality",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_7.json",
        "question": "Let A be an invertible n * n matrix and v and eigenvector of both A and B, is v necesarily an eigenvector of A + B?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet A be an invertible n * n matrix and v and eigenvector of both A and B, is v necesarily an eigenvector of A + B?\n\nDocument 1:\n----\nIf a matrix is left- _and_ right-invertible, then the left and right inverses are unique and equal. To see this, suppose that \\(AX=I\\) and \\(YA=I\\), _i.e._, \\(X\\) is any right inverse and \\(Y\\) is any left inverse of \\(A\\). Then we have\n\\[X=(YA)X=Y(AX)=Y,\\]\n_i.e._, any left inverse of \\(A\\) is equal to any right inverse of \\(A\\). This implies that the left inverse is unique: If we have \\(A\\tilde{X}=I\\), then the argument above tells us that \\(\\tilde{X}=Y\\), so we have \\(\\tilde{X}=X\\), _i.e._, there is only one right inverse of \\(A\\). A similar argument shows that \\(Y\\) (which is the same as \\(X\\)) is the only left inverse of \\(A\\).\nWhen a matrix \\(A\\) has both a left inverse \\(Y\\) and a right inverse \\(X\\), we call the matrix \\(X=Y\\) simply the _inverse_ of \\(A\\), and denote it as \\(A^{-1}\\). We say that \\(A\\) is _invertible_ or _nonsingular_. A square matrix that is not invertible is called _singular_.\nDimensions of invertible matrices.Invertible matrices must be square, since tall matrices are not right-invertible, while wide matrices are not left-invertible. A matrix \\(A\\) and its inverse (if it exists) satisfy\n\\[AA^{-1}=A^{-1}A=I.\\]\nIf \\(A\\) has inverse \\(A^{-1}\\), then the inverse of \\(A^{-1}\\) is \\(A\\); in other words, we have \\((A^{-1})^{-1}=A\\). For this reason we say that \\(A\\) and \\(A^{-1}\\) are inverses (of each other).\nSolving linear equations with the inverse.Consider the square system of \\(n\\) linear equations with \\(n\\) variables, \\(Ax=b\\). If \\(A\\) is invertible, then for any \\(n\\)-vector \\(b\\),\n\\[x=A^{-1}b\\] (11.1)\nis a solution of the equations. (This follows since \\(A^{-1}\\) is a right inverse of \\(A\\).) Moreover, it is the _only_ solution of \\(Ax=b\\). (This follows since \\(A^{-1}\\) is a left inverse of \\(A\\).) We summarize this very important result as\n_The square system of linear equations \\(Ax=b\\), with \\(A\\) invertible, has the unique solution \\(x=A^{-1}b\\), for any \\(n\\)-vector \\(b\\)._\nOne immediate conclusion we can draw from the formula (11.1) is that the solution of a square set of linear equations is a linear function of the right-hand side vector \\(b\\).\nInvertibility conditions.For square matrices, left-invertibility, right-invertibility, and invertibility are equivalent: If a matrix is square and left-invertible, then it is also right-invertible (and therefore invertible) and vice-versa.\nTo see this, suppose \\(A\\) is an \\(n\\times n\\) matrix and left-invertible. This implies that the \\(n\\) columns of \\(A\\) are linearly independent. Therefore they form a basis and so any \\(n\\)-vector can be expressed as a linear combination of the columns of \\(A\\). In particular, each of the \\(n\\) unit vectors \\(e_{i}\\) can be expressed as \\(e_{i}=Ab_{i}\\) for some \\(n\\)-vector \\(b_{i}\\). The matrix \\(B=\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\) satisfies\n\\[AB=\\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right]= \\left[\\begin{array}{cccc}e_{1}&e_{2}&\\cdots&e_{n}\\end{array}\\right]=I.\\]\nSo \\(B\\) is a right inverse of \\(A\\).\nWe have just shown that for a square matrix \\(A\\),\n\\[\\mbox{left-invertibility}\\quad\\Longrightarrow\\quad\\mbox{column independence}\\quad \\Longrightarrow\\quad\\mbox{right-invertibility}.\\]\n(The symbol \\(\\Longrightarrow\\) means that the left-hand condition implies the right-hand condition.) Applying the same result to the transpose of \\(A\\) allows us to also conclude that\n\\[\\mbox{right-invertibility}\\quad\\Longrightarrow\\quad\\mbox{row independence}\\quad \\Longrightarrow\\quad\\mbox{left-invertibility}.\\]\nSo all six of these conditions are equivalent; if any one of them holds, so do the other five.\nIn summary, for a square matrix \\(A\\), the following are equivalent.\n* \\(A\\) is invertible.\n* The columns of \\(A\\) are linearly independent.\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Let A be an invertible n * n matrix and v and eigenvector of both A and B, is v necesarily an eigenvector of A + B?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "wenhuchen/vieta's_formula2.json",
        "question": "The polynomial $x^3 - Ax + 15$ has three real roots. Two of these roots sum to 5. What is |A|?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nThe polynomial $x^3 - Ax + 15$ has three real roots. Two of these roots sum to 5. What is |A|?\n\nDocument 1:\n----\n### 11.4 Examples\nPolynomial interpolation.The 4-vector \\(c\\) gives the coefficients of a cubic polynomial,\n\\[p(x)=c_{1}+c_{2}x+c_{3}x^{2}+c_{4}x^{3}\\]\n(see pages 154 and 120). We seek the coefficients that satisfy\n\\[p(-1.1)=b_{1},\\qquad p(-0.4)=b_{2},\\qquad p(0.2)=b_{3},\\qquad p(0.8)=b_{4}.\\]\nWe can express this as the system of 4 equations in 4 variables \\(Ac=b\\), where\n\\[A=\\left[\\begin{array}{rrrr}1&-1.1&(-1.1)^{2}&(-1.1)^{3}\\\\ 1&-0.4&(-0.4)^{2}&(-0.4)^{3}\\\\ 1&0.2&(0.2)^{2}&(0.2)^{3}\\\\ 1&0.8&(0.8)^{2}&(0.8)^{3}\\end{array}\\right],\\]\nwhich is a specific Vandermonde matrix (see (6.7)). The unique solution is \\(c=A^{-1}b\\), where\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\nBalancing chemical reactions.(See page 154 for background.) We consider the problem of balancing the chemical reaction\n\\[a_{1}\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+a_{2}\\mathrm{Fe}^{2+}+a_{3}\\mathrm{H} ^{+}\\longrightarrow b_{1}\\mathrm{Cr}^{3+}+b_{2}\\mathrm{Fe}^{3+}+b_{3} \\mathrm{H}_{2}\\mathrm{O},\\]\nwhere the superscript gives the charge of each reactant and product. There are 4 atoms (Cr, O, Fe, H) and charge to balance. The reactant and product matrices are (using the order just listed)\n\\[R=\\left[\\begin{array}{ccc}2&0&0\\\\ 7&0&0\\\\ 0&1&0\\\\ 0&0&1\\\\ -2&2&1\\end{array}\\right],\\qquad P=\\left[\\begin{array}{ccc}1&0&0\\\\ 0&0&1\\\\ 0&1&0\\\\ 0&0&2\\\\ 3&3&0\\end{array}\\right].\\]\nFigure 11.1 Cubic interpolants through two sets of points, shown as circles and squares.\n## Chapter 11 Matrix inverses\n### 11.1 Matrix inverses\nThe matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.1)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.2)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.3)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.4)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.5)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.6)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.7)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.8)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\n\n",
        "answer": 22.0,
        "rationale": {
            "Question": "The polynomial $x^3 - Ax + 15$ has three real roots. Two of these roots sum to 5. What is |A|?",
            "Answer": 22.0,
            "Answer_type": "float",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula2.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "mingyin/linear-dependence2.json",
        "question": "suppose a,b,c,\\alpha,\\beta,\\gamma are six real numbers with a^2+b^2+c^2>0.  In addition, $a=b*cos(\\gamma)+c*cos(\\beta), b=c*cos(\\alpha)+a*cos(\\gamma), c=a*cos(\\beta)+b*cos(\\alpha)$. What is the value of $cos^2(\\alpha)+cos^2(\\beta)+cos^2(\\gamma)+2*cos(\\alpha)*cos(\\beta)*cos(\\gamma)? return the numeric.",
        "prediction": 3,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nsuppose a,b,c,\\alpha,\\beta,\\gamma are six real numbers with a^2+b^2+c^2>0.  In addition, $a=b*cos(\\gamma)+c*cos(\\beta), b=c*cos(\\alpha)+a*cos(\\gamma), c=a*cos(\\beta)+b*cos(\\alpha)$. What is the value of $cos^2(\\alpha)+cos^2(\\beta)+cos^2(\\gamma)+2*cos(\\alpha)*cos(\\beta)*cos(\\gamma)? return the numeric.\n\nDocument 1:\n----\nThe (standard) _inner product_ (also called _dot product_) of two \\(n\\)-vectors is defined as the scalar\n\\[a^{T}b=a_{1}b_{1}+a_{2}b_{2}+\\cdots+a_{n}b_{n},\\]\nthe sum of the products of corresponding entries. (The origin of the superscript 'T' in the inner product notation \\(a^{T}b\\) will be explained in chapter 6.) Some other notations for the inner product (that we will not use in this book) are \\(\\langle a,b\\rangle\\), \\(\\langle a|b\\rangle\\), \\((a,b)\\), and \\(a\\cdot b\\). (In the notation used in this book, \\((a,b)\\) denotes a stacked vector of length \\(2n\\).) As you might guess, there is also a vector _outer product_, which we will encounter later, in SS10.1. As a specific example of the inner product, we have\n\\[\\left[\\begin{array}{c}-1\\\\ 2\\\\ 2\\end{array}\\right]^{T}\\left[\\begin{array}{c}1\\\\ 0\\\\ -3\\end{array}\\right]=(-1)(1)+(2)(0)+(2)(-3)=-7.\\]\nWhen \\(n=1\\), the inner product reduces to the usual product of two numbers.\nProperties.The inner product satisfies some simple properties that are easily verified from the definition. If \\(a\\), \\(b\\), and \\(c\\) are vectors of the same size, and \\(\\gamma\\) is a scalar, we have the following.\n* _Commutativity_. \\(a^{T}b=b^{T}a\\). The order of the two vector arguments in the inner product does not matter.\n* _Associativity with scalar multiplication_. \\((\\gamma a)^{T}b=\\gamma(a^{T}b)\\), so we can write both as \\(\\gamma a^{T}b\\).\n* _Distributivity with vector addition_. \\((a+b)^{T}c=a^{T}c+b^{T}c\\). The inner product can be distributed across vector addition.\nThese can be combined to obtain other identities, such as \\(a^{T}(\\gamma b)=\\gamma(a^{T}b)\\), or \\(a^{T}(b+\\gamma c)=a^{T}b+\\gamma a^{T}c\\). As another useful example, we have, for any vectors \\(a,b,c,d\\) of the same size,\n\\[(a+b)^{T}(c+d)=a^{T}c+a^{T}d+b^{T}c+b^{T}d.\\]\nFigure 1.12: The affine combination \\((1-\\theta)a+\\theta b\\) for different values of \\(\\theta\\). These points are on the line passing through \\(a\\) and \\(b\\); for \\(\\theta\\) between \\(0\\) and \\(1\\), the points are on the line segment between \\(a\\) and \\(b\\).\nThis formula expresses an inner product on the left-hand side as a sum of four inner products on the right-hand side, and is analogous to expanding a product of sums in algebra. Note that on the left-hand side, the two addition symbols refer to vector addition, whereas on the right-hand side, the three addition symbols refer to scalar (number) addition.\nGeneral examples.* _Unit vector._\\(e_{i}^{T}a=a_{i}\\). The inner product of a vector with the \\(i\\)th standard unit vector gives (or 'picks out') the \\(i\\)th element \\(a\\).\n* _Sum._\\(\\mathbf{1}^{T}a=a_{1}+\\cdots+a_{n}\\). The inner product of a vector with the vector of ones gives the sum of the elements of the vector.\n* _Average._\\((\\mathbf{1}/n)^{T}a=(a_{1}+\\cdots+a_{n})/n\\). The inner product of an \\(n\\)-vector with the vector \\(\\mathbf{1}/n\\) gives the average or mean of the elements of the vector. The average of the entries of a vector is denoted by \\(\\mathbf{avg}(x)\\). The Greek letter \\(\\mu\\) is a traditional symbol used to denote the average or mean.\n* _Sum of squares._\\(a^{T}a=a_{1}^{2}+\\cdots+a_{n}^{2}\\). The inner product of a vector with itself gives the sum of the squares of the elements of the vector.\n* _Selective sum._ Let \\(b\\) be a vector all of whose entries are either \\(0\\) or \\(1\\). Then \\(b^{T}a\\) is the sum of the elements in \\(a\\) for which \\(b_{i}=1\\).\nBlock vectors.If the vectors \\(a\\) and \\(b\\) are block vectors, and the corresponding blocks have the same sizes (in which case we say they _conform_), then we have\n\n\n",
        "answer": 1.0,
        "rationale": {
            "Question": "suppose a,b,c,\\alpha,\\beta,\\gamma are six real numbers with a^2+b^2+c^2>0.  In addition, $a=b*cos(\\gamma)+c*cos(\\beta), b=c*cos(\\alpha)+a*cos(\\gamma), c=a*cos(\\beta)+b*cos(\\alpha)$. What is the value of $cos^2(\\alpha)+cos^2(\\beta)+cos^2(\\gamma)+2*cos(\\alpha)*cos(\\beta)*cos(\\gamma)? return the numeric.",
            "Answer": 1.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 3.5 example 7",
            "id": "mingyin/linear-dependence2.json",
            "explanation": "NONE",
            "theorem": "linear dependence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "wenhuchen/determinant2.json",
        "question": "For matrix A = [[2, 4, 3], [3, 3, 1], [42, 20, 51]], what is its determinant?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[2, 4, 3], [3, 3, 1], [42, 20, 51]], what is its determinant?\n\nDocument 1:\n----\nHere \\(R_{1},\\ldots,R_{p}\\) are the reactants, \\(P_{1},\\ldots,P_{q}\\) are the products, and the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\) are positive numbers that tell us how many of each of these molecules is involved in the reaction. They are typically integers, but can be scaled arbitrarily; we could double all of these numbers, for example, and we still have the same reaction. As a simple example, we have the electrolysis of water,\n\\[2\\mathrm{H}_{2}\\mathrm{O}\\longrightarrow 2\\mathrm{H}_{2}+\\mathrm{O}_{2},\\]\nwhich has one reactant, water (\\(\\mathrm{H}_{2}\\mathrm{O}\\)), and two products, molecular hydrogen (\\(\\mathrm{H}_{2}\\)) and molecular oxygen (\\(\\mathrm{O}_{2}\\)). The coefficients tell us that 2 water molecules create 2 hydrogen molecules and 1 oxygen molecule. The coefficients in a reaction can be multiplied by any nonzero numbers; for example, we could write the reaction above as \\(3\\mathrm{H}_{2}\\mathrm{O}\\longrightarrow 3\\mathrm{H}_{2}+(3/2)\\mathrm{O}_{2}\\). By convention reactions are written with all coefficients integers, with least common divisor one.\nIn a chemical reaction the numbers of constituent atoms must balance. This means that for each atom appearing in any of the reactants or products, the total amount on the left-hand side must equal the total amount on the right-hand side. (If any of the reactants or products is charged, _i.e._, an ion, then the total charge must also balance.) In the simple water electrolysis reaction above, for example, we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms). The oxygen atoms also balance, so this reaction is balanced.\nBalancing a chemical reaction with specified reactants and products, _i.e._, finding the numbers \\(a_{1},\\ldots,a_{p}\\) and \\(b_{1},\\ldots,b_{q}\\), can be expressed as a system of linear equations. We can express the requirement that the reaction balances as a set of\n\\(m\\) equations, where \\(m\\) is the number of different atoms appearing in the chemical reaction. We define the \\(m\\times p\\) matrix \\(R\\) by\n\\[R_{ij}=\\mbox{number of atoms of type $i$ in $R_{j}$},\\quad i=1,\\ldots,m,\\quad j=1, \\ldots,p.\\]\n(The entries of \\(R\\) are nonnegative integers.) The matrix \\(R\\) is interesting; for example, its \\(j\\)th column gives the chemical formula for reactant \\(R_{j}\\). We let \\(a\\) denote the \\(p\\)-vector with entries \\(a_{1},\\ldots,a_{p}\\). Then, the \\(m\\)-vector \\(Ra\\) gives the total number of atoms of each type appearing in the reactants. We define an \\(m\\times q\\) matrix \\(P\\) in a similar way, so the \\(m\\)-vector \\(Pb\\) gives the total number of atoms of each type that appears in the products.\nWe write the balance condition using vectors and matrices as \\(Ra=Pb\\). We can express this as\n\\[\\left[\\begin{array}{cc}R&-P\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=0,\\]\nwhich is a set of \\(m\\) homogeneous linear equations.\nA simple solution of these equations is \\(a=0\\), \\(b=0\\). But we seek a nonzero solution. We can set one of the coefficients, say \\(a_{1}\\), to be one. (This might cause the other quantities to be fractional-valued.) We can add the condition that \\(a_{1}=1\\) to our system of linear equations as\n\\[\\left[\\begin{array}{cc}R&-P\\\\ e_{1}^{T}&0\\end{array}\\right]\\left[\\begin{array}{c}a\\\\ b\\end{array}\\right]=e_{m+1}.\\]\nFinally, we have a set of \\(m+1\\) equations in \\(p+q\\) variables that expresses the requirement that the chemical reaction balances. Finding a solution of this set of equations is called _balancing_ the chemical reaction.\n\n\n",
        "answer": -376,
        "rationale": {
            "Question": "For matrix A = [[2, 4, 3], [3, 3, 1], [42, 20, 51]], what is its determinant?",
            "Answer": -376,
            "Picture": null,
            "Answer_type": "integer",
            "source": "self",
            "id": "wenhuchen/determinant2.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "wenhuchen/vieta's_formula3.json",
        "question": "Let $x_1$ and $x_2$ be the roots of the equation $x^2 + 3x + 1 =0$. Compute $(x_1/(x_2 + 1))^2 + (x_2 / (x_1 + 1))^2$.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet $x_1$ and $x_2$ be the roots of the equation $x^2 + 3x + 1 =0$. Compute $(x_1/(x_2 + 1))^2 + (x_2 / (x_1 + 1))^2$.\n\nDocument 1:\n----\nFollowing standard mathematical language usage, we will say \"The vectors \\(a_{1},\\ldots,a_{k}\\) are linearly dependent\" to mean \"The list of vectors \\(a_{1},\\ldots,a_{k}\\) is linearly dependent\". But it must be remembered that linear dependence is an attribute of a _collection_ of vectors, and not individual vectors.\nLinearly independent vectors.A collection of \\(n\\)-vectors \\(a_{1},\\ldots,a_{k}\\) (with \\(k\\geq 1\\)) is called _linearly independent_ if it is not linearly dependent, which means that\n\\[\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}=0\\] (5.1)\nonly holds for \\(\\beta_{1}=\\cdots=\\beta_{k}=0\\). In other words, the only linear combination of the vectors that equals the zero vector is the linear combination with all coefficients zero.\nAs with linear dependence, we will say \"The vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent\" to mean \"The list of vectors \\(a_{1},\\ldots,a_{k}\\) is linearly independent\". But, like linear dependence, linear independence is an attribute of a collection of vectors, and not individual vectors.\nIt is generally not easy to determine by casual inspection whether or not a list of vectors is linearly dependent or linearly independent. But we will soon see an algorithm that does this.\n#### Examples.\n* A list consisting of a single vector is linearly dependent only if the vector is zero. It is linearly independent only if the vector is nonzero.\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\\[x=\\gamma_{1}a_{1}+\\cdots+\\gamma_{k}a_{k},\\]\nthen \\(\\beta_{i}=\\gamma_{i}\\) for \\(i=1,\\ldots,k\\). This tells us that, in principle at least, we can find the coefficients that form a vector \\(x\\) as a linear combination of linearly independent vectors.\nTo see this, we subtract the two equations above to get\n\\[0=(\\beta_{1}-\\gamma_{1})a_{1}+\\cdots+(\\beta_{k}-\\gamma_{k})a_{k}.\\]\nSince \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we conclude that \\(\\beta_{i}-\\gamma_{i}\\) are all zero.\nThe converse is also true: If each linear combination of a list of vectors can only be expressed as a linear combination with one set of coefficients, then the list of vectors is linearly independent. This gives a nice interpretation of linear independence: A list of vectors is linearly independent if and only if for any linear combination of them, we can infer or deduce the associated coefficients. (We will see later how to do this.)\n\n\n",
        "answer": 18.0,
        "rationale": {
            "Question": "Let $x_1$ and $x_2$ be the roots of the equation $x^2 + 3x + 1 =0$. Compute $(x_1/(x_2 + 1))^2 + (x_2 / (x_1 + 1))^2$.",
            "Answer": 18.0,
            "Answer_type": "float",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula3.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_3_3.json",
        "question": "What is the vector that spans the kernel of A = [[1, 0, 2, 4], [0, 1, -3, -1], [3, 4, -6, 8], [0, -1, 3, 4]]?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the vector that spans the kernel of A = [[1, 0, 2, 4], [0, 1, -3, -1], [3, 4, -6, 8], [0, -1, 3, 4]]?\n\nDocument 1:\n----\n* A diagonal matrix \\(A\\) is invertible if and only if its diagonal entries are nonzero. The inverse of an \\(n\\times n\\) diagonal matrix \\(A\\) with nonzero diagonal entries is \\[A^{-1}=\\left[\\begin{array}{cccc}1/A_{11}&0&\\cdots&0\\\\ 0&1/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&1/A_{nn}\\end{array}\\right],\\] since \\[AA^{-1}=\\left[\\begin{array}{cccc}A_{11}/A_{11}&0&\\cdots&0\\\\ 0&A_{22}/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&A_{nn}/A_{nn}\\end{array}\\right]=I.\\] In compact notation, we have \\[\\mathbf{diag}(A_{11},\\ldots,A_{nn})^{-1}=\\mathbf{diag}(A_{11}^{-1},\\ldots,A_{ nn}^{-1}).\\] Note that the inverse on the left-hand side of this equation is the matrix inverse, while the inverses appearing on the right-hand side are scalar inverses.\n* As a non-obvious example, the matrix \\[A=\\left[\\begin{array}{cccc}1&-2&3\\\\ 0&2&2\\\\ -3&-4&-4\\end{array}\\right]\\] is invertible, with inverse \\[A^{-1}=\\frac{1}{30}\\left[\\begin{array}{cccc}0&-20&-10\\\\ -6&5&-2\\\\ 6&10&2\\end{array}\\right].\\] This can be verified by checking that \\(AA^{-1}=I\\) (or that \\(A^{-1}A=I\\), since either of these implies the other).\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\nInverse of matrix transpose.If \\(A\\) is invertible, its transpose \\(A^{T}\\) is also invertible and its inverse is \\((A^{-1})^{T}\\):\n\\[(A^{T})^{-1}=(A^{-1})^{T}.\\]\nSince the order of the transpose and inverse operations does not matter, this matrix is sometimes written as \\(A^{-T}\\).\nInverse of matrix product.If \\(A\\) and \\(B\\) are invertible (hence, square) and of the same size, then \\(AB\\) is invertible, and\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\nNow suppose that \\(x\\) is any \\(n\\)-vector. It can be expressed as a linear combination of the basis vectors \\(a_{1},\\ldots,a_{n}\\):\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{n}a_{n}.\\]\nThe dual basis gives us a simple way to find the coefficients \\(\\beta_{1},\\ldots,\\beta_{n}\\).\nWe start with \\(AB=I\\), and multiply by \\(x\\) to get\n\\[x=ABx=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right]\\left[ \\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right]x=(b_{1}^{T}x)a_{1}+\\cdots+(b_{n}^{T}x)a_{n}.\\]\nThis means (since the vectors \\(a_{1},\\ldots,a_{n}\\) are linearly independent) that \\(\\beta_{i}=b_{i}^{T}x\\). In words: The coefficients in the expansion of a vector in a basis are given by the inner products with the dual basis vectors. Using matrix notation, we can say that \\(\\beta=B^{T}x=(A^{-1})^{T}x\\) is the vector of coefficients of \\(x\\) in the basis given by the columns of \\(A\\).\nAs a simple numerical example, consider the basis\n\\[a_{1}=(1,1),\\qquad a_{2}=(1,-1).\\]\nThe dual basis consists of the rows of \\([\\begin{array}{cc}a_{1}&a_{2}\\end{array}]^{-1}\\), which are\n\\[b_{1}^{T}=\\left[\\begin{array}{cc}1/2&1/2\\end{array}\\right],\\qquad b_{2}^{T }=\\left[\\begin{array}{cc}1/2&-1/2\\end{array}\\right].\\]\nTo express the vector \\(x=(-5,1)\\) as a linear combination of \\(a_{1}\\) and \\(a_{2}\\), we have\n\\[x=(b_{1}^{T}x)a_{1}+(b_{2}^{T}x)a_{2}=(-2)a_{1}+(-3)a_{2},\\]\nwhich can be directly verified.\nNegative matrix powers.We can now give a meaning to matrix powers with negative integer exponents. Suppose \\(A\\) is a square invertible matrix and \\(k\\) is a positive integer. Then by repeatedly applying property (11.2), we get\n\n\n",
        "answer": [
            -2,
            3,
            1,
            0
        ],
        "rationale": {
            "Question": "What is the vector that spans the kernel of A = [[1, 0, 2, 4], [0, 1, -3, -1], [3, 4, -6, 8], [0, -1, 3, 4]]?",
            "Answer": [
                -2,
                3,
                1,
                0
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_3.json",
            "explanation": "NONE",
            "theorem": "kernel of linear transformations",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "mingyin/gaussian-elimination2.json",
        "question": "V is a vector space over the real field R. It is known that the vector group u_1, u_2, u_3 in V are linearly independent. Finding the rank of vector group ${u_1-\\lambda u_2, u_2-\\lambda u_3, u_3-\\lambda u_1}$ for $\\lambda=\\sqrt{5}$ and $\\lambda=1$ separately. Return the answer as a list.",
        "prediction": [
            3
        ],
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nV is a vector space over the real field R. It is known that the vector group u_1, u_2, u_3 in V are linearly independent. Finding the rank of vector group ${u_1-\\lambda u_2, u_2-\\lambda u_3, u_3-\\lambda u_1}$ for $\\lambda=\\sqrt{5}$ and $\\lambda=1$ separately. Return the answer as a list.\n\nDocument 1:\n----\n\\[\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}=0\\] (5.1)\nonly holds for \\(\\beta_{1}=\\cdots=\\beta_{k}=0\\). In other words, the only linear combination of the vectors that equals the zero vector is the linear combination with all coefficients zero.\nAs with linear dependence, we will say \"The vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent\" to mean \"The list of vectors \\(a_{1},\\ldots,a_{k}\\) is linearly independent\". But, like linear dependence, linear independence is an attribute of a collection of vectors, and not individual vectors.\nIt is generally not easy to determine by casual inspection whether or not a list of vectors is linearly dependent or linearly independent. But we will soon see an algorithm that does this.\n#### Examples.\n* A list consisting of a single vector is linearly dependent only if the vector is zero. It is linearly independent only if the vector is nonzero.\n* Any list of vectors containing the zero vector is linearly dependent.\n* A list of two vectors is linearly dependent if and only if one of the vectors is a multiple of the other one. More generally, a list of vectors is linearly dependent if any one of the vectors is a multiple of another one.\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}0.2\\\\ -7.0\\\\ 8.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.1\\\\ 2.0\\\\ -1.0\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}0.0\\\\ -1.0\\\\ 2.2\\end{array}\\right]\\] are linearly dependent, since \\(a_{1}+2a_{2}-3a_{3}=0\\). We can express any of these vectors as a linear combination of the other two. For example, we have \\(a_{2}=(-1/2)a_{1}+(3/2)a_{3}\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1\\\\ 0\\\\ 0\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}0\\\\ -1\\\\ 1\\end{array}\\right],\\qquad a_{3}=\\left[\\begin{array}{c}-1\\\\ 1\\\\ 1\\end{array}\\right]\\] are linearly independent. To see this, suppose \\(\\beta_{1}a_{1}+\\beta_{2}a_{2}+\\beta_{3}a_{3}=0\\). This means that \\[\\beta_{1}-\\beta_{3}=0,\\qquad-\\beta_{2}+\\beta_{3}=0,\\qquad\\beta_{2}+\\beta_{3}=0.\\] Adding the last two equations we find that \\(2\\beta_{3}=-0\\), so \\(\\beta_{3}=0\\). Using this, the first equation is then \\(\\beta_{1}=0\\), and the second equation is \\(\\beta_{2}=0\\).\n* The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are linearly independent. To see this, suppose that (5.1) holds. We have \\[0=\\beta_{1}e_{1}+\\cdots+\\beta_{n}e_{n}=\\left[\\begin{array}{c}\\beta_{1}\\\\ \\vdots\\\\ \\beta_{n}\\end{array}\\right],\\] so we conclude that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\).\nLinear combinations of linearly independent vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\),\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nWhen the vectors \\(a_{1},\\ldots,a_{k}\\) are linearly independent, the coefficients that form \\(x\\) are _unique_: If we also have\n\\[x=\\gamma_{1}a_{1}+\\cdots+\\gamma_{k}a_{k},\\]\nthen \\(\\beta_{i}=\\gamma_{i}\\) for \\(i=1,\\ldots,k\\). This tells us that, in principle at least, we can find the coefficients that form a vector \\(x\\) as a linear combination of linearly independent vectors.\nTo see this, we subtract the two equations above to get\n\\[0=(\\beta_{1}-\\gamma_{1})a_{1}+\\cdots+(\\beta_{k}-\\gamma_{k})a_{k}.\\]\nSince \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we conclude that \\(\\beta_{i}-\\gamma_{i}\\) are all zero.\nThe converse is also true: If each linear combination of a list of vectors can only be expressed as a linear combination with one set of coefficients, then the list of vectors is linearly independent. This gives a nice interpretation of linear independence: A list of vectors is linearly independent if and only if for any linear combination of them, we can infer or deduce the associated coefficients. (We will see later how to do this.)\nSupersets and subsets.If a collection of vectors is linearly dependent, then any superset of it is linearly dependent. In other words: If we add vectors to a linearly dependent collection of vectors, the new collection is also linearly dependent. Any nonempty subset of a linearly independent collection of vectors is linearly independent. In other words: Removing vectors from a collection of vectors preserves linear independence.\n\n\n",
        "answer": [
            3,
            2
        ],
        "rationale": {
            "Question": "V is a vector space over the real field R. It is known that the vector group u_1, u_2, u_3 in V are linearly independent. Finding the rank of vector group ${u_1-\\lambda u_2, u_2-\\lambda u_3, u_3-\\lambda u_1}$ for $\\lambda=\\sqrt{5}$ and $\\lambda=1$ separately. Return the answer as a list.",
            "Answer": [
                3,
                2
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "linear algebra 2.6 example 1(2)",
            "id": "mingyin/gaussian-elimination2.json",
            "explanation": "NONE",
            "theorem": "gaussian elimination",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_7_2.json",
        "question": "Is there an eigenbasis for the identity matrix I_n?",
        "prediction": true,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs there an eigenbasis for the identity matrix I_n?\n\nDocument 1:\n----\nA collection of vectors \\(a_{1},\\ldots,a_{k}\\) is _orthogonal_ or _mutually orthogonal_ if \\(a_{i}\\perp a_{j}\\) for any \\(i\\), \\(j\\) with \\(i\\neq j\\), \\(i,j=1,\\ldots,k\\). A collection of vectors \\(a_{1},\\ldots,a_{k}\\) is _orthonormal_ if it is orthogonal and \\(\\|a_{i}\\|=1\\) for \\(i=1,\\ldots,k\\). (A vector of norm one is called _normalized_; dividing a vector by its norm is called _normalizing_ it.) Thus, each vector in an orthonormal collection of vectors is normalized, and two different vectors from the collection are orthogonal. These two conditions can be combined into one statement about the inner products of pairs of vectors in the collection: \\(a_{1},\\ldots,a_{k}\\) is orthonormal means that\n\\[a_{i}^{T}a_{j}=\\left\\{\\begin{array}{ll}1&i=j\\\\ 0&i\\neq j.\\end{array}\\right.\\]\nOrthonormality, like linear dependence and independence, is an attribute of a collection of vectors, and not an attribute of vectors individually. By convention, though, we say \"The vectors \\(a_{1},\\ldots,a_{k}\\) are orthonormal\" to mean \"The collection of vectors \\(a_{1},\\ldots,a_{k}\\) is orthonormal\".\nExamples.The standard unit \\(n\\)-vectors \\(e_{1},\\ldots,e_{n}\\) are orthonormal. As another example, the 3-vectors\n\\[\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right],\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ 1\\\\ 0\\end{array}\\right],\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right],\\] (5.3)\nare orthonormal. Figure 5.2 shows a set of two orthonormal 2-vectors.\nLinear independence of orthonormal vectors.Orthonormal vectors are linearly independent. To see this, suppose \\(a_{1},\\ldots,a_{k}\\) are orthonormal, and\n\\[\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}=0.\\]\nFigure 5.2: Orthonormal vectors in a plane.\nTaking the inner product of this equality with \\(a_{i}\\) yields\n\\[0 = a_{i}^{T}(\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k})\\] \\[= \\beta_{1}(a_{i}^{T}a_{1})+\\cdots+\\beta_{k}(a_{i}^{T}a_{k})\\] \\[= \\beta_{i},\\]\nsince \\(a_{i}^{T}a_{j}=0\\) for \\(j\\neq i\\) and \\(a_{i}^{T}a_{i}=1\\). Thus, the only linear combination of \\(a_{1},\\ldots,a_{k}\\) that is zero is the one with all coefficients zero.\nLinear combinations of orthonormal vectors.Suppose a vector \\(x\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\), where \\(a_{1},\\ldots,a_{k}\\) are orthonormal,\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}.\\]\nTaking the inner product of the left-hand and right-hand sides of this equation with \\(a_{i}\\) yields\n\\[a_{i}^{T}x=a_{i}^{T}(\\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k})=\\beta_{i},\\]\nusing the same argument as above. So if a vector \\(x\\) is a linear combination of orthonormal vectors, we can easily find the coefficients of the linear combination by taking the inner products with the vectors.\nFor any \\(x\\) that is a linear combination of orthonormal vectors \\(a_{1},\\ldots,a_{k}\\), we have the identity\n\\[x=(a_{1}^{T}x)a_{1}+\\cdots+(a_{k}^{T}x)a_{k}.\\] (5.4)\nThis identity gives us a simple way to check if an \\(n\\)-vector \\(y\\) is a linear combination of the orthonormal vectors \\(a_{1},\\ldots,a_{k}\\). If the identity (5.4) holds for \\(y\\), _i.e._,\n\\[y=(a_{1}^{T}y)a_{1}+\\cdots+(a_{k}^{T}y)a_{k},\\]\nthen (evidently) \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\); conversely, if \\(y\\) is a linear combination of \\(a_{1},\\ldots,a_{k}\\), the identity (5.4) holds for \\(y\\).\nOrthonormal basis.If the \\(n\\)-vectors \\(a_{1},\\ldots,a_{n}\\) are orthonormal, they are linearly independent, and therefore also a basis. In this case they are called an _orthonormal basis_. The three examples above (on page 5.2) are orthonormal bases.\nIf \\(a_{1},\\ldots,a_{n}\\) is an orthonormal basis, then we have, for any \\(n\\)-vector \\(x\\), the identity\n\\[x=(a_{1}^{T}x)a_{1}+\\cdots+(a_{n}^{T}x)a_{n}.\\] (5.5)\nTo see this, we note that since \\(a_{1},\\ldots,a_{n}\\) are a basis, \\(x\\) can be expressed as a linear combination of them; hence the identity (5.4) above holds. The equation above is sometimes called the _orthonormal expansion formula_; the right-hand side is called the _expansion of \\(x\\) in the basis \\(a_{1},\\ldots,a_{n}\\)_. It shows that any \\(n\\)-vector can be expressed as a linear combination of the basis elements, with the coefficients given by taking the inner product of \\(x\\) with the elements of the basis.\nAs an example, we express the 3-vector \\(x=(1,2,3)\\) as a linear combination of the orthonormal basis given in (5.3). The inner products of \\(x\\) with these vectors\nare\n\\[\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right]^{T}x=-3,\\qquad\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1\\\\ 1\\\\ 0\\end{array}\\right]^{T}x=\\frac{3}{\\sqrt{2}},\\qquad\\frac{1}{\\sqrt{2}}\\left[ \\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right]^{T}x=\\frac{-1}{\\sqrt{2}}.\\]\nIt can be verified that the expansion of \\(x\\) in this basis is\n\\[x=(-3)\\left[\\begin{array}{c}0\\\\ 0\\\\ -1\\end{array}\\right]+\\frac{3}{\\sqrt{2}}\\left(\\frac{1}{\\sqrt{2}}\\left[\\begin{array} []{c}1\\\\ 1\\\\ 0\\end{array}\\right]\\right)+\\frac{-1}{\\sqrt{2}}\\left(\\frac{1}{\\sqrt{2}}\\left[ \\begin{array}{c}1\\\\ -1\\\\ 0\\end{array}\\right]\\right).\\]\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Is there an eigenbasis for the identity matrix I_n?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_2.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_7_5.json",
        "question": "For which 2 * 2 matrices A does there exist a nonzero matrix M such that AM = MD, where D = [[2, 0], [0, 3]]? Give your answer in terms of eigenvalues of A.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor which 2 * 2 matrices A does there exist a nonzero matrix M such that AM = MD, where D = [[2, 0], [0, 3]]? Give your answer in terms of eigenvalues of A.\n\nDocument 1:\n----\n**10.21**: _Integral of product of polynomials._ Let \\(p\\) and \\(q\\) be two quadratic polynomials, given by\n\\[p(x)=c_{1}+c_{2}x+c_{3}x^{2},\\qquad q(x)=d_{1}+d_{2}x+d_{3}x^{2}.\\]\nExpress the integral \\(J=\\int_{0}^{1}p(x)q(x)\\,dx\\) in the form \\(J=c^{T}Gd\\), where \\(G\\) is a \\(3\\times 3\\) matrix. Give the entries of \\(G\\) (as numbers).\n**10.22**: _Composition of linear dynamical systems._ We consider two time-invariant linear dynamical systems with outputs. The first one is given by\n\\[x_{t+1}=Ax_{t}+Bu_{t},\\qquad y_{t}=Cx_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(x_{t}\\), input \\(u_{t}\\), and output \\(y_{t}\\). The second is given by\n\\[\\tilde{x}_{t+1}=\\tilde{A}\\tilde{x}_{t}+\\tilde{B}w_{t},\\qquad v_{t}=\\tilde{C} \\tilde{x}_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(\\tilde{x}_{t}\\), input \\(w_{t}\\), and output \\(v_{t}\\). We now connect the output of the first linear dynamical system to the input of the second one, which means we take \\(w_{t}=y_{t}\\). (This is called the _composition_ of the two systems.) Show that this composition can also be expressed as a linear dynamical system with state \\(z_{t}=(x_{t},\\tilde{x}_{t})\\), input \\(u_{t}\\), and output \\(v_{t}\\). (Give the state transition matrix, input matrix, and output matrix.)\n**10.23**: Suppose \\(A\\) is an \\(n\\times n\\) matrix that satisfies \\(A^{2}=0\\). Does this imply that \\(A=0\\)? (This is the case when \\(n=1\\).) If this is (always) true, explain why. If it is not, give a specific counterexample, _i.e._, a matrix \\(A\\) that is nonzero but satisfies \\(A^{2}=0\\).\n**10.24**: _Matrix power identity._ A student says that for any square matrix \\(A\\),\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n1. Find all diagonal squareroots of \\(I_{n}\\). How many are there? (For \\(n=1\\), you should get \\(2\\).)\n* Find a nondiagonal \\(2\\times 2\\) matrix \\(A\\) that satisfies \\(A^{2}=I\\). This means that in general there are even more squareroots of \\(I_{n}\\) than you found in part (a).\n* _Circular shift matrices._ Let \\(A\\) be the \\(5\\times 5\\) matrix \\[A=\\left[\\begin{array}{cccc}0&0&0&0&1\\\\ 1&0&0&0&0\\\\ 0&1&0&0&0\\\\ 0&0&1&0&0\\\\ 0&0&0&1&0\\end{array}\\right].\\] 1. How is \\(Ax\\) related to \\(x\\)? Your answer should be in English. _Hint._ See exercise title. 2. What is \\(A^{5}\\)? _Hint._ The answer should make sense, given your answer to part (a).\n* _Dynamics of an economy._ Let \\(x_{1},x_{2},\\ldots\\) be \\(n\\)-vectors that give the level of economic activity of a country in years \\(1,2,\\ldots\\), in \\(n\\) different sectors (like energy, defense, manufacturing). Specifically, \\((x_{t})_{i}\\) is the level of economic activity in economic sector \\(i\\) (say, in billions of dollars) in year \\(t\\). A common model that connects these economic activity vectors is \\(x_{t+1}=Bx_{t}\\), where \\(B\\) is an \\(n\\times n\\) matrix. (See exercise 9.2.) Five a matrix expression for the total economic activity across all sectors in year \\(t=6\\), in terms of the matrix \\(B\\) and the vector of initial activity levels \\(x_{1}\\). Suppose you can increase economic activity in year \\(t=1\\) by some fixed amount (say, one billion dollars) in _one_ sector, by government spending. How should you choose which sector to stimulate so as to maximize the total economic output in year \\(t=6\\)?\n\n\n",
        "answer": [
            2,
            3
        ],
        "rationale": {
            "Question": "For which 2 * 2 matrices A does there exist a nonzero matrix M such that AM = MD, where D = [[2, 0], [0, 3]]? Give your answer in terms of eigenvalues of A.",
            "Answer": [
                2,
                3
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_5.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_2.json",
        "question": "Is the transformation [[-1, 0], [0, -1]] invertible?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs the transformation [[-1, 0], [0, -1]] invertible?\n\nDocument 1:\n----\n### 11.5 Pseudo-inverse\nLinearly independent columns and Gram invertibility.We first show that an \\(m\\times n\\) matrix \\(A\\) has linearly independent columns if and only if its \\(n\\times n\\) Gram matrix \\(A^{T}A\\) is invertible.\nFirst suppose that the columns of \\(A\\) are linearly independent. Let \\(x\\) be an \\(n\\)-vector which satisfies \\((A^{T}A)x=0\\). Multiplying on the left by \\(x^{T}\\) we get\n\\[0=x^{T}0=x^{T}(A^{T}Ax)=x^{T}A^{T}Ax=\\|Ax\\|^{2},\\]\nwhich implies that \\(Ax=0\\). Since the columns of \\(A\\) are linearly independent, we conclude that \\(x=0\\). Since the only solution of \\((A^{T}A)x=0\\) is \\(x=0\\), we conclude that \\(A^{T}A\\) is invertible.\nNow let's show the converse. Suppose the columns of \\(A\\) are linearly dependent, which means there is a nonzero \\(n\\)-vector \\(x\\) which satisfies \\(Ax=0\\). Multiply on the left by \\(A^{T}\\) to get \\((A^{T}A)x=0\\). This shows that the Gram matrix \\(A^{T}A\\) is singular.\nPseudo-inverse of square or tall matrix.We show here that if \\(A\\) has linearly independent columns (and therefore, is square or tall) then it has a left inverse. (We already have observed the converse, that a matrix with a left inverse has linearly independent columns.) Assuming \\(A\\) has linearly independent columns, we know that \\(A^{T}A\\) is invertible. We now observe that the matrix \\((A^{T}A)^{-1}A^{T}\\) is a left inverse of \\(A\\):\n\\[\\left((A^{T}A)^{-1}A^{T}\\right)A=(A^{T}A)^{-1}(A^{T}A)=I.\\]\nThis particular left-inverse of \\(A\\) will come up in the sequel, and has a name,\nFigure 11.3: Temperature distribution on a \\(100\\times 100\\) grid of nodes. Nodes in the top and bottom rows are held at zero temperature. The three sets of nodes with rectilinear shapes are held at temperature one.\nthe _pseudo-inverse_ of \\(A\\). It is denoted \\(A^{\\dagger}\\) (or \\(A^{+}\\)):\n\\[A^{\\dagger}=(A^{T}A)^{-1}A^{T}.\\] (11.5)\nThe pseudo-inverse is also called the _Moore-Penrose inverse_, after the mathematicians Eliakim Moore and Roger Penrose.\nWhen \\(A\\) is square, the pseudo-inverse \\(A^{\\dagger}\\) reduces to the ordinary inverse:\n\\[A^{\\dagger}=(A^{T}A)^{-1}A^{T}=A^{-1}A^{-T}A^{T}=A^{-1}I=A^{-1}.\\]\nNote that this equation does not make sense (and certainly is not correct) when \\(A\\) is not square.\nPseudo-inverse of a square or wide matrix.Transposing all the equations, we can show that a (square or wide) matrix \\(A\\) has a right inverse if and only if its rows are linearly independent. Indeed, one right inverse is given by\n\\[A^{T}(AA^{T})^{-1}.\\] (11.6)\n(The matrix \\(AA^{T}\\) is invertible if and only if the rows of \\(A\\) are linearly independent.)\nThe matrix in (11.6) is also referred to as the pseudo-inverse of \\(A\\), and denoted \\(A^{\\dagger}\\). The only possible confusion in defining the pseudo-inverse using the two different formulas (11.5) and (11.6) occurs when the matrix \\(A\\) is square. In this case, however, they both reduce to the ordinary inverse:\n\\[A^{T}(AA^{T})^{-1}=A^{T}A^{-T}A^{-1}=A^{-1}.\\]\nPseudo-inverse in other cases.The pseudo-inverse \\(A^{\\dagger}\\) is defined for any matrix, including the case when \\(A\\) is tall but its columns are linearly dependent, the case when \\(A\\) is wide but its rows are linearly dependent, and the case when \\(A\\) is square but not invertible. In these cases, however, it is not a left inverse, right inverse, or inverse, respectively. We mention it here since the reader may encounter it. (We will see what \\(A^{\\dagger}\\) means in these cases in exercise 11.)\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Is the transformation [[-1, 0], [0, -1]] invertible?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_2.json",
            "explanation": "solutions/math_algebra_2.txt",
            "theorem": "invertible matrix theorem",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "wenhuchen/eigen_value2.json",
        "question": "For matrix A = [[3, 1, 1], [2, 4, 2], [1, 1, 3]], what are its eigen values?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[3, 1, 1], [2, 4, 2], [1, 1, 3]], what are its eigen values?\n\nDocument 1:\n----\n### 6.1 Matrices\nA _matrix_ is a rectangular array of numbers written between rectangular brackets, as in\n\\[\\left[\\begin{array}{cccc}0&1&-2.3&0.1\\\\ 1.3&4&-0.1&0\\\\ 4.1&-1&0&1.7\\end{array}\\right].\\]\nIt is also common to use large parentheses instead of rectangular brackets, as in\n\\[\\left(\\begin{array}{cccc}0&1&-2.3&0.1\\\\ 1.3&4&-0.1&0\\\\ 4.1&-1&0&1.7\\end{array}\\right).\\]\nAn important attribute of a matrix is its _size_ or _dimensions_, _i.e._, the numbers of rows and columns. The matrix above has 3 rows and 4 columns, so its size is \\(3\\times 4\\). A matrix of size \\(m\\times n\\) is called an \\(m\\times n\\) matrix.\nThe _elements_ (or _entries_ or _coefficients_) of a matrix are the values in the array. The \\(i,j\\) element is the value in the \\(i\\)th row and \\(j\\)th column, denoted by double subscripts: the \\(i,j\\) element of a matrix \\(A\\) is denoted \\(A_{ij}\\) (or \\(A_{i,j}\\), when \\(i\\) or \\(j\\) is more than one digit or character). The positive integers \\(i\\) and \\(j\\) are called the (row and column) _indices_. If \\(A\\) is an \\(m\\times n\\) matrix, then the row index \\(i\\) runs from 1 to \\(m\\) and the column index \\(j\\) runs from 1 to \\(n\\). Row indices go from top to bottom, so row 1 is the top row and row \\(m\\) is the bottom row. Column indices go from left to right, so column 1 is the left column and column \\(n\\) is the right column.\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\nTwo matrices are equal if they have the same size, and the corresponding entries are all equal. As with vectors, we normally deal with matrices with entries that\nare real numbers, which will be our assumption unless we state otherwise. The set of real \\(m\\times n\\) matrices is denoted \\(\\mathbf{R}^{m\\times n}\\). But matrices with complex entries, for example, do arise in some applications.\nMatrix indexing.As with vectors, standard mathematical notation indexes the rows and columns of a matrix starting from 1. In computer languages, matrices are often (but not always) stored as 2-dimensional arrays, which can be indexed in a variety of ways, depending on the language. Lower level languages typically use indices starting from 0; higher level languages and packages that support matrix operations usually use standard mathematical indexing, starting from 1.\nSquare, tall, and wide matrices.A _square_ matrix has an equal number of rows and columns. A square matrix of size \\(n\\times n\\) is said to be of _order_\\(n\\). A _tall_ matrix has more rows than columns (size \\(m\\times n\\) with \\(m>n\\)). A _wide_ matrix has more columns than rows (size \\(m\\times n\\) with \\(n>m\\)).\nColumn and row vectors.An \\(n\\)-vector can be interpreted as an \\(n\\times 1\\) matrix; we do not distinguish between vectors and matrices with one column. A matrix with only one row, _i.e._, with size \\(1\\times n\\), is called a _row vector_; to give its size, we can refer to it as an _\\(n\\)-row-vector_. As an example,\n\\[\\left[\\begin{array}{ccc}-2.1&-3&0\\end{array}\\right]\\]\nis a 3-row-vector (or \\(1\\times 3\\) matrix). To distinguish them from row vectors, vectors are sometimes called _column vectors_. A \\(1\\times 1\\) matrix is considered to be the same as a scalar.\n\n\n",
        "answer": [
            2,
            6
        ],
        "rationale": {
            "Question": "For matrix A = [[3, 1, 1], [2, 4, 2], [1, 1, 3]], what are its eigen values?",
            "Answer": [
                2,
                6
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "self",
            "id": "wenhuchen/eigen_value2.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "wenhuchen/definite_matrix1.json",
        "question": "Consider the matrix of A=[[1, 4], [4, 1]], is this a positive definite matrix?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider the matrix of A=[[1, 4], [4, 1]], is this a positive definite matrix?\n\nDocument 1:\n----\n**10.21**: _Integral of product of polynomials._ Let \\(p\\) and \\(q\\) be two quadratic polynomials, given by\n\\[p(x)=c_{1}+c_{2}x+c_{3}x^{2},\\qquad q(x)=d_{1}+d_{2}x+d_{3}x^{2}.\\]\nExpress the integral \\(J=\\int_{0}^{1}p(x)q(x)\\,dx\\) in the form \\(J=c^{T}Gd\\), where \\(G\\) is a \\(3\\times 3\\) matrix. Give the entries of \\(G\\) (as numbers).\n**10.22**: _Composition of linear dynamical systems._ We consider two time-invariant linear dynamical systems with outputs. The first one is given by\n\\[x_{t+1}=Ax_{t}+Bu_{t},\\qquad y_{t}=Cx_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(x_{t}\\), input \\(u_{t}\\), and output \\(y_{t}\\). The second is given by\n\\[\\tilde{x}_{t+1}=\\tilde{A}\\tilde{x}_{t}+\\tilde{B}w_{t},\\qquad v_{t}=\\tilde{C} \\tilde{x}_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(\\tilde{x}_{t}\\), input \\(w_{t}\\), and output \\(v_{t}\\). We now connect the output of the first linear dynamical system to the input of the second one, which means we take \\(w_{t}=y_{t}\\). (This is called the _composition_ of the two systems.) Show that this composition can also be expressed as a linear dynamical system with state \\(z_{t}=(x_{t},\\tilde{x}_{t})\\), input \\(u_{t}\\), and output \\(v_{t}\\). (Give the state transition matrix, input matrix, and output matrix.)\n**10.23**: Suppose \\(A\\) is an \\(n\\times n\\) matrix that satisfies \\(A^{2}=0\\). Does this imply that \\(A=0\\)? (This is the case when \\(n=1\\).) If this is (always) true, explain why. If it is not, give a specific counterexample, _i.e._, a matrix \\(A\\) that is nonzero but satisfies \\(A^{2}=0\\).\n**10.24**: _Matrix power identity._ A student says that for any square matrix \\(A\\),\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n1. Find all diagonal squareroots of \\(I_{n}\\). How many are there? (For \\(n=1\\), you should get \\(2\\).)\n* Find a nondiagonal \\(2\\times 2\\) matrix \\(A\\) that satisfies \\(A^{2}=I\\). This means that in general there are even more squareroots of \\(I_{n}\\) than you found in part (a).\n* _Circular shift matrices._ Let \\(A\\) be the \\(5\\times 5\\) matrix \\[A=\\left[\\begin{array}{cccc}0&0&0&0&1\\\\ 1&0&0&0&0\\\\ 0&1&0&0&0\\\\ 0&0&1&0&0\\\\ 0&0&0&1&0\\end{array}\\right].\\] 1. How is \\(Ax\\) related to \\(x\\)? Your answer should be in English. _Hint._ See exercise title. 2. What is \\(A^{5}\\)? _Hint._ The answer should make sense, given your answer to part (a).\n* _Dynamics of an economy._ Let \\(x_{1},x_{2},\\ldots\\) be \\(n\\)-vectors that give the level of economic activity of a country in years \\(1,2,\\ldots\\), in \\(n\\) different sectors (like energy, defense, manufacturing). Specifically, \\((x_{t})_{i}\\) is the level of economic activity in economic sector \\(i\\) (say, in billions of dollars) in year \\(t\\). A common model that connects these economic activity vectors is \\(x_{t+1}=Bx_{t}\\), where \\(B\\) is an \\(n\\times n\\) matrix. (See exercise 9.2.) Five a matrix expression for the total economic activity across all sectors in year \\(t=6\\), in terms of the matrix \\(B\\) and the vector of initial activity levels \\(x_{1}\\). Suppose you can increase economic activity in year \\(t=1\\) by some fixed amount (say, one billion dollars) in _one_ sector, by government spending. How should you choose which sector to stimulate so as to maximize the total economic output in year \\(t=6\\)?\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Consider the matrix of A=[[1, 4], [4, 1]], is this a positive definite matrix?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | https://www.math.usm.edu/lambers/mat419/lecture3.pdf",
            "id": "wenhuchen/definite_matrix1.json",
            "explanation": "NONE",
            "theorem": "definite matrix criteria",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_6_4.json",
        "question": "What is the determinant of the matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat is the determinant of the matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]?\n\nDocument 1:\n----\n2. Give an expression for a middle inverse, assuming the conditions in part (a) hold.\n**11.15**: _Invertibility of population dynamics matrix._ Consider the population dynamics matrix\n\\[A=\\left[\\begin{array}{cccccc}b_{1}&b_{2}&\\cdots&b_{99}&b_{100}\\\\ 1-d_{1}&0&\\cdots&0&0\\\\ 0&1-d_{2}&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 0&0&\\cdots&1-d_{99}&0\\end{array}\\right],\\]\nwhere \\(b_{i}\\geq 0\\) are the birth rates and \\(0\\leq d_{i}\\leq 1\\) are death rates. What are the conditions on \\(b_{i}\\) and \\(d_{i}\\) under which \\(A\\) is invertible? (If the matrix is never invertible or always invertible, say so.) Justify your answer.\n**11.16**: _Inverse of running sum matrix._ Find the inverse of the \\(n\\times n\\) running sum matrix,\n\\[S=\\left[\\begin{array}{cccccc}1&0&\\cdots&0&0\\\\ 1&1&\\cdots&0&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&\\cdots&1&0\\\\ 1&1&\\cdots&1&1\\end{array}\\right].\\]\nDoes your answer make sense?\n**11.17**: _A matrix identity._ Suppose \\(A\\) is a square matrix that satisfies \\(A^{k}=0\\) for some integer \\(k\\). (Such a matrix is called _nilpotent_.) A student guesses that \\((I-A)^{-1}=I+A+\\cdots+A^{k-1}\\), based on the infinite series \\(1/(1-a)=1+a+a^{2}+\\cdots\\), which holds for numbers \\(a\\) that satisfy \\(|a|<1\\).\nIs the student right or wrong? If right, show that her assertion holds with no further assumptions about \\(A\\). If she is wrong, give a counterexample, _i.e._, a matrix \\(A\\) that satisfies \\(A^{k}=0\\), but \\(I+A+\\cdots+A^{k-1}\\) is not the inverse of \\(I-A\\).\n**11.18**: _Tall-wide product._ Suppose \\(A\\) is an \\(n\\times p\\) matrix and \\(B\\) is a \\(p\\times n\\) matrix, so \\(C=AB\\) makes sense. Explain why \\(C\\) cannot be invertible if \\(A\\) is tall and \\(B\\) is wide, _i.e._, if \\(p<n\\). _Hint._ First argue that the columns of \\(B\\) must be linearly dependent.\n**11.19**: _Control restricted to one time period._ A linear dynamical system has the form \\(x_{t+1}=Ax_{t}+u_{t}\\), where the \\(n\\)-vector \\(x_{t}\\) is the state and \\(u_{t}\\) is the input at time \\(t\\). Our goal is to choose the input sequence \\(u_{1},\\ldots,u_{N-1}\\) so as to achieve \\(x_{N}=x^{\\rm des}\\), where \\(x^{\\rm des}\\) is a given \\(n\\)-vector, and \\(N\\) is given. The input sequence must satisfy \\(u_{t}=0\\) unless \\(t=K\\), where \\(K<N\\) is given. In other words, the input can only act at time \\(t=K\\). Give a formula for \\(u_{K}\\) that achieves this goal. Your formula can involve \\(A\\), \\(N\\), \\(K\\), \\(x_{1}\\), and \\(x^{\\rm des}\\). You can assume that \\(A\\) is invertible. _Hint._ First derive an expression for \\(x_{K}\\), then use the dynamics equation to find \\(x_{K+1}\\). From \\(x_{K+1}\\) you can find \\(x_{N}\\).\n**11.20**: _Immigration._ The population dynamics of a country is given by \\(x_{t+1}=Ax_{t}+u\\), \\(t=1,\\ldots,T-1\\), where the 100-vector \\(x_{t}\\) gives the population age distribution in year \\(t\\), and \\(u\\) gives the immigration age distribution (with negative entries meaning emigration), which we assume is constant (_i.e._, does not vary with \\(t\\)). You are given \\(A\\), \\(x_{1}\\), and \\(x^{\\rm des}\\), a 100-vector that represents a desired population distribution in year \\(T\\). We seek a constant level of immigration \\(u\\) that achieves \\(x_{T}=x^{\\rm des}\\).\nGive a matrix formula for \\(u\\). If your formula only makes sense when some conditions hold (for example invertibility of one or more matrices), say so.\n**11.21**: _Quadrature weights._ Consider a quadrature problem (see exercise 8.12) with \\(n=4\\), with points \\(t=(-0.6,-0.2,0.2,0.6)\\). We require that the quadrature rule be exact for all polynomials of degree up to \\(d=3\\).\nSet this up as a square system of linear equations in the weight vector. Numerically solve this system to get the weights. Compute the true value and the quadrature estimate,\n\n\n",
        "answer": -3,
        "rationale": {
            "Question": "What is the determinant of the matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]?",
            "Answer": -3,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_4.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "mingyin/orthogonal-similarity1.json",
        "question": "If A and B are both orthogonal square matrices, and det A = -det B. What is det(A+B)? Return the numerical value.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIf A and B are both orthogonal square matrices, and det A = -det B. What is det(A+B)? Return the numerical value.\n\nDocument 1:\n----\n* \\(A\\) is invertible.\n* The columns of \\(A\\) are linearly independent.\n* The rows of \\(A\\) are linearly independent.\n* \\(A\\) has a left inverse.\n* \\(A\\) has a right inverse.\n**Examples.**\n* The identity matrix \\(I\\) is invertible, with inverse \\(I^{-1}=I\\), since \\(II=I\\).\n* A diagonal matrix \\(A\\) is invertible if and only if its diagonal entries are nonzero. The inverse of an \\(n\\times n\\) diagonal matrix \\(A\\) with nonzero diagonal entries is \\[A^{-1}=\\left[\\begin{array}{cccc}1/A_{11}&0&\\cdots&0\\\\ 0&1/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&1/A_{nn}\\end{array}\\right],\\] since \\[AA^{-1}=\\left[\\begin{array}{cccc}A_{11}/A_{11}&0&\\cdots&0\\\\ 0&A_{22}/A_{22}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&A_{nn}/A_{nn}\\end{array}\\right]=I.\\] In compact notation, we have \\[\\mathbf{diag}(A_{11},\\ldots,A_{nn})^{-1}=\\mathbf{diag}(A_{11}^{-1},\\ldots,A_{ nn}^{-1}).\\] Note that the inverse on the left-hand side of this equation is the matrix inverse, while the inverses appearing on the right-hand side are scalar inverses.\n* As a non-obvious example, the matrix \\[A=\\left[\\begin{array}{cccc}1&-2&3\\\\ 0&2&2\\\\ -3&-4&-4\\end{array}\\right]\\] is invertible, with inverse \\[A^{-1}=\\frac{1}{30}\\left[\\begin{array}{cccc}0&-20&-10\\\\ -6&5&-2\\\\ 6&10&2\\end{array}\\right].\\] This can be verified by checking that \\(AA^{-1}=I\\) (or that \\(A^{-1}A=I\\), since either of these implies the other).\n* \\(2\\times 2\\) _matrices._ A \\(2\\times 2\\) matrix \\(A\\) is invertible if and only if \\(A_{11}A_{22}\\neq A_{12}A_{21}\\), with inverse \\[A^{-1}=\\left[\\begin{array}{cccc}A_{11}&A_{12}\\\\ A_{21}&A_{22}\\end{array}\\right]^{-1}=\\frac{1}{A_{11}A_{22}-A_{12}A_{21}}\\left[ \\begin{array}{cccc}A_{22}&-A_{12}\\\\ -A_{21}&A_{11}\\end{array}\\right].\\] (There are similar formulas for the inverse of a matrix of any size, but they grow very quickly in complexity and so are not very useful in most applications.)\n* _Orthogonal matrix._ If \\(A\\) is square with orthonormal columns, we have \\(A^{T}A=I\\), so \\(A\\) is invertible with inverse \\(A^{-1}=A^{T}\\).\nInverse of matrix transpose.If \\(A\\) is invertible, its transpose \\(A^{T}\\) is also invertible and its inverse is \\((A^{-1})^{T}\\):\n\\[(A^{T})^{-1}=(A^{-1})^{T}.\\]\nSince the order of the transpose and inverse operations does not matter, this matrix is sometimes written as \\(A^{-T}\\).\nInverse of matrix product.If \\(A\\) and \\(B\\) are invertible (hence, square) and of the same size, then \\(AB\\) is invertible, and\n\\[(AB)^{-1}=B^{-1}A^{-1}.\\] (11.2)\nThe inverse of a product is the product of the inverses, in reverse order.\nDual basis.Suppose that \\(A\\) is invertible with inverse \\(B=A^{-1}\\). Let \\(a_{1},\\ldots,a_{n}\\) be the columns of \\(A\\), and \\(b_{1}^{T},\\ldots,b_{n}^{T}\\) denote the _rows_ of \\(B\\), _i.e._, the columns of \\(B^{T}\\):\n\\[A=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right],\\qquad B= \\left[\\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right].\\]\nWe know that \\(a_{1},\\ldots,a_{n}\\) form a basis, since the columns of \\(A\\) are linearly independent. The vectors \\(b_{1},\\ldots,b_{n}\\) also form a basis, since the rows of \\(B\\) are linearly independent. They are called the _dual basis_ of \\(a_{1},\\ldots,a_{n}\\). (The dual basis of \\(b_{1},\\ldots,b_{n}\\) is \\(a_{1},\\ldots,a_{n}\\), so they called _dual bases_.)\nNow suppose that \\(x\\) is any \\(n\\)-vector. It can be expressed as a linear combination of the basis vectors \\(a_{1},\\ldots,a_{n}\\):\n\\[x=\\beta_{1}a_{1}+\\cdots+\\beta_{n}a_{n}.\\]\nThe dual basis gives us a simple way to find the coefficients \\(\\beta_{1},\\ldots,\\beta_{n}\\).\nWe start with \\(AB=I\\), and multiply by \\(x\\) to get\n\\[x=ABx=\\left[\\begin{array}{ccc}a_{1}&\\cdots&a_{n}\\end{array}\\right]\\left[ \\begin{array}{c}b_{1}^{T}\\\\ \\vdots\\\\ b_{n}^{T}\\end{array}\\right]x=(b_{1}^{T}x)a_{1}+\\cdots+(b_{n}^{T}x)a_{n}.\\]\nThis means (since the vectors \\(a_{1},\\ldots,a_{n}\\) are linearly independent) that \\(\\beta_{i}=b_{i}^{T}x\\). In words: The coefficients in the expansion of a vector in a basis are given by the inner products with the dual basis vectors. Using matrix notation, we can say that \\(\\beta=B^{T}x=(A^{-1})^{T}x\\) is the vector of coefficients of \\(x\\) in the basis given by the columns of \\(A\\).\nAs a simple numerical example, consider the basis\n\\[a_{1}=(1,1),\\qquad a_{2}=(1,-1).\\]\nThe dual basis consists of the rows of \\([\\begin{array}{cc}a_{1}&a_{2}\\end{array}]^{-1}\\), which are\n\n\n",
        "answer": 0.0,
        "rationale": {
            "Question": "If A and B are both orthogonal square matrices, and det A = -det B. What is det(A+B)? Return the numerical value.",
            "Answer": 0.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 9.3.3",
            "id": "mingyin/orthogonal-similarity1.json",
            "explanation": "NONE",
            "theorem": "orthogonal similarity",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": true
    },
    {
        "id": "wenhuchen/gauss_lemma2.json",
        "question": "Is 10 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs 10 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.\n\nDocument 1:\n----\nFigure 5.3 illustrates the Gram-Schmidt algorithm for two 2-vectors. The top row shows the original vectors; the middle and bottom rows show the first and second iterations of the loop in the Gram-Schmidt algorithm, with the left-hand side showing the orthogonalization step, and the right-hand side showing the normalization step.\n## Chapter 5 Linear independence\nFigure 5.3: Gram–Schmidt algorithm applied to two 2-vectors \\(a_{1}\\), \\(a_{2}\\). _Top._ The original vectors \\(a_{1}\\) and \\(a_{2}\\). The gray circle shows the points with norm one. _Middle left._ The orthogonalization step in the first iteration yields \\(\\tilde{q}_{1}=a_{1}\\). _Middle right._ The normalization step in the first iteration scales \\(\\tilde{q}_{1}\\) to have norm one, which yields \\(q_{1}\\). _Bottom left._ The orthogonalization step in the second iteration subtracts a multiple of \\(q_{1}\\) to yield the vector \\(\\tilde{q}_{2}\\), which is orthogonal to \\(q_{1}\\). _Bottom right._ The normalization step in the second iteration scales \\(\\tilde{q}_{2}\\) to have norm one, which yields \\(q_{2}\\).\nAnalysis of Gram-Schmidt algorithm.Let us show that the following hold, for \\(i=1,\\ldots,k\\), assuming \\(a_{1},\\ldots,a_{k}\\) are linearly independent.\n1. \\(\\tilde{q}_{i}\\neq 0\\), so the linear dependence test in step 2 is not satisfied, and we do not have a divide-by-zero error in step 3.\n2. \\(q_{1},\\ldots,q_{i}\\) are orthonormal.\n3. \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i}\\).\n4. \\(q_{i}\\) is a linear combination of \\(a_{1},\\ldots,a_{i}\\).\nWe show this by induction. For \\(i=1\\), we have \\(\\tilde{q}_{1}=a_{1}\\). Since \\(a_{1},\\ldots,a_{k}\\) are linearly independent, we must have \\(a_{1}\\neq 0\\), and therefore \\(\\tilde{q}_{1}\\neq 0\\), so assertion 1 holds. The single vector \\(q_{1}\\) (considered as a list with one element) is evidently orthonormal, since \\(\\|q_{1}\\|=1\\), so assertion 2 holds. We have \\(a_{1}=\\|\\tilde{q}_{1}\\|q_{1}\\), and \\(q_{1}=(1/\\|\\tilde{q}_{1}\\|)a_{1}\\), so assertions 3 and 4 hold.\nSuppose our assertion holds for some \\(i-1\\), with \\(i<k\\); we will show it holds for \\(i\\). If \\(\\tilde{q}_{i}=0\\), then \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i-1}\\) (from the first step in the algorithm); but each of these is (by the induction hypothesis) a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), so it follows that \\(a_{i}\\) is a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), which contradicts our assumption that \\(a_{1},\\ldots,a_{k}\\) are linearly independent. So assertion 1 holds for \\(i\\).\nStep 3 of the algorithm ensures that \\(q_{1},\\ldots,q_{i}\\) are normalized; to show they are orthogonal we will show that \\(q_{i}\\perp q_{j}\\) for \\(j=1,\\ldots,i-1\\). (Our induction hypothesis tells us that \\(q_{r}\\perp q_{s}\\) for \\(r,s<i\\).) For any \\(j=1,\\ldots,i-1\\), we have (using step 1)\n\\[q_{j}^{T}\\tilde{q}_{i} = q_{j}^{T}a_{i}-(q_{1}^{T}a_{i})(q_{j}^{T}q_{1})-\\cdots-(q_{i-1}^ {T}a_{i})(q_{j}^{T}q_{i-1})\\] \\[= q_{j}^{T}a_{i}-q_{j}^{T}a_{i}=0,\\]\nusing \\(q_{j}^{T}q_{k}=0\\) for \\(j\\neq k\\) and \\(q_{j}^{T}q_{j}=1\\). (This explains why step 1 is called the orthogonalization step: We subtract from \\(a_{i}\\) a linear combination of \\(q_{1},\\ldots,q_{i-1}\\) that ensures \\(q_{i}\\perp\\tilde{q}_{j}\\) for \\(j<i\\).) Since \\(q_{i}=(1/\\|\\tilde{q}_{i}\\|)\\tilde{q}_{i}\\), we have \\(q_{i}^{T}q_{j}=0\\) for \\(j=1,\\ldots,i-1\\). So assertion 2 holds for \\(i\\).\nIt is immediate that \\(a_{i}\\) is a linear combination of \\(q_{1},\\ldots,q_{i}\\):\n\\[a_{i} = \\tilde{q}_{i}+(q_{1}^{T}a_{i})q_{1}+\\cdots+(q_{i-1}^{T}a_{i})q_{i -1}\\] \\[= (q_{1}^{T}a_{i})q_{1}+\\cdots+(q_{i-1}^{T}a_{i})q_{i-1}+\\|\\tilde{ q}_{i}\\|q_{i}.\\]\nFrom step 1 of the algorithm, we see that \\(\\tilde{q}_{i}\\) is a linear combination of the vectors \\(a_{1},q_{1},\\ldots,q_{i-1}\\). By the induction hypothesis, each of \\(q_{1},\\ldots,q_{i-1}\\) is a linear combination of \\(a_{1},\\ldots,a_{i-1}\\), so \\(\\tilde{q}_{i}\\) (and therefore also \\(q_{i}\\)) is a linear combination of \\(a_{1},\\ldots,a_{i}\\). Thus assertions 3 and 4 hold.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is 10 a quadratic residue modulo 19? Use Gauss's Lemma to answer it.",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | http://mathonline.wikidot.com/example-questions-regarding-gauss-s-lemma",
            "id": "wenhuchen/gauss_lemma2.json",
            "explanation": "NONE",
            "theorem": "gauss's lemma",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "jianyu_xu/integer_programming_2.json",
        "question": "Solve the following linear programming problems: maximize 3x + y subject to (1) -x + y <= 1, (2) 2x + y <= 4, (3) x>= 0 and y >= 0. What's [x, y] for the optimal solution?",
        "prediction": [
            1,
            2
        ],
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nSolve the following linear programming problems: maximize 3x + y subject to (1) -x + y <= 1, (2) 2x + y <= 4, (3) x>= 0 and y >= 0. What's [x, y] for the optimal solution?\n\nDocument 1:\n----\n* _Checking the computed solution of linear equations._ One of your colleagues says that whenever you compute the solution \\(x\\) of a square set of \\(n\\) equations \\(Ax=b\\) (say, using QR factorization), you should compute the number \\(\\|Ax-b\\|\\) and check that it is small. (It is not exactly zero due to the small rounding errors made in floating point computations.) Another colleague says that this would be nice to do, but the additional cost of computing \\(\\|Ax-b\\|\\) is too high. Briefly comment on your colleagues' advice. Who is right?\n* _Sensitivity of solution of linear equations._ Let \\(A\\) be an invertible \\(n\\times n\\) matrix, and \\(b\\) and \\(x\\) be \\(n\\)-vectors satisfying \\(Ax=b\\). Suppose we perturb the \\(j\\)th entry of \\(b\\) by \\(\\epsilon\\neq 0\\) (which is a traditional symbol for a small quantity), so \\(b\\) becomes \\(\\tilde{b}=b+\\epsilon e_{j}\\). Let \\(\\tilde{x}\\) be the \\(n\\)-vector that satisfies \\(A\\tilde{x}=\\tilde{b}\\), _i.e._, the solution of the linear equations using the perturbed right-hand side. We are interested in \\(\\|x-\\tilde{x}\\|\\), which is how much the solution changes due to the change in the right-hand side. The ratio \\(\\|x-\\tilde{x}\\|/|\\epsilon|\\) gives the sensitivity of the solution to changes (perturbations) of the \\(j\\)th entry of \\(b\\).\n* Show that \\(\\|x-\\tilde{x}\\|\\) does not depend on \\(b\\); it only depends on the matrix \\(A\\), \\(\\epsilon\\), and \\(j\\).\n* How would you find the index \\(j\\) that maximizes the value of \\(\\|x-\\tilde{x}\\|\\)? By part (a), your answer should be in terms of \\(A\\) (or quantities derived from \\(A\\)) and \\(\\epsilon\\) only. _Remark._ If a small change in the right-hand side vector \\(b\\) can lead to a large change in the solution, we say that the linear equations \\(Ax=b\\) are _poorly conditioned_ or _ill-conditioned_. As a practical matter it means that unless you are very confident in what the entries of \\(b\\) are, the solution \\(A^{-1}b\\) may not be useful in practice.\n* _Timing test._ Generate a random \\(n\\times n\\) matrix \\(A\\) and an \\(n\\)-vector \\(b\\), for \\(n=500\\), \\(n=1000\\), and \\(n=2000\\). For each of these, compute the solution \\(x=A^{-1}b\\) (for example using the backslash operator, if the software you are using supports it), and verify that \\(Ax-b\\) is (very) small. Report the time it takes to solve each of these three sets of linear equations, and for each one work out the implied speed of your processor in Gflop/s, based on the \\(2n^{3}\\) complexity of solving equations using the QR factorization.\n* _Solving multiple linear equations efficiently._ Suppose the \\(n\\times n\\) matrix \\(A\\) is invertible. We can solve the system of linear equations \\(Ax=b\\) in around \\(2n^{3}\\) flops using algorithm 11.2. Once we have done that (specifically, computed the QR factorization of \\(A\\)), we can solve an additional set of linear equations with same matrix but different right-hand side, \\(Ay=c\\), in around \\(3n^{2}\\) additional flops. Assuming we have solved both of these sets of equations, suppose we want to solve \\(Az=d\\), where \\(d=\\alpha b+\\beta c\\) is a linear combination of \\(b\\) and \\(c\\). (We are given the coefficients \\(\\alpha\\) and \\(\\beta\\).) Suggest a method for doing this that is even faster than re-using the QR factorization of \\(A\\). Your method should have a complexity that is _linear_ in \\(n\\). Give rough estimates for the time needed to solve \\(Ax=b\\), \\(Ay=c\\), and \\(Az=d\\) (using your method) for \\(n=3000\\) on a computer capable of carrying out 1 Gflop/s.\n\n\n",
        "answer": [
            2,
            0
        ],
        "rationale": {
            "Question": "Solve the following linear programming problems: maximize 3x + y subject to (1) -x + y <= 1, (2) 2x + y <= 4, (3) x>= 0 and y >= 0. What's [x, y] for the optimal solution?",
            "Answer": [
                2,
                0
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "self",
            "id": "jianyu_xu/integer_programming_2.json",
            "explanation": "NONE",
            "theorem": "integer programming",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "wenhuchen/definite_matrix2.json",
        "question": "Consider the matrix of A=[[1, -1], [-1, 4]], is this a positive definite matrix?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider the matrix of A=[[1, -1], [-1, 4]], is this a positive definite matrix?\n\nDocument 1:\n----\n**10.21**: _Integral of product of polynomials._ Let \\(p\\) and \\(q\\) be two quadratic polynomials, given by\n\\[p(x)=c_{1}+c_{2}x+c_{3}x^{2},\\qquad q(x)=d_{1}+d_{2}x+d_{3}x^{2}.\\]\nExpress the integral \\(J=\\int_{0}^{1}p(x)q(x)\\,dx\\) in the form \\(J=c^{T}Gd\\), where \\(G\\) is a \\(3\\times 3\\) matrix. Give the entries of \\(G\\) (as numbers).\n**10.22**: _Composition of linear dynamical systems._ We consider two time-invariant linear dynamical systems with outputs. The first one is given by\n\\[x_{t+1}=Ax_{t}+Bu_{t},\\qquad y_{t}=Cx_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(x_{t}\\), input \\(u_{t}\\), and output \\(y_{t}\\). The second is given by\n\\[\\tilde{x}_{t+1}=\\tilde{A}\\tilde{x}_{t}+\\tilde{B}w_{t},\\qquad v_{t}=\\tilde{C} \\tilde{x}_{t},\\quad t=1,2,\\ldots,\\]\nwith state \\(\\tilde{x}_{t}\\), input \\(w_{t}\\), and output \\(v_{t}\\). We now connect the output of the first linear dynamical system to the input of the second one, which means we take \\(w_{t}=y_{t}\\). (This is called the _composition_ of the two systems.) Show that this composition can also be expressed as a linear dynamical system with state \\(z_{t}=(x_{t},\\tilde{x}_{t})\\), input \\(u_{t}\\), and output \\(v_{t}\\). (Give the state transition matrix, input matrix, and output matrix.)\n**10.23**: Suppose \\(A\\) is an \\(n\\times n\\) matrix that satisfies \\(A^{2}=0\\). Does this imply that \\(A=0\\)? (This is the case when \\(n=1\\).) If this is (always) true, explain why. If it is not, give a specific counterexample, _i.e._, a matrix \\(A\\) that is nonzero but satisfies \\(A^{2}=0\\).\n**10.24**: _Matrix power identity._ A student says that for any square matrix \\(A\\),\n\\[(A+I)^{3}=A^{3}+3A^{2}+3A+I.\\]\nIs she right? If she is, explain why; if she is wrong, give a specific counterexample, _i.e._, a square matrix \\(A\\) for which it does not hold.\n**10.25**: _Squareroots of the identity._ The number \\(1\\) has two squareroots (_i.e._, numbers who square is \\(1\\)), \\(1\\) and \\(-1\\). The \\(n\\times n\\) identity matrix \\(I_{n}\\) has many more squareroots.\n1. Find all diagonal squareroots of \\(I_{n}\\). How many are there? (For \\(n=1\\), you should get \\(2\\).)\n* Find a nondiagonal \\(2\\times 2\\) matrix \\(A\\) that satisfies \\(A^{2}=I\\). This means that in general there are even more squareroots of \\(I_{n}\\) than you found in part (a).\n* _Circular shift matrices._ Let \\(A\\) be the \\(5\\times 5\\) matrix \\[A=\\left[\\begin{array}{cccc}0&0&0&0&1\\\\ 1&0&0&0&0\\\\ 0&1&0&0&0\\\\ 0&0&1&0&0\\\\ 0&0&0&1&0\\end{array}\\right].\\] 1. How is \\(Ax\\) related to \\(x\\)? Your answer should be in English. _Hint._ See exercise title. 2. What is \\(A^{5}\\)? _Hint._ The answer should make sense, given your answer to part (a).\n* _Dynamics of an economy._ Let \\(x_{1},x_{2},\\ldots\\) be \\(n\\)-vectors that give the level of economic activity of a country in years \\(1,2,\\ldots\\), in \\(n\\) different sectors (like energy, defense, manufacturing). Specifically, \\((x_{t})_{i}\\) is the level of economic activity in economic sector \\(i\\) (say, in billions of dollars) in year \\(t\\). A common model that connects these economic activity vectors is \\(x_{t+1}=Bx_{t}\\), where \\(B\\) is an \\(n\\times n\\) matrix. (See exercise 9.2.) Five a matrix expression for the total economic activity across all sectors in year \\(t=6\\), in terms of the matrix \\(B\\) and the vector of initial activity levels \\(x_{1}\\). Suppose you can increase economic activity in year \\(t=1\\) by some fixed amount (say, one billion dollars) in _one_ sector, by government spending. How should you choose which sector to stimulate so as to maximize the total economic output in year \\(t=6\\)?\n\n\n",
        "answer": true,
        "rationale": {
            "Question": "Consider the matrix of A=[[1, -1], [-1, 4]], is this a positive definite matrix?",
            "Answer": true,
            "Answer_type": "bool",
            "Picture": null,
            "source": "website | https://www.math.usm.edu/lambers/mat419/lecture3.pdf",
            "id": "wenhuchen/definite_matrix2.json",
            "explanation": "NONE",
            "theorem": "definite matrix criteria",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "wenhuchen/vieta's_formula.json",
        "question": "Let m and n be the roots of the equation 2x^2 + 15x + 16 = 0. What is the value of 1/m + 1/n?",
        "prediction": -7.5,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet m and n be the roots of the equation 2x^2 + 15x + 16 = 0. What is the value of 1/m + 1/n?\n\nDocument 1:\n----\n### 16.3 Solving constrained least squares problems\nWe can compute the solution (16.6) of the constrained least squares problem by forming and solving the KKT equations (16.4).\n**Algorithm 16.1** Constrained least squares via KKT equations\n**given** an \\(m\\times n\\) matrix \\(A\\) and a \\(p\\times n\\) matrix \\(C\\) that satisfy (16.5), an \\(m\\)-vector \\(b\\), and a \\(p\\)-vector \\(d\\).\n1. _Form Gram matrix._ Compute \\(A^{T}A\\).\n2. _Solve KKT equations._ Solve KKT equations (16.4) by QR factorization and back substitution.\n**Algorithm 16.2** Constrained least squares via KKT equations\nThe second step cannot fail, provided the assumption (16.5) holds. Let us analyze the complexity of this algorithm. The first step, forming the Gram matrix, requires \\(mn^{2}\\) flops (see page 16.3). The second step requires the solution of a square system of \\(n+p\\) equations, which costs \\(2(n+p)^{3}\\) flops, so the total is\n\\[mn^{2}+2(n+p)^{3}\\]\nflops. This grows linearly in \\(m\\) and cubicly in \\(n\\) and \\(p\\). The assumption (16.5) implies \\(p\\leq n\\), so in terms of order, \\((n+p)^{3}\\) can be replaced with \\(n^{3}\\).\nSolving constrained least squares problems via QR factorization.We now give a method for solving the constrained least squares problem that generalizes the QR factorization method for least squares problems (algorithm 12.1). We assume that \\(A\\) and \\(C\\) satisfy the conditions (16.5).\nWe start by rewriting the KKT equations (16.4) as\n\\[2(A^{T}A+C^{T}C)\\hat{x}+C^{T}w=2A^{T}b,\\qquad C\\hat{x}=d\\] (16.7)\nwith a new variable \\(w=\\hat{z}-2d\\). To obtain (16.7) we multiplied the equation \\(C\\hat{x}=d\\) on the left by \\(2C^{T}\\), then added the result to the first equation of (16.4), and replaced the variable \\(\\hat{z}\\) with \\(w+2d\\).\nNext we use the QR factorization\n\\[\\left[\\begin{array}{c}A\\\\ C\\end{array}\\right]=QR=\\left[\\begin{array}{c}Q_{1}\\\\ Q_{2}\\end{array}\\right]R\\] (16.8)\nto simplify (16.7). This factorization exists because the stacked matrix has linearly independent columns, by our assumption (16.5). In (16.8) we also partition \\(Q\\) in two blocks \\(Q_{1}\\) and \\(Q_{2}\\), of size \\(m\\times n\\) and \\(p\\times n\\), respectively. If we make the substitutions \\(A=Q_{1}R\\), \\(C=Q_{2}R\\), and \\(A^{T}A+C^{T}C=R^{T}R\\) in (16.7) we obtain\n\\[2R^{T}R\\hat{x}+R^{T}Q_{2}^{T}w=2R^{T}Q_{1}^{T}b,\\qquad Q_{2}R\\hat{x}=d.\\]\nWe multiply the first equation on the left by \\(R^{-T}\\) (which we know exists) to get\n\\[R\\hat{x}=Q_{1}^{T}b-(1/2)Q_{2}^{T}w.\\] (16.9)\nSubstituting this expression into \\(Q_{2}R\\hat{x}=d\\) gives an equation in \\(w\\):\n\\[Q_{2}Q_{2}^{T}w=2Q_{2}Q_{1}^{T}b-2d.\\] (16.10)\nWe now use the second part of the assumption (16.5) to show that the matrix \\(Q_{2}^{T}=R^{-T}C^{T}\\) has linearly independent columns. Suppose \\(Q_{2}^{T}z=R^{-T}C^{T}z=0\\). Multiplying with \\(R^{T}\\) gives \\(C^{T}z=0\\). Since \\(C\\) has linearly independent rows, this implies \\(z=0\\), and we conclude that the columns of \\(Q_{2}^{T}\\) are linearly independent.\nThe matrix \\(Q_{2}^{T}\\) therefore has a QR factorization \\(Q_{2}^{T}=\\tilde{Q}\\tilde{R}\\). Substituting this into (16.10) gives\n\\[\\tilde{R}^{T}\\tilde{R}w=2\\tilde{R}^{T}\\tilde{Q}^{T}Q_{1}^{T}b-2d,\\]\nwhich we can write as\n\\[\\tilde{R}w=2\\tilde{Q}^{T}Q_{1}^{T}b-2\\tilde{R}^{-T}d.\\]\nWe can use this to compute \\(w\\), first by computing \\(\\tilde{R}^{-T}d\\) (by forward substitution), then forming the right-hand side, and then solving for \\(w\\) using back substitution. Once we know \\(w\\), we can find \\(\\hat{x}\\) from (16.9). The method is summarized in the following algorithm.\n**Algorithm 16.2** Constrained least squares via QR factorization\nIn the unconstrained case (when \\(p=0\\)), step 1 reduces to computing the QR factorization of \\(A\\), steps 2 and 3 are not needed, and step 4 reduces to solving \\(R\\hat{x}=Q_{1}^{T}b\\). This is the same as algorithm 12.1 for solving (unconstrained) least squares problems.\n\n\n",
        "answer": -0.9375,
        "rationale": {
            "Question": "Let m and n be the roots of the equation 2x^2 + 15x + 16 = 0. What is the value of 1/m + 1/n?",
            "Answer": -0.9375,
            "Answer_type": "float",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_7_4.json",
        "question": "What are the real eigenvalues of the matrix [[3, -2, 5], [1, 0, 7], [0, 0, 2]]?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhat are the real eigenvalues of the matrix [[3, -2, 5], [1, 0, 7], [0, 0, 2]]?\n\nDocument 1:\n----\nIt is also common to use large parentheses instead of rectangular brackets, as in\n\\[\\left(\\begin{array}{cccc}0&1&-2.3&0.1\\\\ 1.3&4&-0.1&0\\\\ 4.1&-1&0&1.7\\end{array}\\right).\\]\nAn important attribute of a matrix is its _size_ or _dimensions_, _i.e._, the numbers of rows and columns. The matrix above has 3 rows and 4 columns, so its size is \\(3\\times 4\\). A matrix of size \\(m\\times n\\) is called an \\(m\\times n\\) matrix.\nThe _elements_ (or _entries_ or _coefficients_) of a matrix are the values in the array. The \\(i,j\\) element is the value in the \\(i\\)th row and \\(j\\)th column, denoted by double subscripts: the \\(i,j\\) element of a matrix \\(A\\) is denoted \\(A_{ij}\\) (or \\(A_{i,j}\\), when \\(i\\) or \\(j\\) is more than one digit or character). The positive integers \\(i\\) and \\(j\\) are called the (row and column) _indices_. If \\(A\\) is an \\(m\\times n\\) matrix, then the row index \\(i\\) runs from 1 to \\(m\\) and the column index \\(j\\) runs from 1 to \\(n\\). Row indices go from top to bottom, so row 1 is the top row and row \\(m\\) is the bottom row. Column indices go from left to right, so column 1 is the left column and column \\(n\\) is the right column.\nIf the matrix above is \\(B\\), then we have \\(B_{13}=-2.3\\), \\(B_{32}=-1\\). The row index of the bottom left element (which has value 4.1) is 3; its column index is 1.\nTwo matrices are equal if they have the same size, and the corresponding entries are all equal. As with vectors, we normally deal with matrices with entries that\nare real numbers, which will be our assumption unless we state otherwise. The set of real \\(m\\times n\\) matrices is denoted \\(\\mathbf{R}^{m\\times n}\\). But matrices with complex entries, for example, do arise in some applications.\nMatrix indexing.As with vectors, standard mathematical notation indexes the rows and columns of a matrix starting from 1. In computer languages, matrices are often (but not always) stored as 2-dimensional arrays, which can be indexed in a variety of ways, depending on the language. Lower level languages typically use indices starting from 0; higher level languages and packages that support matrix operations usually use standard mathematical indexing, starting from 1.\nSquare, tall, and wide matrices.A _square_ matrix has an equal number of rows and columns. A square matrix of size \\(n\\times n\\) is said to be of _order_\\(n\\). A _tall_ matrix has more rows than columns (size \\(m\\times n\\) with \\(m>n\\)). A _wide_ matrix has more columns than rows (size \\(m\\times n\\) with \\(n>m\\)).\nColumn and row vectors.An \\(n\\)-vector can be interpreted as an \\(n\\times 1\\) matrix; we do not distinguish between vectors and matrices with one column. A matrix with only one row, _i.e._, with size \\(1\\times n\\), is called a _row vector_; to give its size, we can refer to it as an _\\(n\\)-row-vector_. As an example,\n\\[\\left[\\begin{array}{ccc}-2.1&-3&0\\end{array}\\right]\\]\nis a 3-row-vector (or \\(1\\times 3\\) matrix). To distinguish them from row vectors, vectors are sometimes called _column vectors_. A \\(1\\times 1\\) matrix is considered to be the same as a scalar.\n\n\n",
        "answer": [
            1,
            2,
            2
        ],
        "rationale": {
            "Question": "What are the real eigenvalues of the matrix [[3, -2, 5], [1, 0, 7], [0, 0, 2]]?",
            "Answer": [
                1,
                2,
                2
            ],
            "Answer_type": "list of integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_7_4.json",
            "explanation": "NONE",
            "theorem": "eigenvalues and eigenvectors",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "wenhuchen/determinant1.json",
        "question": "For matrix A = [[2, 4, 3], [3, 0, 1], [1, 2, 5]], what is its determinant?",
        "prediction": -24,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFor matrix A = [[2, 4, 3], [3, 0, 1], [1, 2, 5]], what is its determinant?\n\nDocument 1:\n----\n* _When is the outer product symmetric?_ Let \\(a\\) and \\(b\\) be \\(n\\)-vectors. The inner product is symmetric, _i.e._, we have \\(a^{T}b=b^{T}a\\). The outer product of the two vectors is generally _not_ symmetric; that is, we generally have \\(ab^{T}\\neq ba^{T}\\). What are the conditions on \\(a\\) and \\(b\\) under which \\(ab=ba^{T}\\)? You can assume that all the entries of \\(a\\) and \\(b\\) are nonzero. (The conclusion you come to will hold even when some entries of \\(a\\) or \\(b\\) are zero.) _Hint_. Show that \\(ab^{T}=ba^{T}\\) implies that \\(a_{i}/b_{i}\\) is a constant (_i.e._, independent of \\(i\\)).\n* _Product of rotation matrices_. Let \\(A\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\theta\\) radians, defined in (7.1), and let \\(B\\) be the \\(2\\times 2\\) matrix that corresponds to rotation by \\(\\omega\\) radians. Show that \\(AB\\) is also a rotation matrix, and give the angle by which it rotates vectors. Verify that \\(AB=BA\\) in this case, and give a simple English explanation.\n* _Two rotations_. Two 3-vectors \\(x\\) and \\(y\\) are related as follows. First, the vector \\(x\\) is rotated \\(40^{\\circ}\\) around the \\(e_{3}\\) axis, counterclockwise (from \\(e_{1}\\) toward \\(e_{2}\\)), to obtain the 3-vector \\(z\\). Then, \\(z\\) is rotated \\(20^{\\circ}\\) around the \\(e_{1}\\) axis, counterclockwise (from \\(e_{2}\\) toward \\(e_{3}\\)), to form \\(y\\). Find the \\(3\\times 3\\) matrix \\(A\\) for which \\(y=Ax\\). Verify that \\(A\\) is an orthogonal matrix. _Hint_. Express \\(A\\) as a product of two matrices, which carry out the two rotations described above.\n* _Entries of matrix triple product_. (See page 10.) Suppose \\(A\\) has dimensions \\(m\\times n\\), \\(B\\) has dimensions \\(n\\times p\\), \\(C\\) has dimensions \\(p\\times q\\), and let \\(D=ABC\\). Show that \\[D_{ij}=\\sum_{k=1}^{n}\\sum_{l=1}^{p}A_{ik}B_{kl}C_{lj}.\\] This is the formula analogous to (10.1) for the product of two matrices.\n* _Multiplication by a diagonal matrix_. Suppose that \\(A\\) is an \\(m\\times n\\) matrix, \\(D\\) is a diagonal matrix, and \\(B=DA\\). Describe \\(B\\) in terms of \\(A\\) and the entries of \\(D\\). You can refer to the rows or columns or entries of \\(A\\).\n**10.10**: _Converting from purchase quantity matrix to purchase dollar matrix_. An \\(n\\times N\\) matrix \\(Q\\) gives the purchase history of a set of \\(n\\) products by \\(N\\) customers, over some period, with \\(Q_{ij}\\) being the quantity of product \\(i\\) bought by customer \\(j\\). The \\(n\\)-vector \\(p\\) gives the product prices. A data analyst needs the \\(n\\times N\\) matrix \\(D\\), where \\(D_{ij}\\) is the total dollar value that customer \\(j\\) spent on product \\(i\\). Express \\(D\\) in terms of \\(Q\\) and \\(p\\), using compact matrix/vector notation. You can use any notation or ideas we have encountered, _e.g._, stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix product, inner product, norm, correlation, \\(\\mathbf{diag}()\\), and so on.\n**10.11**: _Trace of matrix-matrix product_. The sum of the diagonal entries of a square matrix is called the _trace_ of the matrix, denoted \\(\\mathbf{tr}(A)\\).\n1. Suppose \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices. Show that \\[\\mathbf{tr}(A^{T}B)=\\sum_{i=1}^{m}\\sum_{j=1}^{n}A_{ij}B_{ij}.\\] What is the complexity of calculating \\(\\mathbf{tr}(A^{T}B)\\)?\n2. The number \\(\\mathbf{tr}(A^{T}B)\\) is sometimes referred to as the inner product of the matrices \\(A\\) and \\(B\\). (This allows us to extend concepts like angle to matrices.) Show that \\(\\mathbf{tr}(A^{T}B)=\\mathbf{tr}(B^{T}A)\\).\n\n\n",
        "answer": -42,
        "rationale": {
            "Question": "For matrix A = [[2, 4, 3], [3, 0, 1], [1, 2, 5]], what is its determinant?",
            "Answer": -42,
            "Picture": null,
            "Answer_type": "integer",
            "source": "self",
            "id": "wenhuchen/determinant1.json",
            "explanation": "NONE",
            "theorem": "matrix determinant formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "wenhuchen/vieta's_formula5.json",
        "question": "The product of two of the four roots of the quartic equation x^4 - 18x^3 +kx2 + 200x - 1984 = 0 is -32. Determine the value of k.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nThe product of two of the four roots of the quartic equation x^4 - 18x^3 +kx2 + 200x - 1984 = 0 is -32. Determine the value of k.\n\nDocument 1:\n----\nFigure 7.4: Chain graph.\nFigure 7.5: Two vectors of length 100, with Dirichlet energy \\(\\mathcal{D}(a)=1.14\\) and \\(\\mathcal{D}(b)=8.99\\).\nexample, with \\(n=4\\), \\(m=3\\), we have\n\\[c_{1} = a_{1}b_{1}\\] \\[c_{2} = a_{1}b_{2}+a_{2}b_{1}\\] \\[c_{3} = a_{1}b_{3}+a_{2}b_{2}+a_{3}b_{1}\\] \\[c_{4} = a_{2}b_{3}+a_{3}b_{2}+a_{4}b_{1}\\] \\[c_{5} = a_{3}b_{3}+a_{4}b_{2}\\] \\[c_{6} = a_{4}b_{3}.\\]\nConvolution reduces to ordinary multiplication of numbers when \\(n=m=1\\), and to scalar-vector multiplication when either \\(n=1\\) or \\(m=1\\). Convolution arises in many applications and contexts.\nAs a specific numerical example, we have \\((1,0,-1)*(2,1,-1)=(2,1,-3,-1,1)\\), where the entries of the convolution result are found from\n\\[2 = (1)(2)\\] \\[1 = (1)(1)+(0)(2)\\] \\[-3 = (1)(-1)+(0)(1)+(-1)(2)\\] \\[-1 = (0)(-1)+(-1)(1)\\] \\[1 = (-1)(-1).\\]\nPolynomial multiplication.If \\(a\\) and \\(b\\) represent the coefficients of two polynomials\n\\[p(x)=a_{1}+a_{2}x+\\cdots+a_{n}x^{n-1},\\qquad q(x)=b_{1}+b_{2}x+\\cdots+b_{m}x^{ m-1},\\]\nthen the coefficients of the product polynomial \\(p(x)q(x)\\) are represented by \\(c=a*b\\):\n\\[p(x)q(x)=c_{1}+c_{2}x+\\cdots+c_{n+m-1}x^{n+m-2}.\\]\nTo see this we will show that \\(c_{k}\\) is the coefficient of \\(x^{k-1}\\) in \\(p(x)q(x)\\). We expand the product polynomial into \\(mn\\) terms, and collect those terms associated with \\(x^{k-1}\\). These terms have the form \\(a_{i}b_{j}x^{i+j-2}\\), for \\(i\\) and \\(j\\) that satisfy \\(i+j-2=k-1\\), _i.e._, \\(i+j=k-1\\). It follows that \\(c_{k}=\\sum_{i+j=k+1}a_{i}b_{j}\\), which agrees with the convolution formula (7.2).\nProperties of convolution.Convolution is symmetric: We have \\(a*b=b*a\\). It is also associative: We have \\((a*b)*c=a*(b*c)\\), so we can write both as \\(a*b*c\\). Another property is that \\(a*b=0\\) implies that either \\(a=0\\) or \\(b=0\\). These properties follow from the polynomial coefficient property above, and can also be directly shown. As an example, let us show that \\(a*b=b*a\\). Suppose \\(p\\) is the polynomial with coefficients \\(a\\), and \\(q\\) is the polynomial with coefficients \\(b\\). The two polynomials \\(p(x)q(x)\\) and \\(q(x)p(x)\\) are the same (since multiplication of numbers is commutative), so they have the same coefficients. The coefficients of \\(p(x)q(x)\\) are \\(a*b\\) and the coefficients of \\(q(x)p(x)\\) are \\(b*a\\). These must be the same.\nA basic property is that for fixed \\(a\\), the convolution \\(a*b\\) is a linear function of \\(b\\); and for fixed \\(b\\), it is a linear function of \\(a\\). This means we can express \\(a*b\\) as a matrix-vector product:\n\\[a*b=T(b)a=T(a)b,\\]\nwhere \\(T(b)\\) is the \\((n+m-1)\\times n\\) matrix with entries \\[T(b)_{ij}=\\left\\{\\begin{array}{ll}b_{i-j+1}&1\\leq i-j+1\\leq m\\\\ 0&\\text{otherwise}\\end{array}\\right.\\] (7.3) and similarly for \\(T(a)\\). For example, with \\(n=4\\) and \\(m=3\\), we have \\[T(b)=\\left[\\begin{array}{cccc}b_{1}&0&0&0\\\\ b_{2}&b_{1}&0&0\\\\ b_{3}&b_{2}&b_{1}&0\\\\ 0&b_{3}&b_{2}&b_{1}\\\\ 0&0&b_{3}&b_{2}\\\\ 0&0&0&b_{3}\\end{array}\\right],\\qquad T(a)=\\left[\\begin{array}{cccc}a_{1}&0&0 \\\\ a_{2}&a_{1}&0\\\\ a_{3}&a_{2}&a_{1}\\\\ a_{4}&a_{3}&a_{2}\\\\ 0&a_{4}&a_{3}\\\\ 0&0&a_{4}\\end{array}\\right].\\] The matrices \\(T(b)\\) and \\(T(a)\\) are called _Toeplitz_ matrices (named after the mathematician Otto Toeplitz), which means the entries on any diagonal (_i.e._, indices with \\(i-j\\) constant) are the same. The columns of the Toeplitz matrix \\(T(a)\\) are simply shifted versions of the vector \\(a\\), padded with zero entries.\nVariations.Several slightly different definitions of convolution are used in different applications. In one variation, \\(a\\) and \\(b\\) are infinite two-sided sequences (and not vectors) with indices ranging from \\(-\\infty\\) to \\(\\infty\\). In another variation, the rows of \\(T(a)\\) at the top and bottom that do not contain all the coefficients of \\(a\\) are dropped. (In this version, the rows of \\(T(a)\\) are shifted versions of the vector \\(a\\), reversed.) For consistency, we will use the one definition (7.2).\nExamples.\n* _Time series smoothing._ Suppose the \\(n\\)-vector \\(x\\) is a time series, and \\(a=(1/3,1/3,1/3)\\). Then the \\((n+2)\\)-vector \\(y=a*x\\) can be interpreted as a _smoothed_ version of the original time series: for \\(i=3,\\ldots,n\\), \\(y_{i}\\) is the average of \\(x_{i}\\), \\(x_{i-1}\\), \\(x_{i-2}\\). The time series \\(y\\) is called the (3-period) _moving average_ of the time series \\(x\\). Figure 7.6 shows an example.\n\n\n",
        "answer": 86,
        "rationale": {
            "Question": "The product of two of the four roots of the quartic equation x^4 - 18x^3 +kx2 + 200x - 1984 = 0 is -32. Determine the value of k.",
            "Answer": 86,
            "Answer_type": "integer",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula5.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_4.json",
        "question": "Is the set of 3 * 3 matrices in reduced row-echelon form a subspace of R^{3 * 3}?",
        "prediction": false,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIs the set of 3 * 3 matrices in reduced row-echelon form a subspace of R^{3 * 3}?\n\nDocument 1:\n----\nwhere we use the property that \\(\\|(a_{1},\\ldots,a_{k})\\|^{2}=\\|a_{1}\\|^{2}+\\cdots+\\|a_{k}\\|^{2}\\) for any vectors \\(a_{1},\\ldots,a_{k}\\). So we have\n\\[J=\\left\\|\\left[\\begin{array}{c}\\sqrt{\\lambda_{1}}A_{1}\\\\ \\vdots\\\\ \\sqrt{\\lambda_{k}}A_{k}\\end{array}\\right]x-\\left[\\begin{array}{c}\\sqrt{ \\lambda_{1}}b_{1}\\\\ \\vdots\\\\ \\sqrt{\\lambda_{k}}b_{k}\\end{array}\\right]\\right\\|^{2}=\\|\\tilde{A}x-\\tilde{b} \\|^{2},\\]\nwhere \\(\\tilde{A}\\) and \\(\\tilde{b}\\) are the matrix and vector\n\\[\\tilde{A}=\\left[\\begin{array}{c}\\sqrt{\\lambda_{1}}A_{1}\\\\ \\vdots\\\\ \\sqrt{\\lambda_{k}}A_{k}\\end{array}\\right],\\qquad\\tilde{b}=\\left[\\begin{array}[ ]{c}\\sqrt{\\lambda_{1}}b_{1}\\\\ \\vdots\\\\ \\sqrt{\\lambda_{k}}b_{k}\\end{array}\\right].\\] (15.2)\nThe matrix \\(\\tilde{A}\\) is \\(m\\times n\\), and the vector \\(\\tilde{b}\\) has length \\(m\\), where \\(m=m_{1}+\\cdots+m_{k}\\).\nWe have now reduced the problem of minimizing the weighted sum least squares objective to a standard least squares problem. Provided the columns of \\(\\tilde{A}\\) are linearly independent, the minimizer is unique, and given by\n\\[\\hat{x} = (\\tilde{A}^{T}\\tilde{A})^{-1}\\tilde{A}^{T}\\tilde{b}\\] (15.3) \\[= (\\lambda_{1}A_{1}^{T}A_{1}+\\cdots+\\lambda_{k}A_{k}^{T}A_{k})^{-1 }(\\lambda_{1}A_{1}^{T}b_{1}+\\cdots+\\lambda_{k}A_{k}^{T}b_{k}).\\]\nThis reduces to our standard formula for the solution of a least squares problem when \\(k=1\\) and \\(\\lambda_{1}=1\\). (In fact, when \\(k=1\\), \\(\\lambda_{1}\\) does not matter.) We can compute \\(\\hat{x}\\) via the QR factorization of \\(\\tilde{A}\\).\nIndependent columns of stacked matrix.Our assumption (12.2) that the columns of \\(\\tilde{A}\\) in (15.2) are linearly independent is not the same as assuming that each of \\(A_{1},\\ldots,A_{k}\\) has linearly independent columns. We can state the condition that \\(\\tilde{A}\\) has linearly independent columns as: There is no nonzero vector \\(x\\) that satisfies\n\\(A_{i}x=0\\) for \\(i=1,\\ldots,k\\). This implies that if just _one_ of the matrices \\(A_{1},\\ldots,A_{k}\\) has linearly independent columns, then \\(\\tilde{A}\\) does.\nThe stacked matrix \\(\\tilde{A}\\) can have linearly independent columns even when none of the matrices \\(A_{1},\\ldots,A_{k}\\) do. This can happen when \\(m_{i}<n\\) for all \\(i\\), _i.e._, all \\(A_{i}\\) are wide. However, we must have \\(m_{1}+\\cdots+m_{k}\\geq n\\), since \\(\\tilde{A}\\) must be tall or square for the linearly independent columns assumption to hold.\nOptimal trade-off curve.We start with the special case of two objectives (also called the _bi-criterion problem_), and write the weighted sum objective as\n\\[J=J_{1}+\\lambda J_{2}=\\|A_{1}x-b_{1}\\|^{2}+\\lambda\\|A_{2}x-b_{2}\\|^{2},\\]\nwhere \\(\\lambda>0\\) is the relative weight put on the second objective, compared to the first. For small \\(\\lambda\\), we care much more about \\(J_{1}\\) being small than \\(J_{2}\\) being small; for large \\(\\lambda\\), we care much less about \\(J_{1}\\) being small than \\(J_{2}\\) being small.\nLet \\(\\hat{x}(\\lambda)\\) denote the weighted sum least squares solution \\(\\hat{x}\\) as a function of \\(\\lambda\\), assuming the stacked matrices have linearly independent columns. These points are called _Pareto optimal_ (after the economist Vilfredo Pareto) which means there is no point \\(z\\) that satisfies\n\\[\\|A_{1}z-b_{1}\\|^{2}\\leq\\|A_{1}\\hat{x}(\\lambda)-b_{1}\\|^{2},\\qquad\\|A_{2}z-b_{ 2}\\|^{2}\\leq\\|A_{2}\\hat{x}(\\lambda)-b_{2}\\|^{2},\\]\nwith one of the inequalities holding strictly. In other words, there is no point \\(z\\) that is as good as \\(\\hat{x}(\\lambda)\\) in one of the objectives, and beats it on the other one. To see why this is the case, we note that any such \\(z\\) would have a value of \\(J\\) that is less than that achieved by \\(\\hat{x}(\\lambda)\\), which minimizes \\(J\\), a contradiction.\nWe can plot the two objectives \\(\\|A_{1}\\hat{x}(\\lambda)-b_{1}\\|^{2}\\) and \\(\\|A_{2}\\hat{x}(\\lambda)-b_{2}\\|^{2}\\) against each other, as \\(\\lambda\\) varies over \\((0,\\infty)\\), to understand the trade-off of the two objectives. This curve is called the _optimal trade-off curve_ of the two objectives. There is no point \\(z\\) that achieves values of \\(J_{1}\\) and \\(J_{2}\\) that lies below and to the left of the optimal trade-off curve.\nSimple example.We consider a simple example with two objectives, with \\(A_{1}\\) and \\(A_{2}\\) both \\(10\\times 5\\) matrices. The entries of the weighted least squares solution \\(\\hat{x}(\\lambda)\\) are plotted against \\(\\lambda\\) in figure 15.1. On the left, where \\(\\lambda\\) is small, \\(\\hat{x}(\\lambda)\\) is very close to the least squares approximate solution for \\(A_{1},b_{1}\\). On the right, where \\(\\lambda\\) is large, \\(\\hat{x}(\\lambda)\\) is very close to the least squares approximate solution for \\(A_{2},b_{2}\\). In between the behavior of \\(\\hat{x}(\\lambda)\\) is very interesting; for instance, we can see that \\(\\hat{x}(\\lambda)_{3}\\) first increases with increasing \\(\\lambda\\) before eventually decreasing.\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Is the set of 3 * 3 matrices in reduced row-echelon form a subspace of R^{3 * 3}?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_4.json",
            "explanation": "NONE",
            "theorem": "linear subspaces",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": true
    },
    {
        "id": "elainewan/math_algebra_3_5.json",
        "question": "Consider an m * n matrix A and an n * m matrix B (with n != m) such that AB = I_m. Are the columns of A linearly independent?",
        "prediction": true,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nConsider an m * n matrix A and an n * m matrix B (with n != m) such that AB = I_m. Are the columns of A linearly independent?\n\nDocument 1:\n----\nflops. This grows linearly in \\(m\\) and cubicly in \\(n\\) and \\(p\\). The assumption (16.5) implies \\(p\\leq n\\), so in terms of order, \\((n+p)^{3}\\) can be replaced with \\(n^{3}\\).\nSolving constrained least squares problems via QR factorization.We now give a method for solving the constrained least squares problem that generalizes the QR factorization method for least squares problems (algorithm 12.1). We assume that \\(A\\) and \\(C\\) satisfy the conditions (16.5).\nWe start by rewriting the KKT equations (16.4) as\n\\[2(A^{T}A+C^{T}C)\\hat{x}+C^{T}w=2A^{T}b,\\qquad C\\hat{x}=d\\] (16.7)\nwith a new variable \\(w=\\hat{z}-2d\\). To obtain (16.7) we multiplied the equation \\(C\\hat{x}=d\\) on the left by \\(2C^{T}\\), then added the result to the first equation of (16.4), and replaced the variable \\(\\hat{z}\\) with \\(w+2d\\).\nNext we use the QR factorization\n\\[\\left[\\begin{array}{c}A\\\\ C\\end{array}\\right]=QR=\\left[\\begin{array}{c}Q_{1}\\\\ Q_{2}\\end{array}\\right]R\\] (16.8)\nto simplify (16.7). This factorization exists because the stacked matrix has linearly independent columns, by our assumption (16.5). In (16.8) we also partition \\(Q\\) in two blocks \\(Q_{1}\\) and \\(Q_{2}\\), of size \\(m\\times n\\) and \\(p\\times n\\), respectively. If we make the substitutions \\(A=Q_{1}R\\), \\(C=Q_{2}R\\), and \\(A^{T}A+C^{T}C=R^{T}R\\) in (16.7) we obtain\n\\[2R^{T}R\\hat{x}+R^{T}Q_{2}^{T}w=2R^{T}Q_{1}^{T}b,\\qquad Q_{2}R\\hat{x}=d.\\]\nWe multiply the first equation on the left by \\(R^{-T}\\) (which we know exists) to get\n\\[R\\hat{x}=Q_{1}^{T}b-(1/2)Q_{2}^{T}w.\\] (16.9)\nSubstituting this expression into \\(Q_{2}R\\hat{x}=d\\) gives an equation in \\(w\\):\n\\[Q_{2}Q_{2}^{T}w=2Q_{2}Q_{1}^{T}b-2d.\\] (16.10)\nWe now use the second part of the assumption (16.5) to show that the matrix \\(Q_{2}^{T}=R^{-T}C^{T}\\) has linearly independent columns. Suppose \\(Q_{2}^{T}z=R^{-T}C^{T}z=0\\). Multiplying with \\(R^{T}\\) gives \\(C^{T}z=0\\). Since \\(C\\) has linearly independent rows, this implies \\(z=0\\), and we conclude that the columns of \\(Q_{2}^{T}\\) are linearly independent.\nThe matrix \\(Q_{2}^{T}\\) therefore has a QR factorization \\(Q_{2}^{T}=\\tilde{Q}\\tilde{R}\\). Substituting this into (16.10) gives\n\\[\\tilde{R}^{T}\\tilde{R}w=2\\tilde{R}^{T}\\tilde{Q}^{T}Q_{1}^{T}b-2d,\\]\nwhich we can write as\n\\[\\tilde{R}w=2\\tilde{Q}^{T}Q_{1}^{T}b-2\\tilde{R}^{-T}d.\\]\nWe can use this to compute \\(w\\), first by computing \\(\\tilde{R}^{-T}d\\) (by forward substitution), then forming the right-hand side, and then solving for \\(w\\) using back substitution. Once we know \\(w\\), we can find \\(\\hat{x}\\) from (16.9). The method is summarized in the following algorithm.\n**Algorithm 16.2** Constrained least squares via QR factorization\nIn the unconstrained case (when \\(p=0\\)), step 1 reduces to computing the QR factorization of \\(A\\), steps 2 and 3 are not needed, and step 4 reduces to solving \\(R\\hat{x}=Q_{1}^{T}b\\). This is the same as algorithm 12.1 for solving (unconstrained) least squares problems.\nWe now give a complexity analysis. Step 1 involves the QR factorizations of an \\((m+p)\\times n\\) and an \\(n\\times p\\) matrix, which costs \\(2(m+p)n^{2}+2np^{2}\\) flops. Step 2 requires \\(p^{2}\\) flops. In step 3, we first evaluate \\(Q_{1}^{T}b\\) (\\(2mn\\) flops), multiply the result by \\(\\tilde{Q}^{T}\\) (\\(2pn\\) flops), and then solve for \\(w\\) using forward substitution (\\(p^{2}\\) flops). Step 4 requires \\(2mn+2pn\\) flops to form the right-hand side, and \\(n^{2}\\) flops to compute \\(\\hat{x}\\) via back substitution. The costs of steps 2, 3, and 4 are quadratic in the dimensions, and so are negligible compared to the cost of step 1, so our final complexity is\n\\[2(m+p)n^{2}+2np^{2}\\]\nflops. The assumption (16.5) implies the inequalities\n\\[p\\leq n\\leq m+p,\\]\nand therefore \\((m+p)n^{2}\\geq np^{2}\\). So the flop count above is no more than \\(4(m+p)n^{2}\\) flops. In particular, its order is \\((m+p)n^{2}\\).\n\n\n",
        "answer": false,
        "rationale": {
            "Question": "Consider an m * n matrix A and an n * m matrix B (with n != m) such that AB = I_m. Are the columns of A linearly independent?",
            "Answer": false,
            "Answer_type": "bool",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_3_5.json",
            "explanation": "NONE",
            "theorem": "linear independence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "bool",
        "correct": false
    },
    {
        "id": "mingyin/linear-dependence1.json",
        "question": "Finding all the real roots of the equation $\\sqrt{x^2+x+1}+\\sqrt{2 x^2+x+5}=\\sqrt{x^2-3 x+13}$. Return the answer as a list with ascending order.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nFinding all the real roots of the equation $\\sqrt{x^2+x+1}+\\sqrt{2 x^2+x+5}=\\sqrt{x^2-3 x+13}$. Return the answer as a list with ascending order.\n\nDocument 1:\n----\n**11.7**: _Inverse of an upper triangular matrix_. Suppose the \\(n\\times n\\) matrix \\(R\\) is upper triangular and invertible, _i.e._, its diagonal entries are all nonzero. Show that \\(R^{-1}\\) is also upper triangular. _Hint._ Use back substitution to solve \\(Rs_{k}=e_{k}\\), for \\(k=1,\\ldots,n\\), and argue that \\((s_{k})_{i}=0\\) for \\(i>k\\).\n**11.8**: _If a matrix is small, its inverse is large._ If a number \\(a\\) is small, its inverse \\(1/a\\) (assuming \\(a\\neq 0\\)) is large. In this exercise you will explore a matrix analog of this idea. Suppose the \\(n\\times n\\) matrix \\(A\\) is invertible. Show that \\(\\|A^{-1}\\|\\geq\\sqrt{n}/\\|A\\|\\). This implies that if a matrix is small, its inverse is large. _Hint._ You can use the inequality \\(\\|AB\\|\\leq\\|A\\|\\|B\\|\\), which holds for any matrices for which the product makes sense. (See exercise 10.12.)\n**11.9**: _Push-through identity._ Suppose \\(A\\) is \\(m\\times n\\), \\(B\\) is \\(n\\times m\\), and the \\(m\\times m\\) matrix \\(I+AB\\) is invertible.\n1. Show that the \\(n\\times n\\) matrix \\(I+BA\\) is invertible. _Hint._ Show that \\((I+BA)x=0\\) implies \\((I+AB)y=0\\), where \\(y=Ax\\).\n2. Establish the identity \\[B(I+AB)^{-1}=(I+BA)^{-1}B.\\] This is sometimes called the _push-through identity_ since the matrix \\(B\\) appearing on the left 'moves' into the inverse, and 'pushes' the \\(B\\) in the inverse out to the right side. _Hint._ Start with the identity \\[B(I+AB)=(I+BA)B,\\] and multiply on the right by \\((I+AB)^{-1}\\), and on the left by \\((I+BA)^{-1}\\).\n**11.10**: _Reverse-time linear dynamical system._ A linear dynamical system has the form \\[x_{t+1}=Ax_{t},\\] where \\(x_{t}\\) in the (\\(n\\)-vector) state in period \\(t\\), and \\(A\\) is the \\(n\\times n\\) dynamics matrix. This formula gives the state in the next period as a function of the current state. We want to derive a recursion of the form \\[x_{t-1}=A^{\\rm rev}x_{t},\\] which gives the previous state as a function of the current state. We call this the _reverse time linear dynamical system_. 1. When is this possible? When it is possible, what is \\(A^{\\rm rev}\\)? 2. For the specific linear dynamical system with dynamics matrix \\[A=\\left[\\begin{array}{cc}3&2\\\\ -1&4\\end{array}\\right],\\] find \\(A^{\\rm rev}\\), or explain why the reverse time linear dynamical system doesn't exist.\n**11.11**: _Interpolation of rational functions._ (Continuation of exercise 8.8.) Find a rational function \\[f(t)=\\frac{c_{1}+c_{2}t+c_{3}t^{2}}{1+d_{1}t+d_{2}t^{2}}\\] that satisfies the following interpolation conditions: \\[f(1)=2,\\qquad f(2)=5,\\qquad f(3)=9,\\qquad f(4)=-1,\\qquad f(5)=-4.\\] In exercise 8.8 these conditions were expressed as a set of linear equations in the coefficients \\(c_{1}\\), \\(c_{2}\\), \\(c_{3}\\), \\(d_{1}\\) and \\(d_{2}\\); here we are asking you to form and (numerically) solve the system of equations. Plot the rational function you find over the range \\(x=0\\) to \\(x=6\\). Your plot should include markers at the interpolation points \\((1,2),\\ldots,(5,-4)\\). (Your rational function graph should pass through these points.)\n**11.12**: _Combinations of invertible matrices._ Suppose the \\(n\\times n\\) matrices \\(A\\) and \\(B\\) are both invertible. Determine whether each of the matrices given below is invertible, without any further assumptions about \\(A\\) and \\(B\\).\n1. \\(A+B\\).\n2. \\(\\left[\\begin{array}{cc}A&0\\\\ 0&B\\end{array}\\right]\\).\n3. \\(\\left[\\begin{array}{cc}A&A+B\\\\ 0&B\\end{array}\\right]\\).\n4. \\(ABA\\).\n**11.13**: _Another left inverse._ Suppose the \\(m\\times n\\) matrix \\(A\\) is tall and has linearly independent columns. One left inverse of \\(A\\) is the pseudo-inverse \\(A^{\\dagger}\\). In this problem we explore another one. Write \\(A\\) as the block matrix\n\n\n",
        "answer": [
            -1.7807764064,
            0.2807764064
        ],
        "rationale": {
            "Question": "Finding all the real roots of the equation $\\sqrt{x^2+x+1}+\\sqrt{2 x^2+x+5}=\\sqrt{x^2-3 x+13}$. Return the answer as a list with ascending order.",
            "Answer": [
                -1.7807764064,
                0.2807764064
            ],
            "Picture": null,
            "Answer_type": "list of float",
            "source": "linear algebra 2.8 example 2",
            "id": "mingyin/linear-dependence1.json",
            "explanation": "NONE",
            "theorem": "linear dependence",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of float",
        "correct": false
    },
    {
        "id": "wenhuchen/vieta's_formula4.json",
        "question": "If the quartic x^4 + 3x^3 + 11x^2 + 9x + A has roots k, l, m, and n such that kl = mn, find A.",
        "prediction": 3,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nIf the quartic x^4 + 3x^3 + 11x^2 + 9x + A has roots k, l, m, and n such that kl = mn, find A.\n\nDocument 1:\n----\nConvolution reduces to ordinary multiplication of numbers when \\(n=m=1\\), and to scalar-vector multiplication when either \\(n=1\\) or \\(m=1\\). Convolution arises in many applications and contexts.\nAs a specific numerical example, we have \\((1,0,-1)*(2,1,-1)=(2,1,-3,-1,1)\\), where the entries of the convolution result are found from\n\\[2 = (1)(2)\\] \\[1 = (1)(1)+(0)(2)\\] \\[-3 = (1)(-1)+(0)(1)+(-1)(2)\\] \\[-1 = (0)(-1)+(-1)(1)\\] \\[1 = (-1)(-1).\\]\nPolynomial multiplication.If \\(a\\) and \\(b\\) represent the coefficients of two polynomials\n\\[p(x)=a_{1}+a_{2}x+\\cdots+a_{n}x^{n-1},\\qquad q(x)=b_{1}+b_{2}x+\\cdots+b_{m}x^{ m-1},\\]\nthen the coefficients of the product polynomial \\(p(x)q(x)\\) are represented by \\(c=a*b\\):\n\\[p(x)q(x)=c_{1}+c_{2}x+\\cdots+c_{n+m-1}x^{n+m-2}.\\]\nTo see this we will show that \\(c_{k}\\) is the coefficient of \\(x^{k-1}\\) in \\(p(x)q(x)\\). We expand the product polynomial into \\(mn\\) terms, and collect those terms associated with \\(x^{k-1}\\). These terms have the form \\(a_{i}b_{j}x^{i+j-2}\\), for \\(i\\) and \\(j\\) that satisfy \\(i+j-2=k-1\\), _i.e._, \\(i+j=k-1\\). It follows that \\(c_{k}=\\sum_{i+j=k+1}a_{i}b_{j}\\), which agrees with the convolution formula (7.2).\nProperties of convolution.Convolution is symmetric: We have \\(a*b=b*a\\). It is also associative: We have \\((a*b)*c=a*(b*c)\\), so we can write both as \\(a*b*c\\). Another property is that \\(a*b=0\\) implies that either \\(a=0\\) or \\(b=0\\). These properties follow from the polynomial coefficient property above, and can also be directly shown. As an example, let us show that \\(a*b=b*a\\). Suppose \\(p\\) is the polynomial with coefficients \\(a\\), and \\(q\\) is the polynomial with coefficients \\(b\\). The two polynomials \\(p(x)q(x)\\) and \\(q(x)p(x)\\) are the same (since multiplication of numbers is commutative), so they have the same coefficients. The coefficients of \\(p(x)q(x)\\) are \\(a*b\\) and the coefficients of \\(q(x)p(x)\\) are \\(b*a\\). These must be the same.\nA basic property is that for fixed \\(a\\), the convolution \\(a*b\\) is a linear function of \\(b\\); and for fixed \\(b\\), it is a linear function of \\(a\\). This means we can express \\(a*b\\) as a matrix-vector product:\n\\[a*b=T(b)a=T(a)b,\\]\nwhere \\(T(b)\\) is the \\((n+m-1)\\times n\\) matrix with entries \\[T(b)_{ij}=\\left\\{\\begin{array}{ll}b_{i-j+1}&1\\leq i-j+1\\leq m\\\\ 0&\\text{otherwise}\\end{array}\\right.\\] (7.3) and similarly for \\(T(a)\\). For example, with \\(n=4\\) and \\(m=3\\), we have \\[T(b)=\\left[\\begin{array}{cccc}b_{1}&0&0&0\\\\ b_{2}&b_{1}&0&0\\\\ b_{3}&b_{2}&b_{1}&0\\\\ 0&b_{3}&b_{2}&b_{1}\\\\ 0&0&b_{3}&b_{2}\\\\ 0&0&0&b_{3}\\end{array}\\right],\\qquad T(a)=\\left[\\begin{array}{cccc}a_{1}&0&0 \\\\ a_{2}&a_{1}&0\\\\ a_{3}&a_{2}&a_{1}\\\\ a_{4}&a_{3}&a_{2}\\\\ 0&a_{4}&a_{3}\\\\ 0&0&a_{4}\\end{array}\\right].\\] The matrices \\(T(b)\\) and \\(T(a)\\) are called _Toeplitz_ matrices (named after the mathematician Otto Toeplitz), which means the entries on any diagonal (_i.e._, indices with \\(i-j\\) constant) are the same. The columns of the Toeplitz matrix \\(T(a)\\) are simply shifted versions of the vector \\(a\\), padded with zero entries.\nVariations.Several slightly different definitions of convolution are used in different applications. In one variation, \\(a\\) and \\(b\\) are infinite two-sided sequences (and not vectors) with indices ranging from \\(-\\infty\\) to \\(\\infty\\). In another variation, the rows of \\(T(a)\\) at the top and bottom that do not contain all the coefficients of \\(a\\) are dropped. (In this version, the rows of \\(T(a)\\) are shifted versions of the vector \\(a\\), reversed.) For consistency, we will use the one definition (7.2).\nExamples.\n* _Time series smoothing._ Suppose the \\(n\\)-vector \\(x\\) is a time series, and \\(a=(1/3,1/3,1/3)\\). Then the \\((n+2)\\)-vector \\(y=a*x\\) can be interpreted as a _smoothed_ version of the original time series: for \\(i=3,\\ldots,n\\), \\(y_{i}\\) is the average of \\(x_{i}\\), \\(x_{i-1}\\), \\(x_{i-2}\\). The time series \\(y\\) is called the (3-period) _moving average_ of the time series \\(x\\). Figure 7.6 shows an example.\n\n\n",
        "answer": 9,
        "rationale": {
            "Question": "If the quartic x^4 + 3x^3 + 11x^2 + 9x + A has roots k, l, m, and n such that kl = mn, find A.",
            "Answer": 9,
            "Answer_type": "integer",
            "Picture": null,
            "source": "website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf",
            "id": "wenhuchen/vieta's_formula4.json",
            "explanation": "NONE",
            "theorem": "vieta's formula",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_1_2.json",
        "question": "Suppose the graph of a polynomial f(t) = a + bt + ct^2 passes through points (1, -1), (2, 3), and (3, 13). What is f(-1)?",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nSuppose the graph of a polynomial f(t) = a + bt + ct^2 passes through points (1, -1), (2, 3), and (3, 13). What is f(-1)?\n\nDocument 1:\n----\n### 11.4 Examples\nPolynomial interpolation.The 4-vector \\(c\\) gives the coefficients of a cubic polynomial,\n\\[p(x)=c_{1}+c_{2}x+c_{3}x^{2}+c_{4}x^{3}\\]\n(see pages 154 and 120). We seek the coefficients that satisfy\n\\[p(-1.1)=b_{1},\\qquad p(-0.4)=b_{2},\\qquad p(0.2)=b_{3},\\qquad p(0.8)=b_{4}.\\]\nWe can express this as the system of 4 equations in 4 variables \\(Ac=b\\), where\n\\[A=\\left[\\begin{array}{rrrr}1&-1.1&(-1.1)^{2}&(-1.1)^{3}\\\\ 1&-0.4&(-0.4)^{2}&(-0.4)^{3}\\\\ 1&0.2&(0.2)^{2}&(0.2)^{3}\\\\ 1&0.8&(0.8)^{2}&(0.8)^{3}\\end{array}\\right],\\]\nwhich is a specific Vandermonde matrix (see (6.7)). The unique solution is \\(c=A^{-1}b\\), where\n\\[A^{-1}=\\left[\\begin{array}{rrrr}-0.5784&1.9841&-2.1368&0.7310\\\\ 0.3470&0.1984&-1.4957&0.9503\\\\ 0.1388&-1.8651&1.6239&0.1023\\\\ -0.0370&0.3492&0.7521&-0.0643\\end{array}\\right]\\]\n(to 4 decimal places). This is illustrated in figure 11.1, which shows the two cubic polynomials that interpolate the two sets of points shown as filled circles and squares, respectively.\nThe columns of \\(A^{-1}\\) are interesting: They give the coefficients of a polynomial that evaluates to 0 at three of the points, and 1 at the other\nfirst column of \\(A^{-1}\\), which is \\(A^{-1}e_{1}\\), gives the coefficients of the polynomial that has value 1 at \\(-1.1\\), and value 0 at \\(-0.4\\), 0.2, and 0.8. The four polynomials with coefficients given by the columns of \\(A^{-1}\\) are called the _Lagrange polynomials_ associated with the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8. These are plotted in figure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.)\nThe rows of \\(A^{-1}\\) are also interesting: The \\(i\\)th row shows how the values \\(b_{1}\\), ..., \\(b_{4}\\), the polynomial values at the points \\(-1.1\\), \\(-0.4\\), 0.2, 0.8, map into the \\(i\\)th coefficient of the polynomial, \\(c_{i}\\). For example, we see that the coefficient \\(c_{4}\\) is not very sensitive to the value of \\(b_{1}\\) (since \\((A^{-1})_{41}\\) is small). We can also see that for each increase of one in \\(b_{4}\\), the coefficient \\(c_{2}\\) increases by around 0.95.\nBalancing chemical reactions.(See page 154 for background.) We consider the problem of balancing the chemical reaction\n\\[a_{1}\\mathrm{Cr}_{2}\\mathrm{O}_{7}^{2-}+a_{2}\\mathrm{Fe}^{2+}+a_{3}\\mathrm{H} ^{+}\\longrightarrow b_{1}\\mathrm{Cr}^{3+}+b_{2}\\mathrm{Fe}^{3+}+b_{3} \\mathrm{H}_{2}\\mathrm{O},\\]\nwhere the superscript gives the charge of each reactant and product. There are 4 atoms (Cr, O, Fe, H) and charge to balance. The reactant and product matrices are (using the order just listed)\n\\[R=\\left[\\begin{array}{ccc}2&0&0\\\\ 7&0&0\\\\ 0&1&0\\\\ 0&0&1\\\\ -2&2&1\\end{array}\\right],\\qquad P=\\left[\\begin{array}{ccc}1&0&0\\\\ 0&0&1\\\\ 0&1&0\\\\ 0&0&2\\\\ 3&3&0\\end{array}\\right].\\]\nFigure 11.1 Cubic interpolants through two sets of points, shown as circles and squares.\n## Chapter 11 Matrix inverses\n### 11.1 Matrix inverses\nThe matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.1)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.2)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.3)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.4)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.5)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.6)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.7)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\\[\\mathbf{A}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\mathbf{A}\\] (11.8)\nwhere \\(\\mathbf{A}\\) is the matrix of the matrix \\(\\mathbf{A}\\). The matrix \\(\\mathbf{A}\\) is defined as\n\n\n",
        "answer": 9,
        "rationale": {
            "Question": "Suppose the graph of a polynomial f(t) = a + bt + ct^2 passes through points (1, -1), (2, 3), and (3, 13). What is f(-1)?",
            "Answer": 9,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_1_2.json",
            "explanation": "solutions/math_algebra_1_2.txt",
            "theorem": "linear systems",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "mingyin/minimal-polynomial1.json",
        "question": "matrix $A=(\\begin{array}{rrrr} -2 & -1 & -1 & -1 \\ 2 & 1 & 3 & 2 \\ 1 & 1 & 0 & 1 \\ -1 & -1 & -2 & -2 \\end{array})$. Suppose f is the minimal polynomial of A. What is f(99)? Return the numeric without explanation.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nmatrix $A=(\\begin{array}{rrrr} -2 & -1 & -1 & -1 \\ 2 & 1 & 3 & 2 \\ 1 & 1 & 0 & 1 \\ -1 & -1 & -2 & -2 \\end{array})$. Suppose f is the minimal polynomial of A. What is f(99)? Return the numeric without explanation.\n\nDocument 1:\n----\nFigure 13.5: _Top._ Vehicle miles traveled in the US, per month, in the period January 2000 – December 2014 (U.S. Department of Transportation, Bureau of Transportation Statistics, www.transtats.bts.gov). _Bottom._ Least squares fit of a sum of two time series: A linear trend and a seasonal component with a 12-month period.\nwhere \\(\\hat{y}^{\\rm lin}\\) and \\(\\hat{y}^{\\rm seas}\\) are defined as\n\\[\\hat{y}^{\\rm lin}=\\theta_{1}\\left[\\begin{array}{c}1\\\\ 2\\\\ \\vdots\\\\ N\\end{array}\\right],\\qquad\\hat{y}^{\\rm seas}=\\left[\\begin{array}{c}\\theta_{2: (P+1)}\\\\ \\theta_{2:(P+1)}\\\\ \\vdots\\\\ \\theta_{2:(P+1)}\\end{array}\\right].\\]\nThe second component is periodic or _seasonal_, with period \\(P=12\\), and consists of the pattern \\((\\theta_{2},\\ldots,\\theta_{P+1})\\), repeated \\(N/P\\) times (we assume \\(N\\) is a multiple of \\(P\\)). The constant term is omitted in the model because it would be redundant: It has the same effect as adding a constant to the parameters \\(\\theta_{2},\\ldots,\\theta_{P+1}\\).\nThe least squares fit is computed by minimizing \\(\\|A\\theta-y^{\\rm d}\\|^{2}\\) where \\(\\theta\\) is a \\((P+1)\\)-vector and the matrix \\(A\\) in (13.1) is given by\n\\[A=\\left[\\begin{array}{ccccc}1&1&0&\\cdots&0\\\\ 2&0&1&\\cdots&0\\\\ \\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\ P&0&0&\\cdots&1\\\\ P+1&1&0&\\cdots&0\\\\ P+2&0&1&\\cdots&0\\\\ \\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\ 2P&0&0&\\cdots&1\\\\ \\vdots&\\vdots&\\vdots&&\\vdots\\\\ N-P+1&1&0&\\cdots&0\\\\ N-P+2&0&1&\\cdots&0\\\\ \\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\ N&0&0&\\cdots&1\\end{array}\\right].\\]\nIn this example, \\(N=15P=180\\). The residual or prediction error in this case is called the de-trended, seasonally-adjusted series.\nPolynomial fit.A simple extension beyond the straight-line fit is a _polynomial fit_, with\n\\[f_{i}(x)=x^{i-1},\\quad i=1,\\ldots,p,\\]\nso \\(\\hat{f}\\) is a polynomial of degree at most \\(p-1\\),\n\\[\\hat{f}(x)=\\theta_{1}+\\theta_{2}x+\\cdots+\\theta_{p}x^{p-1}.\\]\n(Note that here, \\(x^{i}\\) means the generic scalar value \\(x\\) raised to the \\(i\\)th power; \\(x^{(i)}\\) means the \\(i\\)th observed scalar data value.) In this case the matrix \\(A\\) in (13.1) has the form\n\\[A=\\left[\\begin{array}{ccccc}1&x^{(1)}&\\cdots&(x^{(1)})^{p-1}\\\\ 1&x^{(2)}&\\cdots&(x^{(2)})^{p-1}\\\\ \\vdots&\\vdots&&\\vdots\\\\ 1&x^{(N)}&\\cdots&(x^{(N)})^{p-1}\\end{array}\\right],\\]\n_i.e._, it is a Vandermonde matrix (see (6.7)). Its columns are linearly independent provided the numbers \\(x^{(1)},\\ldots,x^{(N)}\\) include at least \\(p\\) different values. Figure 13.6 shows an example of the least squares fit of polynomials of degree 2, 6, 10, and 15 to a set of 100 data points. Since any polynomial of degree less than \\(r\\) is also a polynomial of degree less than \\(s\\), for \\(r\\leq s\\), it follows that the RMS fit attained by a polynomial with a larger degree is smaller (or at least, no larger) than that obtained by a fit with a smaller degree polynomial. This suggests that we should use the largest degree polynomial that we can, since this results in the smallest residual and the best RMS fit. But we will see in SS13.2 that this is not true, and explore rational methods for choosing a model from among several candidates.\nPiecewise-linear fit.A _piecewise-linear_ function, with _knot points_ or _kink points_\\(a_{1}<a_{2}<\\cdots<a_{k}\\), is a continuous function that is affine in between the knot points. (Such functions should be called piecewise-affine.) We can describe any\nFigure 13.6: Least squares polynomial fits of degree 2, 6, 10, and 15 to 100 points.\npiecewise-linear function with \\(k\\) knot points using the \\(p=k+2\\) basis functions\n\\[f_{1}(x)=1,\\qquad f_{2}(x)=x,\\qquad f_{i+2}(x)=(x-a_{i})_{+},\\quad i=1,\\ldots,k,\\]\nwhere \\((u)_{+}=\\max\\{u,0\\}\\). These basis functions are shown in figure 13.7 for \\(k=2\\) knot points at \\(a_{1}=-1\\), \\(a_{2}=1\\). An example of a piecewise-linear fit with these knot points is shown in figure 13.8.\n#### Regression\nWe now return to the general case when \\(x\\) is an \\(n\\)-vector. Recall that the regression model has the form\n\\[\\hat{y}=x^{T}\\beta+v,\\]\nwhere \\(\\beta\\) is the weight vector and \\(v\\) is the offset. We can put this model in our general data fitting form using the basis functions \\(f_{1}(x)=1\\), and\n\\[f_{i}(x)=x_{i-1},\\quad i=2,\\ldots,n+1,\\]\nso \\(p=n+1\\). The regression model can then be expressed as\n\\[\\hat{y}=x^{T}\\theta_{2:(n+1)}+\\theta_{1},\\]\n\n\n",
        "answer": 990000.0,
        "rationale": {
            "Question": "matrix $A=(\\begin{array}{rrrr} -2 & -1 & -1 & -1 \\ 2 & 1 & 3 & 2 \\ 1 & 1 & 0 & 1 \\ -1 & -1 & -2 & -2 \\end{array})$. Suppose f is the minimal polynomial of A. What is f(99)? Return the numeric without explanation.",
            "Answer": 990000.0,
            "Picture": null,
            "Answer_type": "float",
            "source": "linear algebra 7.1 example 1",
            "id": "mingyin/minimal-polynomial1.json",
            "explanation": "NONE",
            "theorem": "minimal polynomial",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "float",
        "correct": false
    },
    {
        "id": "elainewan/math_algebra_6_2.json",
        "question": "Let V be the space spanned by functions cos(2x) and sin(2x). Find the determinant of the linear transformation D(f) = f' from V to V.",
        "prediction": 0,
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nLet V be the space spanned by functions cos(2x) and sin(2x). Find the determinant of the linear transformation D(f) = f' from V to V.\n\nDocument 1:\n----\nVector-valued functions of vectors.The notation \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}^{m}\\) means that \\(f\\) is a function that maps real \\(n\\)-vectors to real \\(m\\)-vectors. The value of the function \\(f\\), evaluated at an \\(n\\)-vector \\(x\\), is an \\(m\\)-vector \\(f(x)=(f_{1}(x),f_{2}(x),\\ldots,f_{m}(x))\\). Each of the components \\(f_{i}\\) of \\(f\\) is itself a scalar-valued function of \\(x\\). As with scalar-valued functions, we sometimes write \\(f(x)=f(x_{1},x_{2},\\ldots,x_{n})\\) to emphasize that \\(f\\) is a function of \\(n\\) scalar arguments. We use the same notation for each of the components of \\(f\\), writing \\(f_{i}(x)=f_{i}(x_{1},x_{2},\\ldots,x_{n})\\) to emphasize that \\(f_{i}\\) is a function mapping the scalar arguments \\(x_{1},\\ldots,x_{n}\\) into a scalar.\nThe matrix-vector product function.Suppose \\(A\\) is an \\(m\\times n\\) matrix. We can define a function \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}^{m}\\) by \\(f(x)=Ax\\). The inner product function \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}\\), defined as \\(f(x)=a^{T}x\\), discussed in SS2.1, is the special case with \\(m=1\\).\nSuperposition and linearity.The function \\(f:\\mathbf{R}^{n}\\to\\mathbf{R}^{m}\\), defined by \\(f(x)=Ax\\), is _linear_, _i.e._, it satisfies the superposition property:\n\\[f(\\alpha x+\\beta y)=\\alpha f(x)+\\beta f(y)\\] (8.1)\nholds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). It is a good exercise to parse this simple looking equation, since it involves overloading of notation. On the left-hand side, the scalar-vector multiplications \\(\\alpha x\\) and \\(\\beta y\\) involve \\(n\\)-vectors, and the sum \\(\\alpha x+\\beta y\\) is the sum of two \\(n\\)-vectors. The function \\(f\\) maps \\(n\\)-vectors to \\(m\\)-vectors, so \\(f(\\alpha x+\\beta y)\\) is an \\(m\\)-vector. On the right-hand side, the scalar-vector multiplications and the sum are those for \\(m\\)-vectors. Finally, the equality sign is equality between two \\(m\\)-vectors.\nWe can verify that superposition holds for \\(f\\) using properties of matrix-vector and scalar-vector multiplication:\n\\[f(\\alpha x+\\beta y) = A(\\alpha x+\\beta y)\\] \\[= A(\\alpha x)+A(\\beta y)\\] \\[= \\alpha(Ax)+\\beta(Ay)\\] \\[= \\alpha f(x)+\\beta f(y)\\]\nThus we can associate with every matrix \\(A\\) a linear function \\(f(x)=Ax\\).\nThe converse is also true. Suppose \\(f\\) is a function that maps \\(n\\)-vectors to \\(m\\)-vectors, and is linear, _i.e._, (8.1) holds for all \\(n\\)-vectors \\(x\\) and \\(y\\) and all scalars \\(\\alpha\\) and \\(\\beta\\). Then there exists an \\(m\\times n\\) matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\). This can be shown in the same way as for scalar-valued functions in SS2.1, by showing that if \\(f\\) is linear, then\n\\[f(x)=x_{1}f(e_{1})+x_{2}f(e_{2})+\\cdots+x_{n}f(e_{n}),\\] (8.2)\nwhere \\(e_{k}\\) is the \\(k\\)th unit vector of size \\(n\\). The right-hand side can also be written as a matrix-vector product \\(Ax\\), with\n\\[A=\\left[\\begin{array}{cccc}f(e_{1})&f(e_{2})&\\cdots&f(e_{n})\\end{array} \\right].\\]\nThe expression (8.2) is the same as (2.3), but here \\(f(x)\\) and \\(f(e_{k})\\) are vectors. The implications are exactly the same: A linear vector-valued function \\(f\\) is completely characterized by evaluating \\(f\\) at the \\(n\\) unit vectors \\(e_{1},\\ldots,e_{n}\\).\nAs in SS2.1 it is easily shown that the matrix-vector representation of a linear function is unique. If \\(f:{\\bf R}^{n}\\to{\\bf R}^{m}\\) is a linear function, then there exists exactly one matrix \\(A\\) such that \\(f(x)=Ax\\) for all \\(x\\).\nExamples of linear functions.In the examples below we define functions \\(f\\) that map \\(n\\)-vectors \\(x\\) to \\(n\\)-vectors \\(f(x)\\). Each function is described in words, in terms of its effect on an arbitrary \\(x\\). In each case we give the associated matrix multiplication representation.\n* _Negation._\\(f\\) changes the sign of \\(x\\): \\(f(x)=-x\\). Negation can be expressed as \\(f(x)=Ax\\) with \\(A=-I\\).\n\n\n",
        "answer": 4,
        "rationale": {
            "Question": "Let V be the space spanned by functions cos(2x) and sin(2x). Find the determinant of the linear transformation D(f) = f' from V to V.",
            "Answer": 4,
            "Answer_type": "integer",
            "Picture": null,
            "source": "text | Otto Bretscher, Linear Algebra with Applications.",
            "id": "elainewan/math_algebra_6_2.json",
            "explanation": "NONE",
            "theorem": "basis",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "jianyu_xu/geometric_counting_1.json",
        "question": "How many triangles are there whose sides are all integers and whose maximum side length equals 11?",
        "prediction": [
            [
                3,
                4,
                5
            ],
            [
                6,
                8,
                10
            ],
            [
                5,
                12,
                13
            ],
            [
                9,
                12,
                15
            ],
            [
                8,
                15,
                17
            ],
            [
                12,
                16,
                20
            ],
            [
                7,
                24,
                25
            ],
            [
                10,
                24,
                26
            ],
            [
                15,
                20,
                25
            ],
            [
                20,
                21,
                29
            ]
        ],
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nHow many triangles are there whose sides are all integers and whose maximum side length equals 11?\n\nDocument 1:\n----\nFor one, two, and three dimensions, this distance is exactly the usual distance between points with coordinates \\(a\\) and \\(b\\), as illustrated in figure 3.1. But the Euclidean distance is defined for vectors of any dimension; we can refer to the distance between two vectors of dimension 100. Since we only use the Euclidean norm in this book, we will refer to the Euclidean distance between vectors as, simply, the distance between the vectors. If \\(a\\) and \\(b\\) are \\(n\\)-vectors, we refer to the RMS value of the difference, \\(\\|a-b\\|/\\sqrt{n}\\), as the _RMS deviation_ between the two vectors.\nWhen the distance between two \\(n\\)-vectors \\(x\\) and \\(y\\) is small, we say they are 'close' or 'nearby', and when the distance \\(\\|x-y\\|\\) is large, we say they are 'far'. The particular numerical values of \\(\\|x-y\\|\\) that correspond to 'close' or 'far' depend on\nFigure 3.1: The norm of the displacement \\(b-a\\) is the distance between the points with coordinates \\(a\\) and \\(b\\).\nAs an example, consider the 4-vectors\n\\[u=\\left[\\begin{array}{c}1.8\\\\ 2.0\\\\ -3.7\\\\ 4.7\\end{array}\\right],\\qquad v=\\left[\\begin{array}{c}0.6\\\\ 2.1\\\\ 1.9\\\\ -1.4\\end{array}\\right],\\qquad w=\\left[\\begin{array}{c}2.0\\\\ 1.9\\\\ -4.0\\\\ 4.6\\end{array}\\right].\\]\nThe distances between pairs of them are\n\\[\\|u-v\\|=8.368,\\qquad\\|u-w\\|=0.387,\\qquad\\|v-w\\|=8.533,\\]\nso we can say that \\(u\\) is much nearer (or closer) to \\(w\\) than it is to \\(v\\). We can also say that \\(w\\) is much nearer to \\(u\\) than it is to \\(v\\).\nTriangle inequality.We can now explain where the triangle inequality gets its name. Consider a triangle in two or three dimensions, whose vertices have coordinates \\(a\\), \\(b\\), and \\(c\\). The lengths of the sides are the distances between the vertices,\n\\[\\mathbf{dist}(a,b)=\\|a-b\\|,\\qquad\\mathbf{dist}(b,c)=\\|b-c\\|,\\qquad\\mathbf{dist }(a,c)=\\|a-c\\|.\\]\nGeometric intuition tells us that the length of any side of a triangle cannot exceed the sum of the lengths of the other two sides. For example, we have\n\\[\\|a-c\\|\\leq\\|a-b\\|+\\|b-c\\|.\\] (3.3)\nThis follows from the triangle inequality, since\n\\[\\|a-c\\|=\\|(a-b)+(b-c)\\|\\leq\\|a-b\\|+\\|b-c\\|.\\]\nThis is illustrated in figure 3.2.\nFigure 3.2: Triangle inequality.\n#### Examples.\n* _Feature distance._ If \\(x\\) and \\(y\\) represent vectors of \\(n\\) features of two objects, the quantity \\(\\|x-y\\|\\) is called the _feature distance_, and gives a measure of how different the objects are (in terms of their feature values). Suppose for example the feature vectors are associated with patients in a hospital, with entries such as weight, age, presence of chest pain, difficulty breathing, and the results of tests. We can use feature vector distance to say that one patient case is near another one (at least in terms of their feature vectors).\n* _RMS prediction error._ Suppose that the \\(n\\)-vector \\(y\\) represents a time series of some quantity, for example, hourly temperature at some location, and \\(\\hat{y}\\) is another \\(n\\)-vector that represents an estimate or prediction of the time series \\(y\\), based on other information. The difference \\(y-\\hat{y}\\) is called the _prediction error_, and its RMS value \\(\\mathbf{rms}(y-\\hat{y})\\) is called the _RMS prediction error_. If this value is small (say, compared to \\(\\mathbf{rms}(y)\\)) the prediction is good.\n* _Nearest neighbor._ Suppose \\(z_{1},\\ldots,z_{m}\\) is a collection of \\(m\\)\\(n\\)-vectors, and that \\(x\\) is another \\(n\\)-vector. We say that \\(z_{j}\\) is the _nearest neighbor_ of \\(x\\) (among \\(z_{1},\\ldots,z_{m}\\)) if \\[\\|x-z_{j}\\|\\leq\\|x-z_{i}\\|,\\quad i=1,\\ldots,m.\\] In words: \\(z_{j}\\) is the closest vector to \\(x\\) among the vectors \\(z_{1},\\ldots,z_{m}\\). This is illustrated in figure 3.3. The idea of nearest neighbor, and generalizations such as the \\(k\\)-nearest neighbors, are used in many applications.\n\n\n",
        "answer": 36,
        "rationale": {
            "Question": "How many triangles are there whose sides are all integers and whose maximum side length equals 11?",
            "Answer": 36,
            "Answer_type": "integer",
            "Picture": null,
            "source": "self",
            "id": "jianyu_xu/geometric_counting_1.json",
            "explanation": "NONE",
            "theorem": "integer programming",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "integer",
        "correct": false
    },
    {
        "id": "mingyin/gaussian-elimination3.json",
        "question": "Suppose V is a finite-dimensional vector space on F. $M1={a_1,a_2,a_3}$ is a basis of V, $M2={b_1,b_2,b_3}$ is another basis of V. Suppose the coordinates of b_1,b_2,b_3 under M1 are $c_1=(1,1,-1),c_2=(1,-1,1),c_3=(-1,1,1)$. Suppose the coordinate of $d\\in V$ under M1 is (1,3,5). What is the coordinate of d under M2? Return the three coordinate values as a list.",
        "prediction": [
            4,
            2,
            2
        ],
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nSuppose V is a finite-dimensional vector space on F. $M1={a_1,a_2,a_3}$ is a basis of V, $M2={b_1,b_2,b_3}$ is another basis of V. Suppose the coordinates of b_1,b_2,b_3 under M1 are $c_1=(1,1,-1),c_2=(1,-1,1),c_3=(-1,1,1)$. Suppose the coordinate of $d\\in V$ under M1 is (1,3,5). What is the coordinate of d under M2? Return the three coordinate values as a list.\n\nDocument 1:\n----\nwhich, since \\(a_{1},\\ldots,a_{n}\\) are linearly independent, implies that \\(\\beta_{1}=\\cdots=\\beta_{n}=0\\). But then all the \\(\\beta_{i}\\) are zero, a contradiction. So we conclude that \\(\\beta_{n+1}\\neq 0\\). It follows that\n\\[b=(-\\beta_{1}/\\beta_{n+1})a_{1}+\\cdots+(-\\beta_{n}/\\beta_{n+1})a_{n},\\]\n_i.e._, \\(b\\) is a linear combination of \\(a_{1},\\ldots,a_{n}\\).\nCombining this result with the observation above that any linear combination of linearly independent vectors can be expressed in only one way, we conclude:\n_Any \\(n\\)-vector \\(b\\) can be written in a unique way as a linear combination of a basis \\(a_{1},\\ldots,a_{n}\\)._\nExpansion in a basis.When we express an \\(n\\)-vector \\(b\\) as a linear combination of a basis \\(a_{1},\\ldots,a_{n}\\), we refer to\n\\[b=\\alpha_{1}a_{1}+\\cdots+\\alpha_{n}a_{n},\\]\nas the _expansion of \\(b\\) in the \\(a_{1},\\ldots,a_{n}\\) basis_. The numbers \\(\\alpha_{1},\\ldots,\\alpha_{n}\\) are called the _coefficients_ of the expansion of \\(b\\) in the basis \\(a_{1},\\ldots,a_{n}\\). (We will see later how to find the coefficients in the expansion of a vector in a basis.)\nFigure 5.1: Three 2-vectors. _Right._ The vector \\(a_{3}\\) is a linear combination of \\(a_{1}\\) and \\(a_{2}\\), which shows that the vectors are linearly dependent.\n**Examples.**\n* The \\(n\\) standard unit \\(n\\) vectors \\(e_{1},\\ldots,e_{n}\\) are a basis. Any \\(n\\)-vector \\(b\\) can be written as the linear combination \\[b=b_{1}e_{1}+\\cdots+b_{n}e_{n}.\\] (This was already observed on page 17.) This expansion is unique, which means that there is no other linear combination of \\(e_{1},\\ldots,e_{n}\\) that equals \\(b\\).\n* The vectors \\[a_{1}=\\left[\\begin{array}{c}1.2\\\\ -2.6\\end{array}\\right],\\qquad a_{2}=\\left[\\begin{array}{c}-0.3\\\\ -3.7\\end{array}\\right]\\] are a basis. The vector \\(b=(1,1)\\) can be expressed in only one way as a linear combination of them: \\[b=0.6513\\,a_{1}-0.7280\\,a_{2}.\\] (The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)\n**Cash flows and single period loans.** As a practical example, we consider cash flows over \\(n\\) periods, with positive entries meaning income or cash in and negative entries meaning payments or cash out. We define the single-period loan cash flow vectors as\n\\[l_{i}=\\left[\\begin{array}{c}0_{i-1}\\\\ 1\\\\ -(1+r)\\\\ 0_{n-i-1}\\end{array}\\right],\\quad i=1,\\ldots,n-1,\\]\nwhere \\(r\\geq 0\\) is the per-period interest rate. The cash flow \\(l_{i}\\) represents a loan of $1 in period \\(i\\), which is paid back in period \\(i+1\\) with interest \\(r\\). (The subscripts on the zero vectors above give their dimensions.) Scaling \\(l_{i}\\) changes the loan amount; scaling \\(l_{i}\\) by a negative coefficient converts it into a loan _to_ another entity (which is paid back in period \\(i+1\\) with interest).\nThe vectors \\(e_{1},l_{1},\\ldots,l_{n-1}\\) are a basis. (The first vector \\(e_{1}\\) represents income of $1 in period 1.) To see this, we show that they are linearly independent. Suppose that\n\\[\\beta_{1}e_{1}+\\beta_{2}l_{1}+\\cdots+\\beta_{n}l_{n-1}=0.\\]\nWe can express this as\n\\[\\left[\\begin{array}{c}\\beta_{1}+\\beta_{2}\\\\ \\beta_{3}-(1+r)\\beta_{2}\\\\ \\vdots\\\\ \\beta_{n}-(1+r)\\beta_{n-1}\\\\ -(1+r)\\beta_{n}\\end{array}\\right]=0.\\]\nThe last entry is \\(-(1+r)\\beta_{n}=0\\), which implies that \\(\\beta_{n}=0\\) (since \\(1+r>0\\)). Using \\(\\beta_{n}=0\\), the second to last entry becomes \\(-(1+r)\\beta_{n-1}=0\\), so we conclude that \\(\\beta_{n-1}=0\\). Continuing this way we find that \\(\\beta_{n-2},\\ldots,\\beta_{2}\\) are all zero. The\n## Chapter 5 Linear independence\n### 5.1 Linear independence\nLinear independence is a generalization of the theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of the linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of linear independence theory of linear independence theory of linear independence theory of linear independence theory of linear independence. The linear independence is a generalization of\n\n\n",
        "answer": [
            2,
            3,
            4
        ],
        "rationale": {
            "Question": "Suppose V is a finite-dimensional vector space on F. $M1={a_1,a_2,a_3}$ is a basis of V, $M2={b_1,b_2,b_3}$ is another basis of V. Suppose the coordinates of b_1,b_2,b_3 under M1 are $c_1=(1,1,-1),c_2=(1,-1,1),c_3=(-1,1,1)$. Suppose the coordinate of $d\\in V$ under M1 is (1,3,5). What is the coordinate of d under M2? Return the three coordinate values as a list.",
            "Answer": [
                2,
                3,
                4
            ],
            "Picture": null,
            "Answer_type": "list of integer",
            "source": "linear algebra 2.6 example 2",
            "id": "mingyin/gaussian-elimination3.json",
            "explanation": "NONE",
            "theorem": "gaussian elimination",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "list of integer",
        "correct": false
    },
    {
        "id": "xinyi/linear_projection.json",
        "question": "Which of the following matrices takes any vector $v$ and projects it onto the space spanned by the columns of $\\Phi$? (a) $(\\Phi^T\\Phi)^{-1}$. (b) $\\Phi(\\Phi^T\\Phi)^{-1}$. (c) $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$. (d) $\\Phi^T(\\Phi^T\\Phi)^{-1}\\Phi^T$.",
        "prediction": "(c)",
        "llm_system_prompt": "You are a mathematician, you are supposed to answer the given question. You will be provided with a number of documents, which may be helpful. You need to output the answer in your final sentence like \"Therefore, the answer is ...\". The answer can only be one of the following forms:\n1. a numerical value like 0.1, no symbol and no unit at all.\n2. a list of number like [2, 3, 4].\n3. True/False.\n4. an option like (a), (b), (c), (d)\n",
        "llm_user_prompt": "Question:\n----\nWhich of the following matrices takes any vector $v$ and projects it onto the space spanned by the columns of $\\Phi$? (a) $(\\Phi^T\\Phi)^{-1}$. (b) $\\Phi(\\Phi^T\\Phi)^{-1}$. (c) $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$. (d) $\\Phi^T(\\Phi^T\\Phi)^{-1}\\Phi^T$.\n\nDocument 1:\n----\nThis is the same as the usual formula for expanding a product of sums of scalars; but with matrices, we must be careful to preserve the order of the products.\nInner product and matrix-vector products.As an exercise on matrix-vector products and inner products, one can verify that if \\(A\\) is \\(m\\times n\\), \\(x\\) is an \\(n\\)-vector, and \\(y\\) is an \\(m\\)-vector, then\n\\[y^{T}(Ax)=(y^{T}A)x=(A^{T}y)^{T}x,\\]\n_i.e._, the inner product of \\(y\\) and \\(Ax\\) is equal to the inner product of \\(x\\) and \\(A^{T}y\\). (Note that when \\(m\\neq n\\), these inner products involve vectors with different dimensions.)\nProducts of block matrices.Suppose \\(A\\) is a block matrix with \\(m\\times p\\) block entries \\(A_{ij}\\), and \\(B\\) is a block matrix with \\(p\\times n\\) block entries \\(B_{ij}\\), and for each \\(k=1,\\ldots,p\\), the matrix product \\(A_{ik}B_{kj}\\) makes sense, _i.e._, the number of columns of \\(A_{ik}\\) equals the number of rows of \\(B_{kj}\\). (In this case we say that the block matrices _conform_ or\nare _compatible_.) Then \\(C=AB\\) can be expressed as the \\(m\\times n\\) block matrix with entries \\(C_{ij}\\), given by the formula (10.1). For example, we have\n\\[\\left[\\begin{array}{cc}A&B\\\\ C&D\\end{array}\\right]\\left[\\begin{array}{cc}E&F\\\\ G&H\\end{array}\\right]=\\left[\\begin{array}{cc}AE+BG&AF+BH\\\\ CE+DG&CF+DH\\end{array}\\right],\\]\nfor any matrices \\(A,B,\\ldots,H\\) for which the matrix products above make sense. This formula is the same as the formula for multiplying two \\(2\\times 2\\) matrices (_i.e._, with scalar entries); but when the entries of the matrix are themselves matrices (as in the block matrix above), we must be careful to preserve the multiplication order.\nColumn interpretation of matrix-matrix product.We can derive some additional insight into matrix multiplication by interpreting the operation in terms of the columns of the second matrix. Consider the matrix product of an \\(m\\times p\\) matrix \\(A\\) and a \\(p\\times n\\) matrix \\(B\\), and denote the columns of \\(B\\) by \\(b_{k}\\). Using block-matrix notation, we can write the product \\(AB\\) as\n\\[AB=A\\left[\\begin{array}{cccc}b_{1}&b_{2}&\\cdots&b_{n}\\end{array}\\right]\\,= \\left[\\begin{array}{cccc}Ab_{1}&Ab_{2}&\\cdots&Ab_{n}\\end{array}\\right].\\]\nThus, the columns of \\(AB\\) are the matrix-vector products of \\(A\\) and the columns of \\(B\\). The product \\(AB\\) can be interpreted as the matrix obtained by 'applying' \\(A\\) to each of the columns of \\(B\\).\nMultiple sets of linear equations.We can use the column interpretation of matrix multiplication to express a set of \\(k\\) linear equations with the same \\(m\\times n\\) coefficient matrix \\(A\\),\n\\[Ax_{i}=b_{i},\\quad i=1,\\ldots,k,\\]\nin the compact form\n\\[AX=B,\\]\nwhere \\(X=[x_{1}\\;\\cdots\\;x_{k}]\\) and \\(B=[b_{1}\\;\\cdots\\;b_{k}]\\). The matrix equation \\(AX=B\\) is sometimes called a _linear equation with matrix right-hand side_, since it looks like \\(Ax=b\\), but \\(X\\) (the variable) and \\(B\\) (the right-hand side) are now \\(n\\times k\\) matrices, instead of \\(n\\)-vectors (which are \\(n\\times 1\\) matrices).\nRow interpretation of matrix-matrix product.We can give an analogous row interpretation of the product \\(AB\\), by partitioning \\(A\\) and \\(AB\\) as block matrices with row vector blocks. Let \\(a_{1}^{T},\\ldots,a_{m}^{T}\\) be the rows of \\(A\\). Then we have\n\\[AB=\\left[\\begin{array}{c}a_{1}^{T}\\\\ a_{2}^{T}\\\\ \\vdots\\\\ a_{m}^{T}\\end{array}\\right]B=\\left[\\begin{array}{c}a_{1}^{T}B\\\\ a_{2}^{T}B\\\\ \\vdots\\\\ a_{m}^{T}B\\end{array}\\right]=\\left[\\begin{array}{c}(B^{T}a_{1})^{T}\\\\ (B^{T}a_{2})^{T}\\\\ \\vdots\\\\ (B^{T}a_{m})^{T}\\end{array}\\right].\\]\nThis shows that the rows of \\(AB\\) are obtained by applying \\(B^{T}\\) to the transposed row vectors \\(a_{k}\\) of \\(A\\), and transposing the result.\nInner product representation.From the definition of the \\(i,j\\) element of \\(AB\\) in (10.1), we also see that the elements of \\(AB\\) are the inner products of the rows of \\(A\\) with the columns of \\(B\\):\n\n\n",
        "answer": "(c)",
        "rationale": {
            "Question": "Which of the following matrices takes any vector $v$ and projects it onto the space spanned by the columns of $\\Phi$? (a) $(\\Phi^T\\Phi)^{-1}$. (b) $\\Phi(\\Phi^T\\Phi)^{-1}$. (c) $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$. (d) $\\Phi^T(\\Phi^T\\Phi)^{-1}\\Phi^T$.",
            "Answer": "(c)",
            "Answer_type": "option",
            "Picture": null,
            "source": "textbook 3.2",
            "id": "xinyi/linear_projection.json",
            "explanation": "NONE",
            "theorem": "projection theory",
            "subfield": "Algebra",
            "field": "Math"
        },
        "answer_type": "option",
        "correct": true
    }
]